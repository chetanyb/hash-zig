//! GeneralizedXMSS Signature Scheme - Full Rust Compatibility Implementation
//! This implementation matches Rust GeneralizedXMSSSignatureScheme exactly

const std = @import("std");
const log = @import("../../utils/log.zig");
const build_opts = @import("build_options");
const FieldElement = @import("../../core/field.zig").FieldElement;
const ParametersRustCompat = @import("../../core/params_rust_compat.zig").ParametersRustCompat;
const ShakePRFtoF_8_7 = @import("../../prf/shake_prf_to_field.zig").ShakePRFtoF_8_7;
const ShakePRFtoF_7_6 = @import("../../prf/shake_prf_to_field.zig").ShakePRFtoF_7_6;
const Poseidon2RustCompat = @import("../../hash/poseidon2_hash.zig").Poseidon2RustCompat;
const poseidon2_simd = @import("../../hash/poseidon2_hash_simd.zig");
const serialization = @import("../serialization.zig");
const KOALABEAR_PRIME = @import("../../core/field.zig").KOALABEAR_PRIME;
const ChaCha12Rng = @import("../../prf/chacha12_rng.zig").ChaCha12Rng;
const KoalaBearField = @import("../../poseidon2/plonky3_field.zig").KoalaBearField;
const BigInt = std.math.big.int.Managed;
const rng_flow = @import("rng_flow.zig");
const poseidon_top_level = @import("poseidon_top_level.zig");
const target_sum_encoding = @import("target_sum_encoding.zig");
const simd_utils = @import("simd_utils.zig");
const ssz = @import("ssz");

// SSZ tests
test "SSZ: GeneralizedXMSSPublicKey roundtrip" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Create a test public key
    var root: [8]FieldElement = undefined;
    for (0..8) |i| {
        root[i] = FieldElement.fromU32(@intCast(i + 1));
    }
    var parameter: [5]FieldElement = undefined;
    for (0..5) |i| {
        parameter[i] = FieldElement.fromU32(@intCast(i + 10));
    }
    const original = GeneralizedXMSSPublicKey.init(root, parameter, 8);

    // Encode
    var encoded: std.ArrayList(u8) = .{};
    defer encoded.deinit(allocator);
    try original.sszEncode(&encoded, allocator);

    // Decode
    var decoded: GeneralizedXMSSPublicKey = undefined;
    try GeneralizedXMSSPublicKey.sszDecode(encoded.items, &decoded, null);

    // Verify
    const decoded_root = decoded.getRoot();
    const decoded_param = decoded.getParameter();
    for (root, decoded_root) |orig, dec| {
        try std.testing.expect(orig.eql(dec));
    }
    for (parameter, decoded_param) |orig, dec| {
        try std.testing.expect(orig.eql(dec));
    }
    try std.testing.expectEqual(@as(usize, 8), decoded.getHashLenFe());
}

test "SSZ: HashTreeOpening roundtrip" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    // Create test nodes
    var nodes = try allocator.alloc([8]FieldElement, 3);
    defer allocator.free(nodes);
    for (0..3) |i| {
        for (0..8) |j| {
            nodes[i][j] = FieldElement.fromU32(@intCast(i * 8 + j + 1));
        }
    }

    const original = try HashTreeOpening.init(allocator, nodes);
    defer original.deinit();

    // Encode
    var encoded: std.ArrayList(u8) = .{};
    defer encoded.deinit(allocator);
    try original.sszEncode(&encoded, 8, allocator);

    // Decode
    var decoded: HashTreeOpening = undefined;
    defer decoded.deinit();
    try HashTreeOpening.sszDecode(encoded.items, &decoded, allocator, 8);

    // Verify
    const decoded_nodes = decoded.getNodes();
    try std.testing.expectEqual(nodes.len, decoded_nodes.len);
    for (nodes, decoded_nodes) |orig_node, dec_node| {
        for (orig_node, dec_node) |orig_fe, dec_fe| {
            try std.testing.expect(orig_fe.eql(dec_fe));
        }
    }
}

// Constants matching Rust exactly
const MESSAGE_LENGTH = 32;

// Parameter configurations for different lifetimes (matching Rust exactly)
pub const LifetimeParams = struct {
    log_lifetime: usize,
    dimension: usize,
    base: usize,
    final_layer: usize,
    target_sum: usize,
    parameter_len: usize,
    tweak_len_fe: usize,
    msg_len_fe: usize,
    rand_len_fe: usize,
    hash_len_fe: usize,
    capacity: usize,
};

// Rust parameter configurations for each lifetime
pub const LIFETIME_2_8_PARAMS = LifetimeParams{
    .log_lifetime = 8,
    .dimension = 64,
    .base = 8,
    .final_layer = 77,
    .target_sum = 375,
    .parameter_len = 5,
    .tweak_len_fe = 2,
    .msg_len_fe = 9,
    .rand_len_fe = 7,
    .hash_len_fe = 8,
    .capacity = 9,
};

pub const LIFETIME_2_18_PARAMS = LifetimeParams{
    .log_lifetime = 18,
    .dimension = 64,
    .base = 8,
    .final_layer = 77,
    .target_sum = 375,
    .parameter_len = 5,
    .tweak_len_fe = 2,
    .msg_len_fe = 9,
    .rand_len_fe = 6, // Different from 2^8
    .hash_len_fe = 7, // Different from 2^8
    .capacity = 9,
};

pub const LIFETIME_2_32_HASHING_PARAMS = LifetimeParams{
    .log_lifetime = 32,
    .dimension = 64,
    .base = 8,
    .final_layer = 77,
    .target_sum = 375,
    .parameter_len = 5,
    .tweak_len_fe = 2,
    .msg_len_fe = 9,
    .rand_len_fe = 7,
    .hash_len_fe = 8,
    .capacity = 9,
};

// Hash SubTree structure (simplified for now)
pub const PaddedLayer = struct {
    nodes: [][8]FieldElement,
    start_index: usize,
};

pub const HashSubTree = struct {
    root_value: [8]FieldElement,
    layers: ?[]PaddedLayer,
    allocator: std.mem.Allocator,
    depth: usize, // The tree depth (log_lifetime for full trees, log_lifetime/2 for bottom trees)

    pub fn init(allocator: std.mem.Allocator, root_value: [8]FieldElement) !*HashSubTree {
        const self = try allocator.create(HashSubTree);
        self.* = HashSubTree{
            .root_value = root_value,
            .layers = null,
            .allocator = allocator,
            .depth = 0, // Default depth for trees without layers
        };
        return self;
    }

    pub fn initWithLayers(
        allocator: std.mem.Allocator,
        root_value: [8]FieldElement,
        layers: []PaddedLayer,
        depth: usize,
    ) !*HashSubTree {
        const self = try allocator.create(HashSubTree);
        self.* = HashSubTree{
            .root_value = root_value,
            .layers = layers,
            .allocator = allocator,
            .depth = depth,
        };
        return self;
    }

    pub fn deinit(self: *HashSubTree) void {
        if (self.layers) |layers| {
            for (layers) |layer| {
                self.allocator.free(layer.nodes);
            }
            self.allocator.free(layers);
        }
        self.allocator.destroy(self);
    }

    pub fn root(self: *const HashSubTree) [8]FieldElement {
        return self.root_value;
    }

    pub fn getLayers(self: *const HashSubTree) ?[]const PaddedLayer {
        if (self.layers) |layers| {
            return layers;
        }
        return null;
    }
};

/// Helper function to deserialize HashSubTree from leansig SSZ format
/// Serialize HashSubTree to SSZ format (matching Rust leansig)
fn serializeHashSubTree(tree: *const HashSubTree, l: *std.ArrayList(u8), allocator: std.mem.Allocator) !void {
    // Format: [depth:8][lowest_layer:8][layers_offset:4][layers_data]
    const layers = tree.getLayers() orelse return error.NoLayers;

    // Write depth (u64) - Rust stores the tree depth (log_lifetime), not layers.len
    try ssz.serialize(u64, @as(u64, @intCast(tree.depth)), l, allocator);

    // Write lowest_layer (u64) - always 0 for our trees
    try ssz.serialize(u64, @as(u64, 0), l, allocator);

    // Write layers_offset (u32) - points to start of layers array
    const layers_offset: u32 = 20; // 8 + 8 + 4 = 20
    try ssz.serialize(u32, layers_offset, l, allocator);

    // Now serialize layers array
    // First, serialize all layers to get their sizes
    var layer_bytes_list = try tree.allocator.alloc(std.ArrayList(u8), layers.len);
    defer {
        for (layer_bytes_list) |*lb| {
            lb.deinit(tree.allocator);
        }
        tree.allocator.free(layer_bytes_list);
    }

    for (layers, 0..) |layer, i| {
        layer_bytes_list[i] = std.ArrayList(u8){};
        // Check if this is a padded single-node root layer:
        // - It's the last layer
        // - It has exactly 2 nodes
        // - The layer 2 levels down (i-2) has MORE than 2 nodes (indicating natural convergence to 1 root)
        // For top tree: layer i-2 has 2 nodes → layer i has 2 real nodes (no padding skip)
        // For bottom tree: layer i-2 has 4+ nodes → layer i has 1 real node + padding (skip padding)
        const is_padded_single_root = if (i == layers.len - 1 and layer.nodes.len == 2 and i >= 2) blk: {
            const two_below = layers[i - 2];
            break :blk two_below.nodes.len > 2; // More than 2 nodes → converges to 1 → padded
        } else false;
        try serializePaddedLayer(&layer, &layer_bytes_list[i], is_padded_single_root, tree.allocator);
    }

    // Write layer offsets (relative to start of layers array)
    var current_offset: u32 = @as(u32, @intCast(layers.len * 4)); // Space for offsets
    for (layer_bytes_list) |*lb| {
        try ssz.serialize(u32, current_offset, l, allocator);
        current_offset += @as(u32, @intCast(lb.items.len));
    }

    // Write layer data
    for (layer_bytes_list) |*lb| {
        try l.appendSlice(allocator, lb.items);
    }
}

/// Serialize PaddedLayer to SSZ format (matching Rust leansig)
fn serializePaddedLayer(layer: *const PaddedLayer, l: *std.ArrayList(u8), skip_padding: bool, allocator: std.mem.Allocator) !void {
    // Format: [start_index:8][nodes_offset:4][nodes_data]

    // Write start_index (u64)
    try ssz.serialize(u64, @as(u64, @intCast(layer.start_index)), l, allocator);

    // Write nodes_offset (u32) - points to start of nodes array
    const nodes_offset: u32 = 12; // 8 + 4 = 12
    try ssz.serialize(u32, nodes_offset, l, allocator);

    // For padded single-node roots, Rust only serializes the real node (not padding)
    // - If start_index is even (0), padding is at the back: serialize nodes[0]
    // - If start_index is odd (1), padding is at the front: serialize nodes[1]
    const nodes_to_serialize = if (skip_padding) blk: {
        if ((layer.start_index & 1) == 0) {
            // Even start_index: real node is first, padding is last
            break :blk layer.nodes[0..1];
        } else {
            // Odd start_index: padding is first, real node is last
            break :blk layer.nodes[1..2];
        }
    } else layer.nodes;

    // Write nodes as raw field element arrays (no length prefix, Vec<[FE; 8]> in Rust)
    for (nodes_to_serialize) |node| {
        // Each node is [8]FieldElement, serialize as 8 u32s in canonical form
        for (node) |fe| {
            try ssz.serialize(u32, fe.toCanonical(), l, allocator);
        }
    }
}

fn deserializeHashSubTree(allocator: std.mem.Allocator, serialized: []const u8) !*HashSubTree {
    if (serialized.len < 20) return error.InvalidLength;

    var offset: usize = 0;

    // Decode depth (u64)
    const depth = std.mem.readInt(u64, serialized[offset .. offset + 8][0..8], .little);
    offset += 8;

    // Decode lowest_layer (u64)
    const lowest_layer = std.mem.readInt(u64, serialized[offset .. offset + 8][0..8], .little);
    offset += 8;
    _ = lowest_layer;

    // Decode layers_offset (u32)
    const layers_offset = std.mem.readInt(u32, serialized[offset .. offset + 4][0..4], .little);
    offset += 4;

    // Layers array is a Vec<PaddedLayer>
    const layers_data_start = @as(usize, layers_offset);
    if (layers_data_start > serialized.len) return error.InvalidOffset;

    const layers_data = serialized[layers_data_start..];

    // Count number of layer offsets
    const first_layer_offset = std.mem.readInt(u32, layers_data[0..4], .little);
    const num_layers = first_layer_offset / 4;

    // Allocate layers array
    const layers = try allocator.alloc(PaddedLayer, num_layers);
    errdefer {
        for (layers) |layer| {
            allocator.free(layer.nodes);
        }
        allocator.free(layers);
    }

    // Deserialize each layer
    for (0..num_layers) |i| {
        const layer_rel_offset = std.mem.readInt(u32, layers_data[i * 4 ..][0..4], .little);
        const layer_start = layers_data_start + layer_rel_offset;

        // Determine layer end
        const layer_end = if (i + 1 < num_layers) blk: {
            const next_offset = std.mem.readInt(u32, layers_data[(i + 1) * 4 ..][0..4], .little);
            break :blk layers_data_start + next_offset;
        } else serialized.len;

        const layer_bytes = serialized[layer_start..layer_end];
        layers[i] = try deserializePaddedLayer(allocator, layer_bytes);
    }

    // Extract root from the last layer's FIRST node (not last!)
    // In leansig's tree structure, the root is stored as the first node of the last layer
    const root_value = if (layers.len > 0 and layers[layers.len - 1].nodes.len > 0) blk: {
        const root_node = layers[layers.len - 1].nodes[0]; // FIRST node, not last!
        break :blk root_node;
    } else [_]FieldElement{FieldElement{ .value = 0 }} ** 8;

    return try HashSubTree.initWithLayers(allocator, root_value, layers, @intCast(depth));
}

/// Helper function to deserialize PaddedLayer from leansig SSZ format
fn deserializePaddedLayer(allocator: std.mem.Allocator, serialized: []const u8) !PaddedLayer {
    if (serialized.len < 12) return error.InvalidLength;

    // Decode start_index (u64)
    const start_index = std.mem.readInt(u64, serialized[0..8], .little);

    // Decode nodes_offset (u32)
    const nodes_offset = std.mem.readInt(u32, serialized[8..12], .little);

    // Nodes array starts at nodes_offset
    const nodes_data = serialized[nodes_offset..];

    // Each node is 8 x u32 = 32 bytes
    const num_nodes = nodes_data.len / 32;
    const nodes = try allocator.alloc([8]FieldElement, num_nodes);
    errdefer allocator.free(nodes);

    // Deserialize nodes
    // Leansig stores field elements in SSZ as CANONICAL form (confirmed by inspecting bytes)
    for (0..num_nodes) |i| {
        for (0..8) |j| {
            const val = std.mem.readInt(u32, nodes_data[i * 32 + j * 4 .. i * 32 + j * 4 + 4][0..4], .little);
            nodes[i][j] = FieldElement.fromCanonical(val);
        }
    }

    // Verify first node canonical matches raw value
    if (num_nodes > 0) {
        const first_node_raw = std.mem.readInt(u32, nodes_data[0..4], .little);
        const first_node_canonical = nodes[0][0].toCanonical();
        if (first_node_raw != first_node_canonical) {
            std.debug.print("TREE_DECODE_ERROR: First node mismatch! raw=0x{x:0>8}, canonical=0x{x:0>8}\n", .{
                first_node_raw,
                first_node_canonical,
            });
        }
    }

    return PaddedLayer{
        .start_index = @intCast(start_index),
        .nodes = nodes,
    };
}

const CACHE_MAGIC = @as(u32, 0x42544331); // "BTC1"
const CACHE_VERSION: u8 = 2;

const BottomTreeCache = struct {
    allocator: std.mem.Allocator,
    enabled: bool,
    root_path: []u8,
    mutex: std.Thread.Mutex,

    const CacheError = error{
        InvalidCacheFile,
        CacheMismatch,
    };

    pub fn init(allocator: std.mem.Allocator) !BottomTreeCache {
        var enabled = true;
        if (std.process.getEnvVarOwned(allocator, "HASH_ZIG_DISABLE_BT_CACHE")) |value| {
            enabled = false;
            allocator.free(value);
        } else |err| switch (err) {
            error.EnvironmentVariableNotFound => {},
            else => return err,
        }

        const path_env = std.process.getEnvVarOwned(allocator, "HASH_ZIG_BT_CACHE_DIR") catch |err| switch (err) {
            error.EnvironmentVariableNotFound => null,
            else => return err,
        };
        const default_path = "tmp/bottom_tree_cache";
        const source_path = path_env orelse default_path;
        const root_path = try allocator.dupe(u8, source_path);
        if (path_env) |env| {
            allocator.free(env);
        }

        if (enabled) {
            std.fs.cwd().makePath(source_path) catch |err| {
                log.print("ZIG_CACHE: disabling bottom tree cache (makePath failed: {s})\n", .{@errorName(err)});
                enabled = false;
            };
        }

        return BottomTreeCache{
            .allocator = allocator,
            .enabled = enabled,
            .root_path = root_path,
            .mutex = .{},
        };
    }

    pub fn deinit(self: *BottomTreeCache) void {
        self.allocator.free(self.root_path);
    }

    fn computeKey(
        log_lifetime: usize,
        bottom_tree_index: usize,
        prf_key: [32]u8,
        parameter: [5]FieldElement,
    ) [64]u8 {
        var hasher = std.crypto.hash.sha2.Sha256.init(.{});

        var buf8: [8]u8 = undefined;
        std.mem.writeInt(u64, &buf8, @as(u64, @intCast(log_lifetime)), .little);
        hasher.update(&buf8);

        std.mem.writeInt(u64, &buf8, @as(u64, bottom_tree_index), .little);
        hasher.update(&buf8);

        hasher.update(&prf_key);

        for (parameter) |fe| {
            var buf4: [4]u8 = undefined;
            std.mem.writeInt(u32, &buf4, fe.toCanonical(), .little);
            hasher.update(&buf4);
        }

        var digest: [32]u8 = undefined;
        hasher.final(&digest);

        var hex: [64]u8 = undefined;
        _ = std.fmt.bufPrint(&hex, "{x}", .{digest}) catch unreachable;
        return hex;
    }

    fn writeFieldElement(writer: *std.Io.Writer, fe: FieldElement) !void {
        var buf: [4]u8 = undefined;
        std.mem.writeInt(u32, &buf, fe.toCanonical(), .little);
        try writer.writeAll(&buf);
    }

    fn readFieldElement(reader: *std.Io.Reader) !FieldElement {
        const buf = try reader.takeArray(4);
        const value = std.mem.readInt(u32, buf, .little);
        return FieldElement.fromCanonical(value);
    }

    fn openDir(self: *BottomTreeCache) !std.fs.Dir {
        return std.fs.cwd().openDir(self.root_path, .{});
    }

    pub fn load(
        self: *BottomTreeCache,
        allocator: std.mem.Allocator,
        log_lifetime: usize,
        prf_key: [32]u8,
        parameter: [5]FieldElement,
        bottom_tree_index: usize,
    ) !?*HashSubTree {
        if (!self.enabled) return null;

        var filename_buf: [80]u8 = undefined;
        const key = computeKey(log_lifetime, bottom_tree_index, prf_key, parameter);
        const filename = std.fmt.bufPrint(&filename_buf, "{s}.bin", .{key}) catch {
            return null;
        };

        self.mutex.lock();
        defer self.mutex.unlock();

        var dir = self.openDir() catch return null;
        defer dir.close();

        const file = dir.openFile(filename, .{ .mode = .read_only }) catch return null;
        defer file.close();

        var read_buffer: [4096]u8 = undefined;
        var file_reader = file.reader(&read_buffer);
        const reader = &file_reader.interface;

        const magic_bytes = try reader.takeArray(4);
        const magic = std.mem.readInt(u32, magic_bytes, .little);
        if (magic != CACHE_MAGIC) return CacheError.InvalidCacheFile;

        const version_bytes = try reader.takeArray(1);
        const version = version_bytes[0];
        if (version != CACHE_VERSION) return CacheError.InvalidCacheFile;

        const stored_log_bytes = try reader.takeArray(1);
        const stored_log = stored_log_bytes[0];
        _ = try reader.takeArray(2); // reserved
        if (stored_log != log_lifetime) return CacheError.CacheMismatch;

        const stored_index_bytes = try reader.takeArray(4);
        const stored_index = std.mem.readInt(u32, stored_index_bytes, .little);
        if (stored_index != bottom_tree_index) return CacheError.CacheMismatch;

        var stored_prf: [32]u8 = undefined;
        try reader.readSliceAll(&stored_prf);
        if (!std.mem.eql(u8, &stored_prf, &prf_key)) return CacheError.CacheMismatch;

        for (parameter) |expected| {
            const value = try readFieldElement(reader);
            if (!value.eql(expected)) return CacheError.CacheMismatch;
        }

        var root_value: [8]FieldElement = undefined;
        for (&root_value) |*dest| {
            dest.* = try readFieldElement(reader);
        }

        const num_layers_bytes = try reader.takeArray(4);
        const num_layers = std.mem.readInt(u32, num_layers_bytes, .little);
        if (num_layers == 0) return CacheError.InvalidCacheFile;

        const layers = try allocator.alloc(PaddedLayer, num_layers);
        errdefer {
            for (layers) |layer| {
                allocator.free(layer.nodes);
            }
            allocator.free(layers);
        }

        for (layers, 0..) |*layer, layer_idx| {
            _ = layer_idx;
            const start_index_bytes = try reader.takeArray(8);
            const start_index_u64 = std.mem.readInt(u64, start_index_bytes, .little);
            const node_count_bytes = try reader.takeArray(4);
            const node_count = std.mem.readInt(u32, node_count_bytes, .little);

            const start_index = std.math.cast(usize, start_index_u64) orelse return CacheError.InvalidCacheFile;
            var nodes = try allocator.alloc([8]FieldElement, node_count);
            errdefer allocator.free(nodes);

            for (0..node_count) |node_idx| {
                for (0..8) |j| {
                    nodes[node_idx][j] = try readFieldElement(reader);
                }
            }

            layer.* = .{
                .start_index = start_index,
                .nodes = nodes,
            };
        }

        // Cache doesn't store depth, so we use 0 as a placeholder
        return try HashSubTree.initWithLayers(allocator, root_value, layers, 0);
    }

    pub fn store(
        self: *BottomTreeCache,
        log_lifetime: usize,
        prf_key: [32]u8,
        parameter: [5]FieldElement,
        bottom_tree_index: usize,
        root_value: [8]FieldElement,
        layers: []const PaddedLayer,
    ) void {
        if (!self.enabled) return;

        var filename_buf: [80]u8 = undefined;
        const key = computeKey(log_lifetime, bottom_tree_index, prf_key, parameter);
        const filename = std.fmt.bufPrint(&filename_buf, "{s}.bin", .{key}) catch {
            log.print("ZIG_CACHE: failed to format cache filename for bottom tree {}\n", .{bottom_tree_index});
            return;
        };

        self.mutex.lock();
        defer self.mutex.unlock();

        var dir = self.openDir() catch {
            log.print("ZIG_CACHE: unable to open cache dir {s}\n", .{self.root_path});
            return;
        };
        defer dir.close();

        var atomic_write_buffer: [4096]u8 = undefined;
        var atomic_file = dir.atomicFile(filename, .{ .write_buffer = &atomic_write_buffer }) catch {
            log.print("ZIG_CACHE: unable to create cache file {s}\n", .{filename});
            return;
        };
        defer atomic_file.deinit();

        var writer_buffer: [4096]u8 = undefined;
        var file_writer = atomic_file.file_writer.file.writer(&writer_buffer);
        const writer = &file_writer.interface;

        // Helper to write integers
        var buf4: [4]u8 = undefined;
        var buf8: [8]u8 = undefined;
        var buf2: [2]u8 = undefined;

        std.mem.writeInt(u32, &buf4, CACHE_MAGIC, .little);
        writer.writeAll(&buf4) catch return;
        writer.writeByte(CACHE_VERSION) catch return;
        writer.writeByte(@intCast(log_lifetime)) catch return;
        std.mem.writeInt(u16, &buf2, 0, .little);
        writer.writeAll(&buf2) catch return;
        std.mem.writeInt(u32, &buf4, @intCast(bottom_tree_index), .little);
        writer.writeAll(&buf4) catch return;
        writer.writeAll(&prf_key) catch return;

        for (parameter) |fe| {
            writeFieldElement(writer, fe) catch return;
        }

        for (root_value) |fe| {
            writeFieldElement(writer, fe) catch return;
        }

        std.mem.writeInt(u32, &buf4, @intCast(layers.len), .little);
        writer.writeAll(&buf4) catch return;

        for (layers) |layer| {
            std.mem.writeInt(u64, &buf8, @intCast(layer.start_index), .little);
            writer.writeAll(&buf8) catch return;
            std.mem.writeInt(u32, &buf4, @intCast(layer.nodes.len), .little);
            writer.writeAll(&buf4) catch return;
            for (layer.nodes) |node| {
                for (node) |fe| {
                    writeFieldElement(writer, fe) catch return;
                }
            }
        }

        writer.flush() catch return;
        if (atomic_file.finish()) |_| {} else |err| {
            log.print("ZIG_CACHE: failed to finalize cache file {s}: {s}\n", .{ filename, @errorName(err) });
        }
    }
};

// Hash Tree Opening for Merkle paths
pub const HashTreeOpening = struct {
    nodes: [][8]FieldElement,
    allocator: std.mem.Allocator,

    pub fn init(allocator: std.mem.Allocator, nodes: [][8]FieldElement) !*HashTreeOpening {
        const self = try allocator.create(HashTreeOpening);
        const nodes_copy = try allocator.alloc([8]FieldElement, nodes.len);
        @memcpy(nodes_copy, nodes);
        self.* = HashTreeOpening{
            .nodes = nodes_copy,
            .allocator = allocator,
        };
        return self;
    }

    pub fn deinit(self: *HashTreeOpening) void {
        self.allocator.free(self.nodes);
        self.allocator.destroy(self);
    }

    pub fn getNodes(self: *const HashTreeOpening) [][8]FieldElement {
        return self.nodes;
    }

    // SSZ serialization methods
    pub fn sszEncode(self: *const HashTreeOpening, l: *std.ArrayList(u8), hash_len_fe: usize, allocator: std.mem.Allocator) !void {
        // Rust's HashTreeOpening is a struct with one variable field: co_path (Vec<Domain>)
        // SSZ Container encoding: [internal_offset: 4][co_path data]
        // Since Domain (FieldArray) is fixed-size, Vec<Domain> encodes as raw bytes WITHOUT length prefix

        // Write internal offset (always 4, points to where co_path starts)
        try ssz.serialize(u32, @as(u32, 4), l, allocator);

        // Write co_path as raw node data (no length prefix for fixed-size items)
        // Each node is hash_len_fe field elements (28 bytes for 2^18, 32 bytes for 2^8/2^32)
        for (self.nodes) |node| {
            // Write only hash_len_fe field elements per node
            for (0..hash_len_fe) |j| {
                const canonical = node[j].toCanonical();
                try ssz.serialize(u32, canonical, l, allocator);
            }
        }
    }

    pub fn sszDecode(serialized: []const u8, out: *HashTreeOpening, allocator: ?std.mem.Allocator, hash_len_fe: usize) !void {
        const alloc = allocator orelse return error.AllocatorRequired;

        // Rust's HashTreeOpening is a struct with one variable field: co_path (Vec<Domain>)
        // SSZ Container encoding: [internal_offset: 4][co_path data as raw bytes]
        // Since Domain (FieldArray) is fixed-size, Vec<Domain> encodes WITHOUT length prefix

        // Read internal offset (should be 4)
        if (serialized.len < 4) return error.InvalidLength;
        const internal_offset = std.mem.readInt(u32, @as(*const [4]u8, @ptrCast(&serialized[0])), .little);

        if (internal_offset != 4) return error.InvalidOffset;

        // Calculate number of nodes from remaining data
        // Each node is hash_len_fe field elements × 4 bytes
        const node_size = hash_len_fe * 4;
        const copath_data_len = serialized.len - internal_offset;

        if (copath_data_len % node_size != 0) return error.InvalidLength;
        const num_nodes = copath_data_len / node_size;

        if (num_nodes == 0) {
            out.* = HashTreeOpening{
                .nodes = try alloc.alloc([8]FieldElement, 0),
                .allocator = alloc,
            };
            return;
        }

        // Decode nodes
        var nodes = try alloc.alloc([8]FieldElement, num_nodes);
        errdefer alloc.free(nodes);

        var pos: usize = internal_offset;
        for (0..num_nodes) |node_idx| {
            // Read hash_len_fe field elements for this node
            for (0..hash_len_fe) |j| {
                if (serialized.len < pos + 4) return error.InvalidLength;
                var val: u32 = undefined;
                try ssz.deserialize(u32, serialized[pos .. pos + 4], &val, null);
                nodes[node_idx][j] = FieldElement.fromCanonical(val);
                pos += 4;
            }
            // Zero-pad remaining field elements if hash_len_fe < 8
            for (hash_len_fe..8) |j| {
                nodes[node_idx][j] = FieldElement.fromCanonical(0);
            }
        }

        out.* = HashTreeOpening{
            .nodes = nodes,
            .allocator = alloc,
        };
    }

    pub fn isFixedSizeObject(comptime T: type) bool {
        _ = T;
        return false; // Variable size
    }

    /// Serialize to SSZ bytes (convenience method matching Rust's to_bytes)
    pub fn toBytes(self: *const HashTreeOpening, allocator: std.mem.Allocator, hash_len_fe: usize) ![]u8 {
        var encoded: std.ArrayList(u8) = .{};
        errdefer encoded.deinit(allocator);
        try self.sszEncode(&encoded, hash_len_fe, allocator);
        return encoded.toOwnedSlice(allocator);
    }

    /// Deserialize from SSZ bytes (convenience method matching Rust's from_bytes)
    pub fn fromBytes(serialized: []const u8, allocator: std.mem.Allocator) !*HashTreeOpening {
        const decoded = try allocator.create(HashTreeOpening);
        errdefer allocator.destroy(decoded);
        try sszDecode(serialized, decoded, allocator);
        return decoded;
    }
};

// Signature structure matching Rust exactly
pub const GeneralizedXMSSSignature = struct {
    // Private fields - not directly accessible from outside
    path: *HashTreeOpening,
    rho: [7]FieldElement, // IE::Randomness (max length; actual rand_len_fe may be smaller)
    hashes: [][8]FieldElement, // Vec<TH::Domain>
    allocator: std.mem.Allocator,
    is_deserialized: bool, // Track if signature was deserialized from JSON (Rust→Zig)
    rand_len_fe: ?usize, // Store rand_len_fe for SSZ encoding (None = infer from rho)

    pub fn init(allocator: std.mem.Allocator, path: *HashTreeOpening, rho: [7]FieldElement, hashes: [][8]FieldElement) !*GeneralizedXMSSSignature {
        const self = try allocator.create(GeneralizedXMSSSignature);
        const hashes_copy = try allocator.alloc([8]FieldElement, hashes.len);
        @memcpy(hashes_copy, hashes);
        // Infer rand_len_fe from rho (count non-zero elements, but at least 6 for 2^18, 7 for 2^8/2^32)
        // Actually, we can't reliably infer it, so we'll set it to None and infer during SSZ encoding
        var rand_len_fe: ?usize = null;
        // Try to infer: count non-zero elements, but this is not reliable
        for (0..7) |i| {
            if (rho[i].isZero()) {
                if (i >= 6) {
                    rand_len_fe = i;
                    break;
                }
            }
        }
        if (rand_len_fe == null) {
            rand_len_fe = 7; // Default to 7 if all non-zero
        }
        self.* = GeneralizedXMSSSignature{
            .path = path,
            .rho = rho,
            .hashes = hashes_copy,
            .allocator = allocator,
            .is_deserialized = false, // Created directly, not deserialized
            .rand_len_fe = rand_len_fe,
        };
        return self;
    }

    pub fn initDeserialized(allocator: std.mem.Allocator, path: *HashTreeOpening, rho: [7]FieldElement, hashes: [][8]FieldElement) !*GeneralizedXMSSSignature {
        const self = try allocator.create(GeneralizedXMSSSignature);
        const hashes_copy = try allocator.alloc([8]FieldElement, hashes.len);
        @memcpy(hashes_copy, hashes);
        // Infer rand_len_fe from rho (count non-zero elements)
        var rand_len_fe: ?usize = null;
        for (0..7) |i| {
            if (rho[i].isZero()) {
                if (i >= 6) {
                    rand_len_fe = i;
                    break;
                }
            }
        }
        if (rand_len_fe == null) {
            rand_len_fe = 7; // Default to 7 if all non-zero
        }
        self.* = GeneralizedXMSSSignature{
            .path = path,
            .rho = rho,
            .hashes = hashes_copy,
            .allocator = allocator,
            .is_deserialized = true, // Deserialized from JSON (Rust→Zig)
            .rand_len_fe = rand_len_fe,
        };
        return self;
    }

    pub fn deinit(self: *GeneralizedXMSSSignature) void {
        self.path.deinit(); // Free the HashTreeOpening
        self.allocator.free(self.hashes);
        self.allocator.destroy(self);
    }

    // Controlled access methods for private fields
    pub fn getPath(self: *const GeneralizedXMSSSignature) *HashTreeOpening {
        // Direct field access - if this crashes, the struct memory is invalid
        return self.path;
    }

    pub fn getRho(self: *const GeneralizedXMSSSignature) [7]FieldElement {
        // Direct field access - rho is at offset 40
        // Return by value to avoid potential pointer issues
        const rho = self.rho;
        return rho;
    }

    pub fn getHashes(self: *const GeneralizedXMSSSignature) [][8]FieldElement {
        return self.hashes;
    }

    // Serialization method using controlled access
    pub fn serialize(self: *const GeneralizedXMSSSignature, allocator: std.mem.Allocator) ![]u8 {
        return serialization.serializeSignature(allocator, self);
    }

    // SSZ serialization methods
    pub fn sszEncode(self: *const GeneralizedXMSSSignature, l: *std.ArrayList(u8), allocator: std.mem.Allocator) !void {
        // SSZ struct encoding for GeneralizedXMSSSignature:
        // Field order: path (variable), rho (fixed), hashes (variable)
        // var_start = path_offset(4) + rho(28) + hashes_offset(4) = 36

        // Determine sizes based on rand_len_fe
        // For 2^18: rand_len_fe=6 → rho is 24 bytes, hash_len_fe=7 → nodes are 28 bytes
        // For 2^8/2^32: rand_len_fe=7 → rho is 28 bytes, hash_len_fe=8 → nodes are 32 bytes
        const rand_len = self.rand_len_fe orelse 7;
        const hash_len_fe = rand_len + 1; // 2^18: 6+1=7, 2^8/2^32: 7+1=8
        const rho_size = rand_len * 4;
        const node_size = hash_len_fe * 4;

        // Calculate path size: [internal_offset: 4][nodes as raw bytes]
        const path_size = 4 + (self.path.nodes.len * node_size);

        // Calculate var_start: path_offset(4) + rho + hashes_offset(4)
        const var_start: usize = 4 + rho_size + 4;

        // Calculate hashes size
        var hashes_canonical = try self.allocator.alloc([8]u32, self.hashes.len);
        defer self.allocator.free(hashes_canonical);
        for (self.hashes, 0..) |hash, i| {
            for (hash, 0..) |fe, j| {
                hashes_canonical[i][j] = fe.toCanonical();
            }
        }

        // Write path offset (absolute offset from start of serialized data)
        const path_offset: u32 = @as(u32, @intCast(var_start));
        try ssz.serialize(u32, path_offset, l, allocator);

        // Write rho - CRITICAL: Match Rust's decoder expectations (28 bytes for all lifetimes)
        // Even though Rust's encoder uses 24 bytes for 2^18, Rust's decoder expects 28 bytes
        // This is a bug/inconsistency in Rust's SSZ implementation, but we must match the decoder
        var rho_canonical: [7]u32 = undefined;
        for (0..7) |i| {
            rho_canonical[i] = self.rho[i].toCanonical();
        }
        log.print("SSZ_DEBUG: sszEncode: Encoding rho[0]=0x{x:0>8} (canonical) / 0x{x:0>8} (Montgomery)\n", .{ rho_canonical[0], self.rho[0].value });

        // Write rho - size depends on rand_len_fe
        // For 2^18: rand_len_fe=6, write 6 field elements (24 bytes)
        // For 2^8/2^32: rand_len_fe=7, write 7 field elements (28 bytes)
        for (0..rand_len) |i| {
            try ssz.serialize(u32, rho_canonical[i], l, allocator);
        }

        // Write hashes offset (absolute offset from start of serialized data)
        const hashes_offset: u32 = @as(u32, @intCast(var_start + path_size));
        try ssz.serialize(u32, hashes_offset, l, allocator);

        // Write path data
        try self.path.sszEncode(l, hash_len_fe, allocator);

        // Write hashes data
        // Hashes are Vec<TH::Domain> where Domain = FieldArray<hash_len_fe>
        // Since Domain is fixed-size, Vec encodes as raw bytes WITHOUT length prefix
        // For 2^18: each hash is 7 field elements (28 bytes)
        // For 2^8/2^32: each hash is 8 field elements (32 bytes)
        for (hashes_canonical) |hash| {
            // Write only hash_len_fe field elements per hash
            for (0..hash_len_fe) |j| {
                try ssz.serialize(u32, hash[j], l, allocator);
            }
        }
    }

    pub fn sszDecode(serialized: []const u8, out: *GeneralizedXMSSSignature, allocator: ?std.mem.Allocator) !void {
        const alloc = allocator orelse return error.AllocatorRequired;

        log.print("SSZ_DEBUG: sszDecode: Starting, serialized len={}, out=0x{x}\n", .{ serialized.len, @intFromPtr(out) });

        // SSZ struct deserialization for GeneralizedXMSSSignature:
        // Field order: path (variable), rho (fixed), hashes (variable)
        // First pass: Read offsets for variable fields
        var offset: usize = 0;

        // Read path offset (4 bytes) - absolute offset from start
        if (serialized.len < offset + 4) return error.InvalidLength;
        const path_offset = std.mem.readInt(u32, @as(*const [4]u8, @ptrCast(&serialized[offset])), .little);
        offset += 4;
        log.print("SSZ_DEBUG: sszDecode: path_offset={}\n", .{path_offset});

        // Read rho - size depends on path_offset
        // For 2^18: path_offset=32 means rho is 24 bytes (6 field elements, rand_len_fe=6)
        // For 2^8/2^32: path_offset=36 means rho is 28 bytes (7 field elements, rand_len_fe=7)
        const rho_size: usize = if (path_offset == 32) 24 else 28;
        const rand_len_decode: usize = if (path_offset == 32) 6 else 7;

        const rho_start: usize = 4; // rho is a fixed field, starts right after path_offset field at offset 0x04
        if (serialized.len < rho_start + rho_size) return error.InvalidLength;
        var rho_canonical: [7]u32 = undefined;
        var rho_offset: usize = rho_start;
        // ssz.deserialize doesn't work correctly for fixed-size arrays, deserialize each u32 individually
        for (0..rand_len_decode) |i| {
            if (serialized.len < rho_offset + 4) return error.InvalidLength;
            var val: u32 = undefined;
            try ssz.deserialize(u32, serialized[rho_offset .. rho_offset + 4], &val, null);
            rho_canonical[i] = val;
            rho_offset += 4;
        }
        // Zero-pad remaining elements if rand_len_decode < 7
        for (rand_len_decode..7) |i| {
            rho_canonical[i] = 0;
        }
        var rho: [7]FieldElement = undefined;
        for (rho_canonical, 0..) |val, i| {
            rho[i] = FieldElement.fromCanonical(val);
        }
        log.print("SSZ_DEBUG: sszDecode: rho decoded, first value=0x{x}, rand_len={}\n", .{ rho[0].toCanonical(), rand_len_decode });

        // Read hashes offset (4 bytes) - position depends on rho_size
        // For 2^18: rho is 24 bytes, so hashes_offset is at offset 28-31
        // For 2^8/2^32: rho is 28 bytes, so hashes_offset is at offset 32-35
        const hashes_offset_pos = rho_start + rho_size;
        if (serialized.len < hashes_offset_pos + 4) return error.InvalidLength;
        const hashes_offset = std.mem.readInt(u32, @as(*const [4]u8, @ptrCast(&serialized[hashes_offset_pos])), .little);
        log.print("SSZ_DEBUG: sszDecode: hashes_offset={} (read from offset {}, path_offset={})\n", .{ hashes_offset, hashes_offset_pos, path_offset });

        // Second pass: Deserialize variable fields using offsets
        // Decode path (absolute offset)
        // Path data is from path_offset to hashes_offset
        if (path_offset > serialized.len) return error.InvalidOffset;
        if (hashes_offset > serialized.len) return error.InvalidOffset;
        if (hashes_offset < path_offset) return error.InvalidOffset;
        const path_data_len = hashes_offset - path_offset;
        if (serialized.len < path_offset + path_data_len) return error.InvalidLength;

        const path = try alloc.create(HashTreeOpening);
        errdefer alloc.destroy(path);
        log.print("SSZ_DEBUG: sszDecode: Created path at 0x{x}, path_data_len={}\n", .{ @intFromPtr(path), path_data_len });

        // Determine hash_len_fe from path_offset: 32 → hash_len_fe=7, 36 → hash_len_fe=8
        const decode_hash_len_fe: usize = if (path_offset == 32) 7 else 8;
        try HashTreeOpening.sszDecode(serialized[path_offset .. path_offset + path_data_len], path, alloc, decode_hash_len_fe);
        log.print("SSZ_DEBUG: sszDecode: Path decoded successfully, nodes len={}\n", .{path.nodes.len});
        // Debug: log first few path nodes to verify decoding
        for (0..@min(8, path.nodes.len)) |i| {
            log.print("SSZ_DEBUG: sszDecode: Path node {}: 0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical)\n", .{ i, path.nodes[i][0].value, path.nodes[i][0].toCanonical() });
        }

        // Decode hashes (absolute offset)
        // Hashes are Vec<TH::Domain> where Domain = FieldArray<hash_len_fe>
        // Since Domain is fixed-size, Vec encodes as raw bytes WITHOUT length prefix
        // For 2^18: each hash is 7 field elements (28 bytes)
        // For 2^8/2^32: each hash is 8 field elements (32 bytes)
        if (hashes_offset > serialized.len) return error.InvalidOffset;
        const hashes_data_len = serialized.len - hashes_offset;
        const hash_size = decode_hash_len_fe * 4;
        if (hashes_data_len % hash_size != 0) return error.InvalidLength;
        const num_hashes = hashes_data_len / hash_size;
        log.print("SSZ_DEBUG: sszDecode: num_hashes={}, hash_size={}\n", .{ num_hashes, hash_size });

        if (num_hashes == 0) {
            log.print("SSZ_DEBUG: sszDecode: Assigning struct with empty hashes\n", .{});
            log.print("SSZ_DEBUG: sszDecode: Before assignment - out=0x{x}, path=0x{x}, rho offset={}\n", .{
                @intFromPtr(out),
                @intFromPtr(path),
                @offsetOf(GeneralizedXMSSSignature, "rho"),
            });
            out.* = GeneralizedXMSSSignature{
                .path = path,
                .rho = rho,
                .hashes = try alloc.alloc([8]FieldElement, 0),
                .allocator = alloc,
                .is_deserialized = true,
                .rand_len_fe = rand_len_decode, // Store decoded rand_len_fe
            };
            log.print("SSZ_DEBUG: sszDecode: Struct assigned, out.path=0x{x}, out.rho[0]=0x{x}\n", .{
                @intFromPtr(out.path),
                out.rho[0].toCanonical(),
            });
            return;
        }

        var hashes_canonical = try alloc.alloc([8]u32, num_hashes);
        defer alloc.free(hashes_canonical);

        var pos = hashes_offset; // No length prefix to skip
        for (0..num_hashes) |i| {
            if (serialized.len < pos + hash_size) return error.InvalidLength;
            // Deserialize hash_len_fe field elements for this hash
            for (0..decode_hash_len_fe) |j| {
                if (serialized.len < pos + 4) return error.InvalidLength;
                var val: u32 = undefined;
                try ssz.deserialize(u32, serialized[pos .. pos + 4], &val, null);
                hashes_canonical[i][j] = val;
                pos += 4;
            }
            // Zero-pad remaining field elements if hash_len_fe < 8
            for (decode_hash_len_fe..8) |j| {
                hashes_canonical[i][j] = 0;
            }
        }

        // Convert back to FieldElement arrays
        var hashes = try alloc.alloc([8]FieldElement, num_hashes);
        for (hashes_canonical, 0..) |hash_canonical, i| {
            for (hash_canonical, 0..) |val, j| {
                hashes[i][j] = FieldElement.fromCanonical(val);
            }
        }
        log.print("SSZ_DEBUG: sszDecode: Hashes converted, len={}\n", .{hashes.len});
        // Debug: log first hash to verify conversion
        if (hashes.len > 0) {
            log.print("SSZ_DEBUG: sszDecode: First hash[0] after decode (Montgomery): 0x{x:0>8}, (Canonical): 0x{x:0>8}\n", .{ hashes[0][0].value, hashes[0][0].toCanonical() });
            // Also log the full first hash for comparison
            log.print("SSZ_DEBUG: sszDecode: First hash full (Montgomery): ", .{});
            for (0..@min(8, hashes[0].len)) |i| {
                log.print("0x{x:0>8} ", .{hashes[0][i].value});
            }
            log.print("\n", .{});
        }

        log.print("SSZ_DEBUG: sszDecode: Before assignment - out=0x{x}, path=0x{x}, rho offset={}, hashes=0x{x}\n", .{
            @intFromPtr(out),
            @intFromPtr(path),
            @offsetOf(GeneralizedXMSSSignature, "rho"),
            @intFromPtr(hashes.ptr),
        });
        log.print("SSZ_DEBUG: sszDecode: Struct size={}, rho offset={}, hashes offset={}\n", .{
            @sizeOf(GeneralizedXMSSSignature),
            @offsetOf(GeneralizedXMSSSignature, "rho"),
            @offsetOf(GeneralizedXMSSSignature, "hashes"),
        });
        out.* = GeneralizedXMSSSignature{
            .path = path,
            .rho = rho,
            .hashes = hashes,
            .allocator = alloc,
            .is_deserialized = true,
            .rand_len_fe = rand_len_decode, // Store decoded rand_len_fe
        };
        log.print("SSZ_DEBUG: sszDecode: Struct assigned successfully\n", .{});
        log.print("SSZ_DEBUG: sszDecode: After assignment - out.path=0x{x}, out.rho[0]=0x{x}, out.hashes len={}\n", .{
            @intFromPtr(out.path),
            out.rho[0].toCanonical(),
            out.hashes.len,
        });
        log.print("SSZ_DEBUG: sszDecode: out.allocator address=0x{x}\n", .{@intFromPtr(&out.allocator)});
        // Verify struct is still accessible after assignment
        const test_path = out.path;
        const test_rho = out.rho;
        const test_hashes = out.hashes;
        log.print("SSZ_DEBUG: sszDecode: Verification - path=0x{x}, rho[0]=0x{x}, hashes len={}\n", .{
            @intFromPtr(test_path),
            test_rho[0].toCanonical(),
            test_hashes.len,
        });
    }

    pub fn isFixedSizeObject(comptime T: type) bool {
        _ = T;
        return false; // Variable size (path and hashes are variable-length)
    }

    /// Serialize to SSZ bytes (convenience method matching Rust's to_bytes)
    pub fn toBytes(self: *const GeneralizedXMSSSignature, allocator: std.mem.Allocator) ![]u8 {
        var encoded: std.ArrayList(u8) = .{};
        errdefer encoded.deinit(allocator);
        try self.sszEncode(&encoded, allocator);
        return encoded.toOwnedSlice(allocator);
    }

    /// Deserialize from SSZ bytes (convenience method matching Rust's from_bytes)
    pub fn fromBytes(serialized: []const u8, allocator: std.mem.Allocator) !*GeneralizedXMSSSignature {
        log.print("SSZ_DEBUG: fromBytes: Creating struct, serialized len={}\n", .{serialized.len});
        const decoded = try allocator.create(GeneralizedXMSSSignature);
        errdefer allocator.destroy(decoded);
        log.print("SSZ_DEBUG: fromBytes: Struct created at address 0x{x}, size={}\n", .{ @intFromPtr(decoded), @sizeOf(GeneralizedXMSSSignature) });
        // Zero-initialize the struct to ensure it's in a valid state
        @memset(@as([*]u8, @ptrCast(decoded))[0..@sizeOf(GeneralizedXMSSSignature)], 0);
        log.print("SSZ_DEBUG: fromBytes: Struct zero-initialized\n", .{});
        try sszDecode(serialized, decoded, allocator);
        log.print("SSZ_DEBUG: fromBytes: sszDecode completed successfully\n", .{});
        log.print("SSZ_DEBUG: fromBytes: Final struct - path=0x{x}, rho offset={}, hashes len={}\n", .{
            @intFromPtr(decoded.path),
            @offsetOf(GeneralizedXMSSSignature, "rho"),
            decoded.hashes.len,
        });
        // Validate struct is properly initialized by accessing all fields
        _ = decoded.path;
        _ = decoded.rho;
        _ = decoded.hashes;
        _ = decoded.allocator;
        _ = decoded.is_deserialized;
        log.print("SSZ_DEBUG: fromBytes: Struct validation passed, returning\n", .{});
        return decoded;
    }
};

// Public key structure matching Rust exactly
pub const GeneralizedXMSSPublicKey = struct {
    // Private fields - not directly accessible from outside
    root: [8]FieldElement, // Root stored as 8 field elements (last element may be padding)
    parameter: [5]FieldElement, // TH::Parameter
    /// Active hash length in field elements (7 for lifetime 2^18, 8 for 2^8/2^32)
    hash_len_fe: usize,

    pub fn init(root: [8]FieldElement, parameter: [5]FieldElement, hash_len_fe: usize) GeneralizedXMSSPublicKey {
        return GeneralizedXMSSPublicKey{
            .root = root,
            .parameter = parameter,
            .hash_len_fe = hash_len_fe,
        };
    }

    // Controlled access methods for private fields
    pub fn getRoot(self: *const GeneralizedXMSSPublicKey) [8]FieldElement {
        return self.root;
    }

    pub fn getHashLenFe(self: *const GeneralizedXMSSPublicKey) usize {
        return self.hash_len_fe;
    }

    pub fn getParameter(self: *const GeneralizedXMSSPublicKey) [5]FieldElement {
        return self.parameter;
    }

    // Serialization method using controlled access
    pub fn serialize(self: *const GeneralizedXMSSPublicKey, allocator: std.mem.Allocator) ![]u8 {
        return serialization.serializePublicKey(allocator, self);
    }

    // SSZ serialization methods
    pub fn sszEncode(self: *const GeneralizedXMSSPublicKey, l: *std.ArrayList(u8), allocator: std.mem.Allocator) !void {
        // Convert root to canonical u32 array and serialize
        // Rust encodes root based on hash_len_fe, not always 8 u32s
        // For lifetime 2^18 (hash_len_fe=7), encode as [7]u32 (28 bytes)
        // For lifetime 2^8/2^32 (hash_len_fe=8), encode as [8]u32 (32 bytes)
        const hash_len = self.hash_len_fe;
        if (hash_len == 7) {
            var root_canonical: [7]u32 = undefined;
            for (0..7) |i| {
                root_canonical[i] = self.root[i].toCanonical();
            }
            try ssz.serialize([7]u32, root_canonical, l, allocator);
        } else {
            var root_canonical: [8]u32 = undefined;
            for (0..8) |i| {
                root_canonical[i] = self.root[i].toCanonical();
            }
            try ssz.serialize([8]u32, root_canonical, l, allocator);
        }

        // Convert parameter to canonical u32 array and serialize
        var param_canonical: [5]u32 = undefined;
        for (self.parameter, 0..) |fe, i| {
            param_canonical[i] = fe.toCanonical();
        }
        try ssz.serialize([5]u32, param_canonical, l, allocator);
    }

    pub fn sszDecode(serialized: []const u8, out: *GeneralizedXMSSPublicKey, allocator: ?std.mem.Allocator) !void {
        _ = allocator; // Not needed for fixed-size types

        // Decode root - Rust encodes based on hash_len_fe
        // For lifetime 2^18 (hash_len_fe=7), root is [7]u32 (28 bytes)
        // For lifetime 2^8/2^32 (hash_len_fe=8), root is [8]u32 (32 bytes)
        // We need to determine hash_len_fe from the serialized data length
        // Total size is 48 bytes (2^18) or 52 bytes (2^8/2^32): root + parameter (20 bytes)
        // So root size = serialized.len - 20
        const root_size = serialized.len - 20;
        if (root_size != 28 and root_size != 32) return error.InvalidLength;
        const hash_len: usize = if (root_size == 28) 7 else 8;

        // ssz.deserialize doesn't work correctly for fixed-size arrays, deserialize each u32 individually
        var root_canonical: [8]u32 = undefined;
        var root_offset: usize = 0;
        for (0..hash_len) |i| {
            if (serialized.len < root_offset + 4) return error.InvalidLength;
            // Direct little-endian read instead of ssz.deserialize which may have issues
            const bytes = serialized[root_offset .. root_offset + 4];
            const val = std.mem.readInt(u32, bytes[0..4], .little);
            root_canonical[i] = val;
            root_offset += 4;
        }
        // Zero-pad remaining elements if hash_len < 8
        for (hash_len..8) |i| {
            root_canonical[i] = 0;
        }
        var root: [8]FieldElement = undefined;
        for (root_canonical, 0..) |val, i| {
            root[i] = FieldElement.fromCanonical(val);
        }

        // Decode parameter (20 bytes for 5 u32s) - starts after root
        // ssz.deserialize doesn't work correctly for fixed-size arrays, deserialize each u32 individually
        const param_offset = root_size; // Parameter starts after root
        if (serialized.len < param_offset + 20) return error.InvalidLength;
        var param_canonical: [5]u32 = undefined;
        var param_pos = param_offset;
        for (0..5) |i| {
            var val: u32 = undefined;
            try ssz.deserialize(u32, serialized[param_pos .. param_pos + 4], &val, null);
            param_canonical[i] = val;
            param_pos += 4;
        }
        // Debug: log decoded parameter values
        log.print("SSZ_DEBUG: sszDecode PublicKey: param_canonical decoded: ", .{});
        for (0..5) |i| {
            log.print("0x{x:0>8} ", .{param_canonical[i]});
        }
        log.print("\n", .{});
        var parameter: [5]FieldElement = undefined;
        for (param_canonical, 0..) |val, i| {
            parameter[i] = FieldElement.fromCanonical(val);
        }
        // Debug: log parameter after conversion to FieldElement
        log.print("SSZ_DEBUG: sszDecode PublicKey: parameter after conversion (canonical): ", .{});
        for (0..5) |i| {
            log.print("0x{x:0>8} ", .{parameter[i].toCanonical()});
        }
        log.print("\n", .{});

        // Determine hash_len_fe from root (count non-zero elements, or use 8 if all non-zero)
        var hash_len_fe: usize = 8;
        for (root, 0..) |fe, i| {
            if (fe.isZero() and i > 0) {
                hash_len_fe = i;
                break;
            }
        }

        out.* = GeneralizedXMSSPublicKey.init(root, parameter, hash_len_fe);
    }

    pub fn isFixedSizeObject(comptime T: type) bool {
        _ = T;
        return true; // 32 + 20 = 52 bytes
    }

    /// Serialize to SSZ bytes (convenience method matching Rust's to_bytes)
    pub fn toBytes(self: *const GeneralizedXMSSPublicKey, allocator: std.mem.Allocator) ![]u8 {
        var encoded: std.ArrayList(u8) = .{};
        errdefer encoded.deinit(allocator);
        try self.sszEncode(&encoded, allocator);
        return encoded.toOwnedSlice(allocator);
    }

    /// Deserialize from SSZ bytes (convenience method matching Rust's from_bytes)
    pub fn fromBytes(serialized: []const u8, allocator: ?std.mem.Allocator) !GeneralizedXMSSPublicKey {
        var decoded: GeneralizedXMSSPublicKey = undefined;
        try sszDecode(serialized, &decoded, allocator);
        return decoded;
    }
};

// Secret key structure matching Rust exactly
pub const GeneralizedXMSSSecretKey = struct {
    // Private fields - not directly accessible from outside
    prf_key: [32]u8, // PRF::Key
    parameter: [5]FieldElement, // TH::Parameter
    activation_epoch: usize,
    num_active_epochs: usize,
    top_tree: *HashSubTree,
    left_bottom_tree_index: usize,
    left_bottom_tree: *HashSubTree,
    right_bottom_tree: *HashSubTree,
    allocator: std.mem.Allocator,

    pub fn init(
        allocator: std.mem.Allocator,
        prf_key: [32]u8,
        _parameter: [5]FieldElement,
        activation_epoch: usize,
        num_active_epochs: usize,
        top_tree: *HashSubTree,
        left_bottom_tree_index: usize,
        left_bottom_tree: *HashSubTree,
        right_bottom_tree: *HashSubTree,
    ) !*GeneralizedXMSSSecretKey {
        const self = try allocator.create(GeneralizedXMSSSecretKey);
        self.* = GeneralizedXMSSSecretKey{
            .prf_key = prf_key,
            .parameter = _parameter,
            .activation_epoch = activation_epoch,
            .num_active_epochs = num_active_epochs,
            .top_tree = top_tree,
            .left_bottom_tree_index = left_bottom_tree_index,
            .left_bottom_tree = left_bottom_tree,
            .right_bottom_tree = right_bottom_tree,
            .allocator = allocator,
        };
        return self;
    }

    pub fn deinit(self: *GeneralizedXMSSSecretKey) void {
        self.top_tree.deinit();
        self.left_bottom_tree.deinit();
        self.right_bottom_tree.deinit();
        self.allocator.destroy(self);
    }

    // Controlled access methods for private fields
    pub fn getActivationEpoch(self: *const GeneralizedXMSSSecretKey) usize {
        return self.activation_epoch;
    }

    pub fn getNumActiveEpochs(self: *const GeneralizedXMSSSecretKey) usize {
        return self.num_active_epochs;
    }

    pub fn getLeftBottomTreeIndex(self: *const GeneralizedXMSSSecretKey) usize {
        return self.left_bottom_tree_index;
    }

    // Note: These methods expose sensitive data for serialization
    // In a production system, you might want to restrict access to these
    pub fn getPrfKey(self: *const GeneralizedXMSSSecretKey) [32]u8 {
        return self.prf_key;
    }

    pub fn getParameter(self: *const GeneralizedXMSSSecretKey) [5]FieldElement {
        return self.parameter;
    }

    // SSZ serialization methods
    pub fn sszEncode(self: *const GeneralizedXMSSSecretKey, l: *std.ArrayList(u8), allocator: std.mem.Allocator) !void {
        // Full leansig-compatible encoding with trees
        // Format: [prf_key:32][parameter:20][activation_epoch:8][num_active_epochs:8]
        //         [top_tree_offset:4][left_bottom_tree_index:8][left_bottom_tree_offset:4][right_bottom_tree_offset:4]
        //         [top_tree_data][left_bottom_tree_data][right_bottom_tree_data]

        // Encode fixed-size fields
        try ssz.serialize([32]u8, self.prf_key, l, allocator);

        // Convert parameter to canonical u32 array and serialize (20 bytes for 5 u32s)
        var param_canonical: [5]u32 = undefined;
        for (self.parameter, 0..) |fe, i| {
            param_canonical[i] = fe.toCanonical();
        }
        try ssz.serialize([5]u32, param_canonical, l, allocator);

        // Encode activation_epoch as u64 (8 bytes)
        try ssz.serialize(u64, @as(u64, @intCast(self.activation_epoch)), l, allocator);

        // Encode num_active_epochs as u64 (8 bytes)
        try ssz.serialize(u64, @as(u64, @intCast(self.num_active_epochs)), l, allocator);

        // Now we're at offset 68 (32+20+8+8)
        // Encode offsets for variable-size fields
        const fixed_part_end: u32 = 88; // 68 + 4 + 8 + 4 + 4 = 88

        // Serialize top_tree to get its size
        var top_tree_bytes: std.ArrayList(u8) = .{};
        defer top_tree_bytes.deinit(self.allocator);
        try serializeHashSubTree(self.top_tree, &top_tree_bytes, allocator);

        // Serialize left_bottom_tree to get its size
        var left_bottom_tree_bytes: std.ArrayList(u8) = .{};
        defer left_bottom_tree_bytes.deinit(self.allocator);
        try serializeHashSubTree(self.left_bottom_tree, &left_bottom_tree_bytes, allocator);

        // Serialize right_bottom_tree to get its size
        var right_bottom_tree_bytes: std.ArrayList(u8) = .{};
        defer right_bottom_tree_bytes.deinit(self.allocator);
        try serializeHashSubTree(self.right_bottom_tree, &right_bottom_tree_bytes, allocator);

        // Write offsets
        const top_tree_offset = fixed_part_end;
        const left_bottom_tree_offset = top_tree_offset + @as(u32, @intCast(top_tree_bytes.items.len));
        const right_bottom_tree_offset = left_bottom_tree_offset + @as(u32, @intCast(left_bottom_tree_bytes.items.len));

        try ssz.serialize(u32, top_tree_offset, l, allocator);
        try ssz.serialize(u64, @as(u64, @intCast(self.left_bottom_tree_index)), l, allocator);
        try ssz.serialize(u32, left_bottom_tree_offset, l, allocator);
        try ssz.serialize(u32, right_bottom_tree_offset, l, allocator);

        // Write tree data
        try l.appendSlice(allocator, top_tree_bytes.items);
        try l.appendSlice(allocator, left_bottom_tree_bytes.items);
        try l.appendSlice(allocator, right_bottom_tree_bytes.items);
    }

    pub fn sszDecode(serialized: []const u8, out: *GeneralizedXMSSSecretKey, allocator: ?std.mem.Allocator) !void {
        const alloc = allocator orelse return error.AllocatorRequired;

        if (serialized.len < 88) return error.InvalidLength;

        var offset: usize = 0;

        // Decode prf_key (32 bytes)
        var prf_key: [32]u8 = undefined;
        @memcpy(&prf_key, serialized[offset .. offset + 32]);
        offset += 32;

        // Decode parameter (20 bytes for 5 u32s)
        var param_canonical: [5]u32 = undefined;
        for (0..5) |i| {
            param_canonical[i] = std.mem.readInt(u32, serialized[offset .. offset + 4][0..4], .little);
            offset += 4;
        }
        var parameter: [5]FieldElement = undefined;
        for (param_canonical, 0..) |val, i| {
            parameter[i] = FieldElement.fromCanonical(val);
        }

        // Decode activation_epoch (u64)
        const activation_epoch = std.mem.readInt(u64, serialized[offset .. offset + 8][0..8], .little);
        offset += 8;

        // Decode num_active_epochs (u64)
        const num_active_epochs = std.mem.readInt(u64, serialized[offset .. offset + 8][0..8], .little);
        offset += 8;

        // Decode offsets for variable fields
        const top_tree_offset = std.mem.readInt(u32, serialized[offset .. offset + 4][0..4], .little);
        offset += 4;

        const left_bottom_tree_index = std.mem.readInt(u64, serialized[offset .. offset + 8][0..8], .little);
        offset += 8;

        const left_bottom_tree_offset = std.mem.readInt(u32, serialized[offset .. offset + 4][0..4], .little);
        offset += 4;

        const right_bottom_tree_offset = std.mem.readInt(u32, serialized[offset .. offset + 4][0..4], .little);
        offset += 4;

        // Now offset should be 88, which matches top_tree_offset
        if (offset != top_tree_offset) return error.InvalidSSZStructure;

        // Deserialize top_tree
        const top_tree = try deserializeHashSubTree(alloc, serialized[top_tree_offset..left_bottom_tree_offset]);
        errdefer top_tree.deinit();

        // Deserialize left_bottom_tree
        const left_bottom_tree = deserializeHashSubTree(alloc, serialized[left_bottom_tree_offset..right_bottom_tree_offset]) catch |err| {
            top_tree.deinit();
            return err;
        };
        errdefer left_bottom_tree.deinit();

        // Deserialize right_bottom_tree
        const right_bottom_tree = deserializeHashSubTree(alloc, serialized[right_bottom_tree_offset..]) catch |err| {
            left_bottom_tree.deinit();
            top_tree.deinit();
            return err;
        };

        // Initialize the secret key
        out.* = GeneralizedXMSSSecretKey{
            .prf_key = prf_key,
            .parameter = parameter,
            .activation_epoch = @intCast(activation_epoch),
            .num_active_epochs = @intCast(num_active_epochs),
            .top_tree = top_tree,
            .left_bottom_tree_index = @intCast(left_bottom_tree_index),
            .left_bottom_tree = left_bottom_tree,
            .right_bottom_tree = right_bottom_tree,
            .allocator = alloc,
        };
    }

    pub fn isFixedSizeObject(comptime T: type) bool {
        _ = T;
        return true; // 32 + 20 + 8 + 8 = 68 bytes
    }

    /// Serialize to SSZ bytes (convenience method matching Rust's to_bytes)
    /// Note: Only serializes prf_key, parameter, and epochs. Trees are not serialized.
    pub fn toBytes(self: *const GeneralizedXMSSSecretKey, allocator: std.mem.Allocator) ![]u8 {
        var encoded: std.ArrayList(u8) = .{};
        errdefer encoded.deinit(allocator);
        try self.sszEncode(&encoded, allocator);
        return encoded.toOwnedSlice(allocator);
    }

    /// Deserialize from SSZ bytes (convenience method matching Rust's from_bytes)
    /// Note: Returns error.SecretKeyRequiresKeyGen since trees cannot be deserialized.
    /// The caller must use keyGen to reconstruct the full secret key.
    pub fn fromBytes(serialized: []const u8, allocator: ?std.mem.Allocator) !void {
        var dummy: GeneralizedXMSSSecretKey = undefined;
        try sszDecode(serialized, &dummy, allocator);
    }

    // Serialization method using controlled access
    pub fn serialize(self: *const GeneralizedXMSSSecretKey, allocator: std.mem.Allocator) ![]u8 {
        return serialization.serializeSecretKey(allocator, self);
    }

    /// Get activation interval (matching Rust get_activation_interval)
    pub fn getActivationInterval(self: *const GeneralizedXMSSSecretKey) struct { start: u64, end: u64 } {
        const start = @as(u64, @intCast(self.activation_epoch));
        const end = start + @as(u64, @intCast(self.num_active_epochs));
        return .{ .start = start, .end = end };
    }

    /// Get prepared interval (matching Rust get_prepared_interval)
    pub fn getPreparedInterval(self: *const GeneralizedXMSSSecretKey, log_lifetime: usize) struct { start: u64, end: u64 } {
        const leafs_per_bottom_tree = @as(usize, 1) << @intCast(log_lifetime / 2);
        const start = @as(u64, @intCast(self.left_bottom_tree_index * leafs_per_bottom_tree));
        const end = start + @as(u64, @intCast(2 * leafs_per_bottom_tree));
        return .{ .start = start, .end = end };
    }

    /// Advance preparation (matching Rust advance_preparation exactly)
    /// Note: This method needs access to the scheme to compute bottom trees
    pub fn advancePreparation(self: *GeneralizedXMSSSecretKey, scheme: *GeneralizedXMSSSignatureScheme, log_lifetime: usize) !void {
        const leafs_per_bottom_tree = @as(usize, 1) << @intCast(log_lifetime / 2);
        const next_prepared_end_epoch = self.left_bottom_tree_index * leafs_per_bottom_tree + 3 * leafs_per_bottom_tree;

        // Match Rust: compare with get_activation_interval().end
        const activation_interval = self.getActivationInterval();
        if (@as(u64, @intCast(next_prepared_end_epoch)) > activation_interval.end) {
            return; // Cannot advance
        }

        // Compute new right bottom tree using scheme's method (matching Rust bottom_tree_from_prf_key)
        const new_right_bottom_tree = try scheme.bottomTreeFromPrfKey(self.prf_key, self.left_bottom_tree_index + 2, self.parameter);

        // Clean up the old left bottom tree before replacing it
        self.left_bottom_tree.deinit();

        // Move right to left and update index (matching Rust exactly)
        self.left_bottom_tree = self.right_bottom_tree;
        self.right_bottom_tree = new_right_bottom_tree;
        self.left_bottom_tree_index += 1;
    }
};

// Main GeneralizedXMSS Signature Scheme
pub const GeneralizedXMSSSignatureScheme = struct {
    lifetime_params: LifetimeParams,
    poseidon2: *Poseidon2RustCompat,
    allocator: std.mem.Allocator,
    rng: ChaCha12Rng,
    layer_cache: std.HashMap(usize, poseidon_top_level.AllLayerInfoForBase, std.hash_map.AutoContext(usize), std.hash_map.default_max_load_percentage),
    layer_cache_mutex: std.Thread.Mutex,
    bottom_tree_cache: BottomTreeCache,
    rng_mutex: std.Thread.Mutex, // Mutex for thread-safe RNG access during parallel tree generation

    pub fn init(allocator: std.mem.Allocator, lifetime: @import("../../core/params_rust_compat.zig").KeyLifetime) !*GeneralizedXMSSSignatureScheme {
        const poseidon2 = try Poseidon2RustCompat.init(allocator);

        // Select the correct lifetime parameters (only 3 lifetimes supported: 2^8, 2^18, 2^32)
        const lifetime_params = switch (lifetime) {
            .lifetime_2_8 => LIFETIME_2_8_PARAMS,
            .lifetime_2_18 => LIFETIME_2_18_PARAMS,
            .lifetime_2_32 => LIFETIME_2_32_HASHING_PARAMS,
        };

        const bottom_tree_cache = try BottomTreeCache.init(allocator);
        const self = try allocator.create(GeneralizedXMSSSignatureScheme);
        self.* = GeneralizedXMSSSignatureScheme{
            .lifetime_params = lifetime_params,
            .poseidon2 = try allocator.create(Poseidon2RustCompat),
            .allocator = allocator,
            .rng = ChaCha12Rng.init(initDefaultSeed()),
            .layer_cache = std.HashMap(usize, poseidon_top_level.AllLayerInfoForBase, std.hash_map.AutoContext(usize), std.hash_map.default_max_load_percentage).init(allocator),
            .layer_cache_mutex = .{},
            .bottom_tree_cache = bottom_tree_cache,
            .rng_mutex = .{},
        };

        self.poseidon2.* = poseidon2;

        return self;
    }

    pub fn initWithSeed(allocator: std.mem.Allocator, lifetime: @import("../../core/params_rust_compat.zig").KeyLifetime, seed: [32]u8) !*GeneralizedXMSSSignatureScheme {
        const poseidon2 = try Poseidon2RustCompat.init(allocator);
        // Select the correct lifetime parameters (only 3 lifetimes supported: 2^8, 2^18, 2^32)
        const lifetime_params = switch (lifetime) {
            .lifetime_2_8 => LIFETIME_2_8_PARAMS,
            .lifetime_2_18 => LIFETIME_2_18_PARAMS,
            .lifetime_2_32 => LIFETIME_2_32_HASHING_PARAMS,
        };
        const bottom_tree_cache = try BottomTreeCache.init(allocator);
        const self = try allocator.create(GeneralizedXMSSSignatureScheme);
        self.* = GeneralizedXMSSSignatureScheme{
            .lifetime_params = lifetime_params,
            .poseidon2 = try allocator.create(Poseidon2RustCompat),
            .allocator = allocator,
            .rng = ChaCha12Rng.init(seed),
            .layer_cache = std.HashMap(usize, poseidon_top_level.AllLayerInfoForBase, std.hash_map.AutoContext(usize), std.hash_map.default_max_load_percentage).init(allocator),
            .layer_cache_mutex = .{},
            .bottom_tree_cache = bottom_tree_cache,
            .rng_mutex = .{},
        };
        self.poseidon2.* = poseidon2;
        return self;
    }

    fn initDefaultSeed() [32]u8 {
        var seed: [32]u8 = undefined;
        const now = @as(u64, @intCast(std.time.timestamp()));
        // Expand timestamp into 32 bytes deterministically
        var tmp = now;
        var i: usize = 0;
        while (i < 32) : (i += 1) {
            tmp = tmp ^ (tmp << 13) ^ (tmp >> 7) ^ (tmp << 17);
            seed[i] = @as(u8, @truncate(tmp >> @intCast((i & 7) * 8)));
        }
        return seed;
    }

    pub fn deinit(self: *GeneralizedXMSSSignatureScheme) void {
        // Clean up layer cache
        var it = self.layer_cache.iterator();
        while (it.next()) |entry| {
            entry.value_ptr.deinit();
        }
        self.layer_cache.deinit();
        self.bottom_tree_cache.deinit();

        self.allocator.destroy(self.poseidon2);
        self.allocator.destroy(self);
    }

    /// OPTIMIZATION: Mark as inline since this is called very frequently in hot loops
    pub inline fn prfDomainElement(
        self: *const GeneralizedXMSSSignatureScheme,
        prf_key: [32]u8,
        epoch: u32,
        index: u64,
    ) [8]u32 {
        const hash_len = self.lifetime_params.hash_len_fe;
        var padded: [8]u32 = undefined;
        if (self.lifetime_params.rand_len_fe == 6 and hash_len == 7) {
            const raw = ShakePRFtoF_7_6.getDomainElement(prf_key, epoch, index);
            for (0..hash_len) |i| {
                padded[i] = raw[i];
            }
        } else {
            const raw = ShakePRFtoF_8_7.getDomainElement(prf_key, epoch, index);
            for (0..hash_len) |i| {
                padded[i] = raw[i];
            }
        }
        for (hash_len..8) |i| {
            padded[i] = 0;
        }
        return padded;
    }

    /// Expand activation time (matching Rust expand_activation_time exactly)
    fn expandActivationTime(log_lifetime: usize, desired_activation_epoch: usize, desired_num_active_epochs: usize) struct { start: usize, end: usize } {
        const lifetime = @as(usize, 1) << @intCast(log_lifetime);
        const c = @as(usize, 1) << @intCast(log_lifetime / 2);
        const c_mask = ~(c - 1);

        const desired_start = desired_activation_epoch;
        const desired_end = desired_activation_epoch + desired_num_active_epochs;

        // 1. Align start downward to multiple of C
        var start = desired_start & c_mask;

        // 2. Round end upward to multiple of C
        var end = (desired_end + c - 1) & c_mask;

        // 3. Enforce minimum duration of 2*C
        if (end - start < 2 * c) {
            end = start + 2 * c;
        }

        // 4. If interval exceeds lifetime, shift left to fit
        if (end > lifetime) {
            const duration = end - start;
            if (duration > lifetime) {
                start = 0;
                end = lifetime;
            } else {
                end = lifetime;
                start = (lifetime - duration) & c_mask;
            }
        }

        // Divide by c to get bottom tree indices
        start >>= @intCast(log_lifetime / 2);
        end >>= @intCast(log_lifetime / 2);

        return .{ .start = start, .end = end };
    }

    /// Bottom tree from PRF key (matching Rust bottom_tree_from_prf_key exactly)
    ///
    /// Note: This matches the structure from leanSig PR #5 (simd: apply packing for tree leaves).
    /// The Rust implementation uses SIMD packing via Plonky3's Packing trait to optimize
    /// tree leaf computation. Future optimizations can use Zig's @Vector for SIMD operations.
    pub fn bottomTreeFromPrfKey(
        self: *GeneralizedXMSSSignatureScheme,
        prf_key: [32]u8,
        bottom_tree_index: usize,
        parameter: [5]FieldElement,
    ) !*HashSubTree {
        // Debug: For bottom tree 0, log the inputs (only when debug logs are enabled)
        if (build_opts.enable_debug_logs and bottom_tree_index == 0) {
            log.print("ZIG_TREEBUILD_INPUTS: bottom_tree_index=0, prf_key[0..8]=", .{});
            for (prf_key[0..8]) |b| log.print("{x:0>2}", .{b});
            log.print(", parameter[0]=0x{x:0>8} (canonical: 0x{x:0>8})\n", .{ parameter[0].value, parameter[0].toCanonical() });
        }
        const num_chains = self.lifetime_params.dimension;
        _ = self.lifetime_params.base; // chain_length unused for now

        // OPTIMIZATION: Profile cache and computation times
        const profile_keygen = @hasDecl(build_opts, "enable_profile_keygen") and build_opts.enable_profile_keygen;
        var cache_timer: std.time.Timer = undefined;
        var leaf_timer: std.time.Timer = undefined;
        var tree_timer: std.time.Timer = undefined;
        var cache_time_ns: u64 = 0;
        var leaf_time_ns: u64 = 0;
        var tree_time_ns: u64 = 0;
        if (profile_keygen) {
            cache_timer = try std.time.Timer.start();
        }

        // Cache control: allow cache by default, can be disabled via environment
        // HASH_ZIG_DISABLE_BT_CACHE or by setting bottom_tree_cache.enabled = false.
        const force_disable_cache = false;
        if (self.bottom_tree_cache.enabled and !force_disable_cache) {
            if (build_opts.enable_debug_logs) {
                log.print("ZIG_TREEBUILD_DEBUG: Cache enabled, attempting to load bottom tree {}\n", .{bottom_tree_index});
            }
            const cached = self.bottom_tree_cache.load(
                self.allocator,
                self.lifetime_params.log_lifetime,
                prf_key,
                parameter,
                bottom_tree_index,
            ) catch |err| blk: {
                log.print("ZIG_CACHE: failed to load bottom tree {}: {s}\n", .{ bottom_tree_index, @errorName(err) });
                break :blk null;
            };
            if (profile_keygen) {
                cache_time_ns = cache_timer.read();
            }
            if (cached) |tree| {
                if (build_opts.enable_debug_logs) {
                    log.print("ZIG_TREEBUILD_DEBUG: Using cached bottom tree {}\n", .{bottom_tree_index});
                }
                if (profile_keygen) {
                    const cache_sec = @as(f64, @floatFromInt(cache_time_ns)) / 1_000_000_000.0;
                    log.print("PROFILE_BTREE: tree={} cache_hit={d:.3}ms\n", .{ bottom_tree_index, cache_sec * 1000.0 });
                }
                return tree;
            }
            if (build_opts.enable_debug_logs) {
                log.print("ZIG_TREEBUILD_DEBUG: Cache miss, building bottom tree {}\n", .{bottom_tree_index});
            }
        } else if (force_disable_cache) {
            if (build_opts.enable_debug_logs) {
                log.print("ZIG_TREEBUILD_DEBUG: Cache disabled, building bottom tree {}\n", .{bottom_tree_index});
            }
        }
        if (profile_keygen) {
            cache_time_ns = cache_timer.read();
            leaf_timer = try std.time.Timer.start();
        }

        // Calculate leaves per bottom tree
        const leafs_per_bottom_tree = @as(usize, 1) << @intCast(self.lifetime_params.log_lifetime / 2);

        // Calculate epoch range for this bottom tree
        const epoch_range_start = bottom_tree_index * leafs_per_bottom_tree;
        const epoch_range_end = epoch_range_start + leafs_per_bottom_tree;

        // Generate leaf domains (8-wide) for each epoch
        // Use parallel processing for large bottom trees (matching Rust behavior)
        var leaf_domains = try self.allocator.alloc([8]FieldElement, leafs_per_bottom_tree);
        defer self.allocator.free(leaf_domains);

        const num_cpus = std.Thread.getCpuCount() catch 1;
        // OPTIMIZATION: Increased threshold to reduce thread creation overhead for small workloads
        // Only use parallel processing when there's enough work to justify thread overhead
        const min_parallel_leaves = 256; // Increased from 128 to reduce overhead

        if (leafs_per_bottom_tree < min_parallel_leaves or num_cpus <= 1) {
            // Sequential processing for small workloads
            if (build_opts.enable_debug_logs) {
                log.print("ZIG_TREEBUILD_DEBUG: Entering sequential path, epoch_range={}..{}\n", .{ epoch_range_start, epoch_range_end });
            }
            for (epoch_range_start..epoch_range_end) |epoch| {
                // Debug: For epoch 1, verify the epoch value
                if (build_opts.enable_debug_logs and epoch == 1 and bottom_tree_index == 0) {
                    log.print("ZIG_TREEBUILD_DEBUG: Processing epoch 1, bottom_tree_index=0, epoch_range_start={}, epoch_range_end={}\n", .{ epoch_range_start, epoch_range_end });
                }
                // Generate chain end domains (8-wide) for this epoch
                var chain_domains = try self.allocator.alloc([8]FieldElement, num_chains);
                defer self.allocator.free(chain_domains);

                // SIMD-optimized chain computation: process chains in batches of 4
                // This matches Rust's SIMD packing approach from PR #5
                const simd_batch_size = 4;
                var chain_idx: usize = 0;
                while (chain_idx < num_chains) {
                    const batch_end = @min(chain_idx + simd_batch_size, num_chains);
                    const batch_size = batch_end - chain_idx;

                    // Process chains in SIMD batches where possible
                    if (batch_size == simd_batch_size) {
                        // Full SIMD batch - process 4 chains in parallel
                        var batch_domains: [4][8]FieldElement = undefined;
                        for (0..simd_batch_size) |i| {
                            const chain_index = chain_idx + i;
                            const epoch_u32 = @as(u32, @intCast(epoch));
                            const domain_elements = self.prfDomainElement(prf_key, epoch_u32, @as(u64, @intCast(chain_index)));
                            // Debug: For epoch 1, chain 0, log the inputs and PRF computation
                            if (build_opts.enable_debug_logs and epoch == 1 and chain_index == 0 and bottom_tree_index == 0) {
                                // Use log.print for visibility in release builds
                                log.print("ZIG_TREEBUILD_SCALAR: Epoch 1, chain 0: prf_key[0..8]=", .{});
                                for (prf_key[0..8]) |b| log.print("{x:0>2}", .{b});
                                log.print(", epoch={} (u32={}), chain_index={}, domain_elements[0]=0x{x:0>8}, parameter[0]=0x{x:0>8} (canonical: 0x{x:0>8})\n", .{ epoch, epoch_u32, chain_index, domain_elements[0], parameter[0].value, parameter[0].toCanonical() });
                            }
                            batch_domains[i] = try self.computeHashChainDomain(domain_elements, epoch_u32, @as(u8, @intCast(chain_index)), parameter);
                            // Debug: For epoch 1, chain 0, log the result
                            if (build_opts.enable_debug_logs and epoch == 1 and chain_index == 0 and bottom_tree_index == 0) {
                                log.print("ZIG_TREEBUILD_SCALAR: Epoch 1, chain 0 result: batch_domains[{}][0]=0x{x:0>8}\n", .{ i, batch_domains[i][0].value });
                            }
                        }
                        // Copy batch results
                        for (0..simd_batch_size) |i| {
                            chain_domains[chain_idx + i] = batch_domains[i];
                        }
                    } else {
                        // Partial batch - process remaining chains sequentially
                        for (chain_idx..batch_end) |chain_index| {
                            const domain_elements = self.prfDomainElement(prf_key, @as(u32, @intCast(epoch)), @as(u64, @intCast(chain_index)));
                            chain_domains[chain_index] = try self.computeHashChainDomain(domain_elements, @as(u32, @intCast(epoch)), @as(u8, @intCast(chain_index)), parameter);
                        }
                    }
                    chain_idx = batch_end;
                }

                // Reduce chain domains to a single leaf domain using tree-tweak hashing
                const hash_len = self.lifetime_params.hash_len_fe;

                // Debug: For epoch 0, log first chain domain at position base_minus_one
                if (epoch == 0 and bottom_tree_index == 0) {
                    log.debugPrint("ZIG_TREEBUILD_DEBUG: Epoch 0 chain 0 final domain at position {} (Montgomery): ", .{self.lifetime_params.base - 1});
                    for (0..hash_len) |h| {
                        log.debugPrint("0x{x:0>8} ", .{chain_domains[0][h].value});
                    }
                    log.debugPrint("\n", .{});
                }

                // OPTIMIZATION: Use stack-allocated buffer for leaf domain output
                var leaf_domain_buffer: [8]FieldElement = undefined;
                try self.reduceChainDomainsToLeafDomain(chain_domains, parameter, @as(u32, @intCast(epoch)), &leaf_domain_buffer);
                const leaf_domain_slice = leaf_domain_buffer[0..hash_len];
                // Convert to fixed-size [8]FieldElement array (pad with zeros if needed)
                // OPTIMIZATION: Use @memcpy and @memset for efficient copying
                var leaf_domain: [8]FieldElement = undefined;
                @memcpy(leaf_domain[0..hash_len], leaf_domain_slice[0..hash_len]);
                @memset(leaf_domain[hash_len..8], FieldElement{ .value = 0 });
                leaf_domains[epoch - epoch_range_start] = leaf_domain;

                // Debug: Print leaf domain head for all epochs in first bottom tree
                if (build_opts.enable_debug_logs and bottom_tree_index == 0) {
                    log.print("DEBUG: Bottom tree {} epoch {} leaf domain[0]: 0x{x}\n", .{ bottom_tree_index, epoch, leaf_domain[0].value });
                    if (epoch == 0) {
                        log.debugPrint("ZIG_TREEBUILD_LEAF: Epoch 0 leaf domain (Montgomery): ", .{});
                        for (0..hash_len) |h| {
                            log.debugPrint("0x{x:0>8} ", .{leaf_domain[h].value});
                        }
                        log.debugPrint("\n", .{});
                        log.print("ZIG_LEAF_DOMAIN_EPOCH0:[", .{});
                        for (leaf_domain, 0..) |fe, i| {
                            log.print("\"0x{x}\"", .{fe.value});
                            if (i < 7) log.print(",", .{});
                        }
                        log.print("]\n", .{});
                        // Also print full leaf domain for comparison with verification
                        if (build_opts.enable_debug_logs) {
                            log.debugPrint("ZIG_KEYGEN_DEBUG: Leaf domain epoch={} after reduction (Montgomery): ", .{epoch});
                            for (0..hash_len) |h| {
                                log.debugPrint("0x{x:0>8} ", .{leaf_domain[h].value});
                            }
                            log.debugPrint("\n", .{});
                        }
                    }
                    if (epoch == 1) {
                        log.debugPrint("ZIG_TREEBUILD_LEAF: Epoch 1 leaf domain (Montgomery): ", .{});
                        for (0..hash_len) |h| {
                            log.debugPrint("0x{x:0>8} ", .{leaf_domain[h].value});
                        }
                        log.debugPrint("\n", .{});
                    }
                }
            }
        } else {
            // Parallel processing for large workloads
            const LeafComputeContext = struct {
                scheme: *GeneralizedXMSSSignatureScheme,
                prf_key: [32]u8,
                parameter: [5]FieldElement,
                num_chains: usize,
                hash_len: usize,
                epoch_range_start: usize,
                leaf_domains: [][8]FieldElement,
                bottom_tree_index: usize,
                // index removed - using pre-divided chunks instead of work-stealing
                error_flag: std.atomic.Value(bool),
                error_mutex: std.Thread.Mutex,
                stored_error: ?anyerror,
            };

            const leafWorker = struct {
                fn worker(ctx: *LeafComputeContext, chunk_start: usize, chunk_end: usize) void {
                    // Use compile-time SIMD_WIDTH (set via -Dsimd-width build option)
                    // On x86-64 with AVX-512: build with -Dsimd-width=8 for 8-wide SIMD
                    // On ARM/Apple Silicon: always use 4-wide (default)
                    // Runtime detection is available via simd_cpu.getSIMDWidth() but can't be used for types
                    const SIMD_WIDTH = simd_utils.SIMD_WIDTH;

                    // Create Poseidon2SIMD instance once per thread and reuse it
                    // This matches Rust's approach: let chain_perm = poseidon2_16();
                    var simd_poseidon2 = poseidon2_simd.Poseidon2SIMD.init(ctx.scheme.allocator, ctx.scheme.poseidon2);

                    // OPTIMIZATION: Pack parameter once per thread (constant across all epochs and batches)
                    // Parameter is constant across all epochs, so we can pack it once per thread
                    // and reuse it for all batches in this chunk
                    // OPTIMIZATION: Align for SIMD
                    const align_bytes_param = if (SIMD_WIDTH == 8) 32 else 16;
                    var packed_parameter: [5]simd_utils.PackedF align(align_bytes_param) = undefined;
                    // Use @splat for better SIMD optimization when all lanes have same value
                    for (0..5) |i| {
                        packed_parameter[i] = simd_utils.PackedF{ .values = @splat(ctx.parameter[i].value) };
                    }

                    // OPTIMIZATION FIX 5: Process pre-assigned chunk without atomic operations
                    // This matches Rust's par_chunks_exact approach - more cache-friendly
                    var epoch_idx = chunk_start;

                    // OPTIMIZATION: Pre-allocate reusable buffers outside loops to reduce allocations
                    // These buffers are reused across all batches and remainder epochs
                    // OPTIMIZATION: Align buffers for better cache performance
                    const align_bytes_buf = if (SIMD_WIDTH == 8) 32 else 16;
                    var simd_output_buffer: [SIMD_WIDTH][8]FieldElement align(align_bytes_buf) = undefined;
                    var chain_domains_stack: [64][8]FieldElement align(align_bytes_buf) = undefined;
                    var leaf_domain_buffer: [8]FieldElement align(align_bytes_buf) = undefined;

                    // Process complete SIMD-width batches
                    while (epoch_idx + SIMD_WIDTH <= chunk_end) {
                        const batch_start_idx = epoch_idx;
                        const batch_end = epoch_idx + SIMD_WIDTH;
                        const actual_batch_size = SIMD_WIDTH; // Always SIMD_WIDTH for complete batches

                        // Get batch of epochs (complete batch, no padding needed)
                        // OPTIMIZATION: Create @Vector directly without intermediate array
                        const packed_epochs: @Vector(SIMD_WIDTH, u32) = blk: {
                            var epochs: [SIMD_WIDTH]u32 = undefined;
                            const base_epoch = @as(u32, @intCast(ctx.epoch_range_start + batch_start_idx));
                            for (0..SIMD_WIDTH) |i| {
                                epochs[i] = base_epoch + @as(u32, @intCast(i));
                            }
                            break :blk epochs;
                        };

                        // Use stack-allocated array instead of heap allocation
                        // This matches Rust's approach: let mut packed_chains: [[PackedF; HASH_LEN]; NUM_CHUNKS]
                        // num_chains is always 64, hash_len is always 8, so [64][8]PackedF = ~2KB per thread (safe for stack)
                        // OPTIMIZATION: Explicitly align for SIMD (16-byte for NEON/SSE, 32-byte for AVX-512)
                        // AVX-512 requires 32-byte alignment for optimal performance
                        // Note: Zig's align() on stack variables ensures proper alignment for SIMD operations
                        const align_bytes = if (SIMD_WIDTH == 8) 32 else 16;
                        var packed_chains_stack: [64][8]simd_utils.PackedF align(align_bytes) = undefined;

                        // Generate and pack chain starting points for all epochs in batch
                        // Note: actual_batch_size is always SIMD_WIDTH for complete batches
                        // OPTIMIZATION: Pre-compute zero-packed value outside loop
                        const zero_packed = simd_utils.PackedF{ .values = @splat(@as(u32, 0)) };
                        for (0..ctx.num_chains) |chain_idx| {
                            // Generate starting points for this chain across all epochs in batch
                            var starts: [SIMD_WIDTH][8]u32 = undefined;
                            for (0..SIMD_WIDTH) |lane| {
                                starts[lane] = ctx.scheme.prfDomainElement(ctx.prf_key, packed_epochs[lane], @as(u64, @intCast(chain_idx)));
                            }

                            // Transpose: [lane][element] -> [element][lane] for SIMD processing
                            // OPTIMIZATION: Unroll small loops for better performance
                            for (0..ctx.hash_len) |h| {
                                var values: [SIMD_WIDTH]u32 = undefined;
                                // Unroll for common SIMD widths (4 or 8)
                                if (SIMD_WIDTH == 4) {
                                    values[0] = starts[0][h];
                                    values[1] = starts[1][h];
                                    values[2] = starts[2][h];
                                    values[3] = starts[3][h];
                                } else if (SIMD_WIDTH == 8) {
                                    values[0] = starts[0][h];
                                    values[1] = starts[1][h];
                                    values[2] = starts[2][h];
                                    values[3] = starts[3][h];
                                    values[4] = starts[4][h];
                                    values[5] = starts[5][h];
                                    values[6] = starts[6][h];
                                    values[7] = starts[7][h];
                                } else {
                                    // Fallback for other widths
                                    for (0..SIMD_WIDTH) |lane| {
                                        values[lane] = starts[lane][h];
                                    }
                                }
                                packed_chains_stack[chain_idx][h] = simd_utils.PackedF{ .values = values };
                            }
                            // Zero-pad remaining hash_len elements if needed
                            // OPTIMIZATION: Only pad if needed (hash_len < 8)
                            if (ctx.hash_len < 8) {
                                for (ctx.hash_len..8) |h| {
                                    packed_chains_stack[chain_idx][h] = zero_packed;
                                }
                            }
                        }

                        // Create slice references for walkChainsSIMD (it expects [][]PackedF)
                        var packed_chains_slices: [64][]simd_utils.PackedF = undefined;
                        for (0..ctx.num_chains) |chain_idx| {
                            packed_chains_slices[chain_idx] = packed_chains_stack[chain_idx][0..ctx.hash_len];
                        }

                        // Walk all chains for all epochs in batch using SIMD
                        const chain_length = ctx.scheme.lifetime_params.base;
                        for (0..ctx.num_chains) |chain_index| {
                            // walkChainsSIMD does `for (0..chain_length - 1)`, so pass full chain_length to get base - 1 steps
                            ctx.scheme.walkChainsSIMD(
                                &simd_poseidon2, // Pass reusable instance
                                &packed_chains_slices, // Use stack-allocated slices
                                packed_epochs,
                                chain_index,
                                chain_length, // Pass full chain_length, not chain_length - 1
                                packed_parameter,
                            ) catch |err| {
                                ctx.error_mutex.lock();
                                defer ctx.error_mutex.unlock();
                                if (ctx.stored_error == null) {
                                    ctx.stored_error = err;
                                }
                                ctx.error_flag.store(true, .monotonic);
                                return;
                            };
                        }

                        // Use SIMD sponge hash for batch processing
                        // Test with single epoch to isolate multi-epoch issues
                        if (false and actual_batch_size == 1) {
                            // Single epoch - use scalar path for correctness
                            const epoch = packed_epochs[0];
                            const local_idx = batch_start_idx;

                            // Extract chain domains for this epoch from packed chains
                            // Reuse pre-allocated chain_domains_stack
                            const chain_domains = chain_domains_stack[0..ctx.num_chains];
                            for (0..ctx.num_chains) |chain_idx| {
                                for (0..ctx.hash_len) |h| {
                                    chain_domains[chain_idx][h] = FieldElement.fromMontgomery(packed_chains_slices[chain_idx][h].values[0]);
                                }
                                for (ctx.hash_len..8) |h| {
                                    chain_domains[chain_idx][h] = FieldElement.zero();
                                }
                            }

                            // Extract parameter for this epoch
                            var epoch_parameter: [5]FieldElement = undefined;
                            for (0..5) |p_idx| {
                                epoch_parameter[p_idx] = FieldElement.fromMontgomery(packed_parameter[p_idx].values[0]);
                            }

                            // Use scalar sponge hash
                            // Reuse pre-allocated leaf_domain_buffer
                            ctx.scheme.reduceChainDomainsToLeafDomain(chain_domains, epoch_parameter, epoch, &leaf_domain_buffer) catch |err| {
                                ctx.error_mutex.lock();
                                defer ctx.error_mutex.unlock();
                                if (ctx.stored_error == null) {
                                    ctx.stored_error = err;
                                }
                                ctx.error_flag.store(true, .monotonic);
                                return;
                            };
                            const leaf_domain_slice = leaf_domain_buffer[0..ctx.hash_len];

                            // OPTIMIZATION: Use @memcpy for efficient copying
                            @memcpy(ctx.leaf_domains[local_idx][0..ctx.hash_len], leaf_domain_slice[0..ctx.hash_len]);
                            // Zero-pad remaining elements
                            @memset(ctx.leaf_domains[local_idx][ctx.hash_len..8], FieldElement{ .value = 0 });
                        } else {
                            // Multiple epochs - use SIMD sponge hash
                            // Reuse pre-allocated simd_output_buffer
                            ctx.scheme.reduceChainDomainsToLeafDomainSIMD(
                                &simd_poseidon2,
                                &packed_chains_slices,
                                packed_epochs,
                                packed_parameter,
                                &simd_output_buffer,
                            ) catch |err| {
                                ctx.error_mutex.lock();
                                defer ctx.error_mutex.unlock();
                                if (ctx.stored_error == null) {
                                    ctx.stored_error = err;
                                }
                                ctx.error_flag.store(true, .monotonic);
                                return;
                            };

                            // Copy results to leaf_domains
                            // OPTIMIZATION: Use @memcpy for efficient copying
                            for (0..actual_batch_size) |batch_offset| {
                                const local_idx = batch_start_idx + batch_offset;
                                @memcpy(ctx.leaf_domains[local_idx][0..ctx.hash_len], simd_output_buffer[batch_offset][0..ctx.hash_len]);
                                // Zero-pad remaining elements
                                @memset(ctx.leaf_domains[local_idx][ctx.hash_len..8], FieldElement{ .value = 0 });
                            }
                        }

                        epoch_idx = batch_end; // Update for next iteration

                        // Cleanup packed chains
                        // No cleanup needed - using stack allocation
                    }

                    // Handle remainder epochs with scalar code (matching Rust's approach)
                    // Rust processes remainder separately to avoid padding issues
                    // Reuse pre-allocated chain_domains_stack and leaf_domain_buffer
                    while (epoch_idx < chunk_end) {
                        const epoch = @as(u32, @intCast(ctx.epoch_range_start + epoch_idx));
                        const local_idx = epoch_idx;

                        // Process this epoch using scalar code (same as sequential path)
                        // Reuse pre-allocated chain_domains_stack
                        const chain_domains = chain_domains_stack[0..ctx.num_chains];

                        // Compute chain domains for this epoch using scalar path
                        for (0..ctx.num_chains) |chain_idx| {
                            const domain_elements = ctx.scheme.prfDomainElement(ctx.prf_key, epoch, @as(u64, @intCast(chain_idx)));
                            chain_domains[chain_idx] = ctx.scheme.computeHashChainDomain(domain_elements, epoch, @as(u8, @intCast(chain_idx)), ctx.parameter) catch |err| {
                                ctx.error_mutex.lock();
                                defer ctx.error_mutex.unlock();
                                if (ctx.stored_error == null) {
                                    ctx.stored_error = err;
                                }
                                ctx.error_flag.store(true, .monotonic);
                                return;
                            };
                        }

                        // OPTIMIZATION: Reuse pre-allocated leaf_domain_buffer
                        ctx.scheme.reduceChainDomainsToLeafDomain(chain_domains, ctx.parameter, epoch, &leaf_domain_buffer) catch |err| {
                            ctx.error_mutex.lock();
                            defer ctx.error_mutex.unlock();
                            if (ctx.stored_error == null) {
                                ctx.stored_error = err;
                            }
                            ctx.error_flag.store(true, .monotonic);
                            return;
                        };
                        const leaf_domain_slice = leaf_domain_buffer[0..ctx.hash_len];

                        // OPTIMIZATION: Use @memcpy for efficient copying
                        @memcpy(ctx.leaf_domains[local_idx][0..ctx.hash_len], leaf_domain_slice[0..ctx.hash_len]);
                        // Zero-pad remaining elements
                        @memset(ctx.leaf_domains[local_idx][ctx.hash_len..8], FieldElement{ .value = 0 });

                        epoch_idx += 1;
                    }
                }
            };

            var leaf_ctx = LeafComputeContext{
                .scheme = self,
                .prf_key = prf_key,
                .parameter = parameter,
                .num_chains = num_chains,
                .hash_len = self.lifetime_params.hash_len_fe,
                .epoch_range_start = epoch_range_start,
                .leaf_domains = leaf_domains,
                .bottom_tree_index = bottom_tree_index,
                .error_flag = std.atomic.Value(bool).init(false),
                .error_mutex = .{},
                .stored_error = null,
            };

            // OPTIMIZATION: Pre-divide work into chunks (matching Rust's par_chunks_exact)
            // This is more cache-friendly and reduces atomic operations
            // OPTIMIZATION: Smarter thread count calculation to balance parallelism vs overhead
            // Use fewer threads with more work each to reduce thread creation overhead
            // For small workloads, prefer sequential; for large, use fewer but more efficient threads
            const min_epochs_per_thread = 64; // Increased from 32 to reduce thread overhead
            const max_threads = num_cpus;
            const optimal_threads = @min(max_threads, @max(1, leafs_per_bottom_tree / min_epochs_per_thread));
            const num_threads = @min(optimal_threads, leafs_per_bottom_tree);
            const num_epochs = leafs_per_bottom_tree;
            const chunk_size = (num_epochs + num_threads - 1) / num_threads; // Round up

            // OPTIMIZATION: Use stack allocation for small thread counts to reduce allocator overhead
            // For typical cases (num_threads <= 16), stack allocation is faster
            if (num_threads <= 16) {
                var threads_stack: [16]std.Thread = undefined;
                const threads = threads_stack[0..num_threads];

                // Spawn worker threads with pre-assigned chunks
                for (0..num_threads) |t| {
                    const chunk_start = t * chunk_size;
                    const chunk_end = @min(chunk_start + chunk_size, num_epochs);
                    threads[t] = try std.Thread.spawn(.{}, leafWorker.worker, .{ &leaf_ctx, chunk_start, chunk_end });
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            } else {
                // Fallback to heap allocation for large thread counts
                var threads = try self.allocator.alloc(std.Thread, num_threads);
                defer self.allocator.free(threads);

                // Spawn worker threads with pre-assigned chunks
                for (0..num_threads) |t| {
                    const chunk_start = t * chunk_size;
                    const chunk_end = @min(chunk_start + chunk_size, num_epochs);
                    threads[t] = try std.Thread.spawn(.{}, leafWorker.worker, .{ &leaf_ctx, chunk_start, chunk_end });
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            }

            // Check for errors
            if (leaf_ctx.error_flag.load(.monotonic)) {
                leaf_ctx.error_mutex.lock();
                defer leaf_ctx.error_mutex.unlock();
                if (leaf_ctx.stored_error) |err| {
                    return err;
                }
                return error.UnknownError;
            }
        }

        // Build bottom tree layers from leaf domains (shared with signing path)
        // Store layers so they can be reused during signing
        if (profile_keygen) {
            leaf_time_ns = leaf_timer.read();
            tree_timer = try std.time.Timer.start();
        }
        const bottom_layers = try self.buildBottomTreeLayersFromLeafDomains(leaf_domains, parameter, bottom_tree_index);
        // Don't defer free - we're transferring ownership to HashSubTree
        if (profile_keygen) {
            tree_time_ns = tree_timer.read();
            const cache_sec = @as(f64, @floatFromInt(cache_time_ns)) / 1_000_000_000.0;
            const leaf_sec = @as(f64, @floatFromInt(leaf_time_ns)) / 1_000_000_000.0;
            const tree_sec = @as(f64, @floatFromInt(tree_time_ns)) / 1_000_000_000.0;
            log.print("PROFILE_BTREE: tree={} cache={d:.3}ms leaf_gen={d:.3}ms tree_build={d:.3}ms total={d:.3}ms\n", .{
                bottom_tree_index,
                cache_sec * 1000.0,
                leaf_sec * 1000.0,
                tree_sec * 1000.0,
                (cache_sec + leaf_sec + tree_sec) * 1000.0,
            });
        }

        if (bottom_layers.len == 0 or bottom_layers[bottom_layers.len - 1].nodes.len == 0) {
            for (bottom_layers) |layer| self.allocator.free(layer.nodes);
            self.allocator.free(bottom_layers);
            return error.InvalidBottomTree;
        }

        // Debug: check root layer structure (only when debug logs enabled)
        const root_layer = bottom_layers[bottom_layers.len - 1];
        if (build_opts.enable_debug_logs and bottom_tree_index == 1) {
            log.print("ZIG_BOTTOM_ROOT: bottom_tree_index={}, root_layer.nodes.len={}, root_layer.start_index={}\n", .{ bottom_tree_index, root_layer.nodes.len, root_layer.start_index });
        }

        // Rust extracts from layers[depth / 2].nodes[bottom_tree_index % 2]
        // For depth=8: layers[4].nodes[bottom_tree_index % 2]
        // For bottom_tree_index=1: layers[4].nodes[1]
        // But we build with full_depth=4, so our root layer should have only 1 node
        // However, if the root layer has multiple nodes, we should use bottom_tree_index % 2
        const root_node_index = if (root_layer.nodes.len > 1) bottom_tree_index % 2 else 0;
        var bottom_root: [8]FieldElement = undefined;
        @memcpy(&bottom_root, &root_layer.nodes[root_node_index]);

        if (build_opts.enable_debug_logs and bottom_tree_index == 1) {
            log.print("ZIG_BOTTOM_ROOT: Using root_node_index={}, root[0]=0x{x:0>8}\n", .{ root_node_index, bottom_root[0].value });
        }

        if (self.bottom_tree_cache.enabled) {
            if (profile_keygen) {
                cache_timer.reset();
            }
            self.bottom_tree_cache.store(
                self.lifetime_params.log_lifetime,
                prf_key,
                parameter,
                bottom_tree_index,
                bottom_root,
                bottom_layers,
            );
            if (profile_keygen) {
                const store_time_ns = cache_timer.read();
                const store_sec = @as(f64, @floatFromInt(store_time_ns)) / 1_000_000_000.0;
                log.print("PROFILE_BTREE: tree={} cache_store={d:.3}ms\n", .{ bottom_tree_index, store_sec * 1000.0 });
            }
        }

        // Store layers in HashSubTree so they can be reused during signing (major optimization!)
        // Bottom tree depth is log_lifetime (32 for 2^32), matching Rust's encoding
        const tree_depth = self.lifetime_params.log_lifetime;
        return try HashSubTree.initWithLayers(self.allocator, bottom_root, bottom_layers, tree_depth);
    }

    /// Compute hash chain (matching Rust chain function)
    pub fn computeHashChain(
        self: *GeneralizedXMSSSignatureScheme,
        domain_elements: [8]u32,
        epoch: u32,
        chain_index: u8,
        parameter: [5]FieldElement,
    ) !FieldElement {
        // Convert domain elements to field elements
        var current: [8]FieldElement = undefined;
        for (0..8) |i| {
            current[i] = FieldElement{ .value = domain_elements[i] };
        }

        // Debug: Print initial state for first bottom tree, epoch 0, chain 0
        if (epoch == 0 and chain_index == 0) {
            // log.print("DEBUG: Chain initial state epoch={} chain={}: [{}, {}, {}, {}, {}, {}, {}, {}]\n", .{ epoch, chain_index, current[0].value, current[1].value, current[2].value, current[3].value, current[4].value, current[5].value, current[6].value, current[7].value });
        }

        // Walk the chain for BASE-1 steps (matching Rust chain function)
        for (0..self.lifetime_params.base - 1) |j| {
            const pos_in_chain = @as(u8, @intCast(j + 1));

            // Apply chain tweak hash (matching Rust TH::apply with chain_tweak)
            const next = try self.applyPoseidonChainTweakHash(current, epoch, chain_index, pos_in_chain, parameter);

            // Debug: Print chain step for first bottom tree, epoch 0, chain 0
            if (epoch == 0 and chain_index == 0) {
                // log.print("DEBUG: Chain step {} epoch={} chain={}: [{}, {}, {}, {}, {}, {}, {}, {}] -> [{}, {}, {}, {}, {}, {}, {}, {}]\n", .{ j + 1, epoch, chain_index, current[0].value, current[1].value, current[2].value, current[3].value, current[4].value, current[5].value, current[6].value, current[7].value, next[0].value, next[1].value, next[2].value, next[3].value, next[4].value, next[5].value, next[6].value, next[7].value });
            }

            // Update current state
            current = next;
        }

        // Debug: Print final chain result for first bottom tree, epoch 0, chain 0
        if (epoch == 0 and chain_index == 0) {
            log.print("DEBUG: Chain final result epoch={} chain={}: 0x{x}\n", .{ epoch, chain_index, current[0].value });
            // Also print full domain for comparison with verification
            log.debugPrint("ZIG_KEYGEN_DEBUG: Chain {} final domain epoch={} (Montgomery): ", .{ chain_index, epoch });
            const hash_len_keygen = self.lifetime_params.hash_len_fe;
            for (0..hash_len_keygen) |h| {
                log.debugPrint("0x{x:0>8} ", .{current[h].value});
            }
            log.debugPrint("\n", .{});
        }

        return current[0];
    }

    /// Compute hash chain and return the full 8-wide domain state after BASE-1 steps
    /// domain_elements are in Montgomery form (from ShakePRFtoF)
    pub fn computeHashChainDomain(
        self: *GeneralizedXMSSSignatureScheme,
        domain_elements: [8]u32,
        epoch: u32,
        chain_index: u8,
        parameter: [5]FieldElement,
    ) ![8]FieldElement {
        // domain_elements are in Montgomery form (from ShakePRFtoF)
        // applyPoseidonChainTweakHash expects input in Montgomery form
        // So we can use domain_elements directly as Montgomery values
        // SIMD-optimized initialization: batch copy for better performance
        var current: [8]FieldElement = undefined;
        const simd_width = 4;
        var i: usize = 0;
        while (i + simd_width <= 8) : (i += simd_width) {
            // Copy 4 elements at once (SIMD-friendly)
            current[i] = FieldElement{ .value = domain_elements[i] };
            current[i + 1] = FieldElement{ .value = domain_elements[i + 1] };
            current[i + 2] = FieldElement{ .value = domain_elements[i + 2] };
            current[i + 3] = FieldElement{ .value = domain_elements[i + 3] };
        }
        // Copy remaining elements
        while (i < 8) : (i += 1) {
            current[i] = FieldElement{ .value = domain_elements[i] };
        }

        const hash_len = self.lifetime_params.hash_len_fe;
        const base_minus_1 = self.lifetime_params.base - 1;

        // Batch hash operations: process chain steps with reduced function call overhead
        // While we can't parallelize chain steps (each depends on previous), we can optimize
        // the memory operations and reduce overhead by batching the state updates
        for (0..base_minus_1) |j| {
            const pos_in_chain = @as(u8, @intCast(j + 1));
            const next = try self.applyPoseidonChainTweakHash(current, epoch, chain_index, pos_in_chain, parameter);

            // Batch copy hash_len elements efficiently using memcpy (faster than loop)
            @memcpy(current[0..hash_len], next[0..hash_len]);
            // Batch zero remaining elements
            @memset(current[hash_len..8], FieldElement{ .value = 0 });
        }

        // Debug: For epoch 0, chain 0, log final domain at position base_minus_one
        if (epoch == 0 and chain_index == 0) {
            log.debugPrint("ZIG_TREEBUILD_DEBUG: computeHashChainDomain epoch 0 chain 0 final domain at position {} (Montgomery): ", .{base_minus_1});
            for (0..hash_len) |h| {
                log.debugPrint("0x{x:0>8} ", .{current[h].value});
            }
            log.debugPrint("\n", .{});
        }

        return current;
    }

    /// Apply Poseidon2 tweak hash (matching Rust PoseidonTweakHash)
    fn applyPoseidonTweakHash(
        self: *GeneralizedXMSSSignatureScheme,
        input: []const FieldElement,
        epoch: u32,
        chain_index: u8,
        parameter: [5]FieldElement,
    ) ![]FieldElement {
        // Convert epoch and chain_index to field elements for tweak using Rust's encoding
        // ChainTweak: ((epoch as u128) << 24) | ((chain_index as u128) << 16) | ((pos_in_chain as u128) << 8) | TWEAK_SEPARATOR_FOR_CHAIN_HASH
        const field = @import("../../core/field.zig");
        const pos_in_chain = 0; // For chain computation, pos_in_chain is always 0
        const tweak_encoding = (@as(u128, epoch) << 24) | (@as(u128, chain_index) << 16) | (@as(u128, pos_in_chain) << 8) | field.TWEAK_SEPARATOR_FOR_CHAIN_HASH;

        // Convert to field elements using base-p representation
        const tweak = tweakToFieldElements(tweak_encoding);

        // Prepare combined input: parameter + tweak + message
        const total_input_len = 5 + 2 + input.len;
        var combined_input = try self.allocator.alloc(FieldElement, total_input_len);
        defer self.allocator.free(combined_input);

        var input_index: usize = 0;

        // Add parameter elements
        for (0..5) |i| {
            combined_input[input_index] = parameter[i];
            input_index += 1;
        }

        // Add tweak elements
        for (tweak) |t| {
            combined_input[input_index] = t;
            input_index += 1;
        }

        // Add message elements
        for (input) |fe| {
            combined_input[input_index] = fe;
            input_index += 1;
        }

        // Apply Poseidon2-16
        const hash_len_msg = self.lifetime_params.hash_len_fe;
        const hash_result = try self.poseidon2.hashFieldElements16(self.allocator, combined_input, hash_len_msg);
        defer self.allocator.free(hash_result);

        // Return result with capacity elements
        const result = try self.allocator.alloc(FieldElement, self.lifetime_params.capacity);
        for (0..self.lifetime_params.capacity) |i| {
            result[i] = hash_result[i];
        }

        return result;
    }

    /// Apply Poseidon2 chain tweak hash (matching Rust chain_tweak)
    /// Uses stack allocation instead of heap allocation for hot path
    pub inline fn applyPoseidonChainTweakHash(
        self: *GeneralizedXMSSSignatureScheme,
        input: [8]FieldElement,
        epoch: u32,
        chain_index: u8,
        pos_in_chain: u8,
        parameter: [5]FieldElement,
    ) ![8]FieldElement {
        // Convert epoch, chain_index, and pos_in_chain to field elements for tweak using Rust's encoding
        // ChainTweak: ((epoch as u128) << 24) | ((chain_index as u128) << 16) | ((pos_in_chain as u128) << 8) | TWEAK_SEPARATOR_FOR_CHAIN_HASH
        const field = @import("../../core/field.zig");
        const tweak_encoding = (@as(u128, epoch) << 24) | (@as(u128, chain_index) << 16) | (@as(u128, pos_in_chain) << 8) | field.TWEAK_SEPARATOR_FOR_CHAIN_HASH;

        // Convert to field elements using base-p representation (canonical form)
        const tweak = tweakToFieldElements(tweak_encoding);

        // Only use hash_len_fe elements from input (7 for lifetime 2^18, 8 for lifetime 2^8)
        const hash_len = self.lifetime_params.hash_len_fe;

        // Ensure input[hash_len..8] is zero before using it
        // Even though we only copy input[0..hash_len], we want to be explicit about this
        var sanitized_input: [8]FieldElement = input;
        @memset(sanitized_input[hash_len..8], FieldElement.zero());

        // OPTIMIZATION: Use stack allocation instead of heap allocation (max size: 5+2+8=15 elements)
        // This eliminates heap allocations in the hot path, significantly improving performance
        const total_input_len = 5 + 2 + hash_len;
        var combined_input: [15]FieldElement = undefined;
        const combined_input_slice = combined_input[0..total_input_len];

        // Parameter and tweak are already stored in Montgomery form.
        // Rust: parameter.iter().chain(tweak_fe.iter()).chain(single.iter())
        for (0..5) |i| {
            combined_input_slice[i] = parameter[i];
        }
        for (0..2) |i| {
            combined_input_slice[5 + i] = tweak[i];
        }

        // Copy input (already in Montgomery form) - only hash_len_fe elements
        // Rust: single.iter() iterates over all HASH_LEN elements
        // Only use the first hash_len elements from input[8]FieldElement
        // For lifetime 2^18: hash_len=7, so we only use input[0..7]
        // For lifetime 2^8/2^32: hash_len=8, so we use input[0..8]
        @memcpy(combined_input_slice[7 .. 7 + hash_len], sanitized_input[0..hash_len]);

        // OPTIMIZATION: Use direct Poseidon2-16 compression with stack-allocated state
        // This avoids the heap allocation in hashFieldElements16
        // combined_input_slice values are already in Montgomery form
        // We need to convert them to the KoalaBearField type which also uses Montgomery form
        const poseidon2_root = @import("../../poseidon2/root.zig");
        const F = poseidon2_root.Poseidon2KoalaBear16.Field;
        var state: [16]F = undefined;
        var padded_input: [16]F = undefined;

        // combined_input_slice values are already in Montgomery form, so we can use them directly
        // Store both in state and padded_input for feed-forward
        for (0..total_input_len) |i| {
            // FieldElement.value is already in Montgomery form, so create F directly
            state[i] = F{ .value = combined_input_slice[i].value };
            padded_input[i] = state[i]; // Store for feed-forward
        }
        // Pad remaining with zeros
        for (total_input_len..16) |i| {
            state[i] = F.zero;
            padded_input[i] = F.zero;
        }

        // Apply Poseidon2-16 permutation
        poseidon2_root.Poseidon2KoalaBear16.permutation(&state);

        // Feed-forward: Add the input back into the state element-wise (matching Rust's poseidon_compress)
        for (0..16) |i| {
            state[i] = state[i].add(padded_input[i]);
        }

        // Return type is [8]FieldElement, so pad with zeros if hash_len < 8
        // state[i].value is already in Montgomery form
        // Don't use toU32() which converts to canonical - we need Montgomery form
        var result: [8]FieldElement = undefined;
        for (0..hash_len) |i| {
            result[i] = FieldElement{ .value = state[i].value };
        }
        @memset(result[hash_len..8], FieldElement{ .value = 0 });
        return result;
    }

    /// Apply Poseidon2 chain tweak hash with SIMD-packed inputs (processes multiple epochs simultaneously)
    /// Uses true SIMD Poseidon2 permutation for maximum performance
    pub fn applyPoseidonChainTweakHashSIMD(
        self: *GeneralizedXMSSSignatureScheme,
        simd_poseidon2: *poseidon2_simd.Poseidon2SIMD, // Reusable instance (passed from caller)
        packed_input: []const simd_utils.PackedF, // SIMD-packed: [element][epoch]
        packed_epochs: @Vector(simd_utils.SIMD_WIDTH, u32), // Epochs in batch
        chain_index: u8,
        pos_in_chain: u8,
        packed_parameter: [5]simd_utils.PackedF, // Parameter broadcast to all lanes
        packed_output: []simd_utils.PackedF, // NEW: Output buffer to avoid allocation
    ) !void {
        const SIMD_WIDTH = simd_utils.SIMD_WIDTH;
        const field = @import("../../core/field.zig");

        // Pre-compute tweaks for all lanes
        var tweak0_values: [SIMD_WIDTH]u32 = undefined;
        var tweak1_values: [SIMD_WIDTH]u32 = undefined;

        for (0..SIMD_WIDTH) |lane| {
            const epoch = packed_epochs[lane];
            // Only compute tweak if epoch is non-zero (valid epoch)
            // For padding lanes (epoch=0), use zero tweak
            if (epoch != 0) {
                const tweak_encoding = (@as(u128, epoch) << 24) | (@as(u128, chain_index) << 16) | (@as(u128, pos_in_chain) << 8) | field.TWEAK_SEPARATOR_FOR_CHAIN_HASH;
                const tweak = tweakToFieldElements(tweak_encoding);
                tweak0_values[lane] = tweak[0].value;
                tweak1_values[lane] = tweak[1].value;
            } else {
                // Padding lane - use zero tweak
                tweak0_values[lane] = 0;
                tweak1_values[lane] = 0;
            }
        }

        const packed_tweaks = [2]simd_utils.PackedF{
            simd_utils.PackedF{ .values = tweak0_values },
            simd_utils.PackedF{ .values = tweak1_values },
        };

        return self.applyPoseidonChainTweakHashSIMDWithTweaks(
            simd_poseidon2,
            packed_input,
            packed_epochs,
            chain_index,
            pos_in_chain,
            packed_parameter,
            packed_tweaks,
            packed_output,
        );
    }

    /// Internal helper: Apply Poseidon2 chain tweak hash with pre-computed tweaks
    /// This allows callers to pre-compute tweaks once and reuse them
    inline fn applyPoseidonChainTweakHashSIMDWithTweaks(
        self: *GeneralizedXMSSSignatureScheme,
        simd_poseidon2: *poseidon2_simd.Poseidon2SIMD, // Reusable instance (passed from caller)
        packed_input: []const simd_utils.PackedF, // SIMD-packed: [element][epoch]
        packed_epochs: @Vector(simd_utils.SIMD_WIDTH, u32), // Epochs in batch
        chain_index: u8,
        pos_in_chain: u8,
        packed_parameter: [5]simd_utils.PackedF, // Parameter broadcast to all lanes
        packed_tweaks: [2]simd_utils.PackedF, // Pre-computed tweaks
        packed_output: []simd_utils.PackedF, // Output buffer
    ) !void {
        _ = packed_epochs; // Used for debug logging only
        _ = chain_index; // Used for debug logging only
        _ = pos_in_chain; // Used for debug logging only
        const hash_len = self.lifetime_params.hash_len_fe;
        const CHAIN_COMPRESSION_WIDTH = 16; // 5 param + 2 tweak + 8 hash + 1 padding

        // Use stack-allocated array instead of heap allocation
        // This matches Rust's approach: let mut packed_input = [PackedF::ZERO; CHAIN_COMPRESSION_WIDTH];
        // OPTIMIZATION: Align for SIMD (16-byte for NEON/SSE, 32-byte for AVX-512)
        const SIMD_WIDTH_LOCAL = simd_utils.SIMD_WIDTH;
        const align_bytes_combined = if (SIMD_WIDTH_LOCAL == 8) 32 else 16;
        var packed_combined_input: [CHAIN_COMPRESSION_WIDTH]simd_utils.PackedF align(align_bytes_combined) = undefined;

        // Copy parameter (already packed)
        @memcpy(packed_combined_input[0..5], &packed_parameter);

        // Copy tweak (packed)
        @memcpy(packed_combined_input[5..7], &packed_tweaks);

        // Copy input (already packed)
        @memcpy(packed_combined_input[7 .. 7 + hash_len], packed_input[0..hash_len]);

        // OPTIMIZATION: Pad remaining with zeros using @memset for better performance
        // Only pad if there's actually something to pad (hash_len < 8)
        if (7 + hash_len < CHAIN_COMPRESSION_WIDTH) {
            const zero_packed = simd_utils.PackedF{ .values = @splat(@as(u32, 0)) };
            // Use loop for small range (typically 0-1 iterations)
            for (7 + hash_len..CHAIN_COMPRESSION_WIDTH) |i| {
                packed_combined_input[i] = zero_packed;
            }
        }

        // Use SIMD Poseidon2 compression - write directly to output buffer (no allocation!)
        try simd_poseidon2.compress16SIMD(&packed_combined_input, hash_len, packed_output[0..hash_len]);
    }

    /// Walk chains for multiple epochs simultaneously using SIMD
    inline fn walkChainsSIMD(
        self: *GeneralizedXMSSSignatureScheme,
        simd_poseidon2: *poseidon2_simd.Poseidon2SIMD, // Reusable instance (passed from caller)
        packed_chains: [][]simd_utils.PackedF,
        packed_epochs: @Vector(simd_utils.SIMD_WIDTH, u32),
        chain_index: usize,
        chain_length: usize,
        packed_parameter: [5]simd_utils.PackedF,
    ) !void {
        const hash_len = self.lifetime_params.hash_len_fe;
        const SIMD_WIDTH = simd_utils.SIMD_WIDTH;
        const field = @import("../../core/field.zig");

        // Use stack-allocated buffer for packed_next
        // hash_len is at most 8, so 8 PackedF = 128 bytes (safe for stack)
        // This avoids 64 chains × 7 steps = 448 allocations per batch!
        // OPTIMIZATION: Explicitly align for SIMD (16-byte for NEON/SSE, 32-byte for AVX-512)
        const align_bytes_next = if (SIMD_WIDTH == 8) 32 else 16;
        var packed_next_stack: [8]simd_utils.PackedF align(align_bytes_next) = undefined;

        // OPTIMIZATION: Pre-compute all tweaks for all steps once
        // This avoids recomputing tweaks in applyPoseidonChainTweakHashSIMD for each step
        // chain_length - 1 steps, each with 2 tweak values (tweak0, tweak1) × SIMD_WIDTH lanes
        const num_steps = chain_length - 1;
        // Max 256 steps × 2 tweaks × SIMD_WIDTH lanes (4 or 8) = 2-4KB (safe for stack)
        // Use a comptime switch to select the correct array type based on SIMD_WIDTH
        // OPTIMIZATION: Explicitly align for SIMD (16-byte for NEON/SSE, 32-byte for AVX-512)
        const PrecomputedTweaksType = if (SIMD_WIDTH == 8) [256][2][8]u32 else [256][2][4]u32;
        const align_bytes = if (SIMD_WIDTH == 8) 32 else 16;
        var precomputed_tweaks: PrecomputedTweaksType align(align_bytes) = undefined;
        const tweaks_slice = precomputed_tweaks[0..num_steps];

        // Pre-compute tweaks for all steps and all lanes
        // Note: All epochs in packed_epochs are valid (complete batches only)
        for (0..num_steps) |step| {
            const pos_in_chain = @as(u8, @intCast(step + 1));
            for (0..SIMD_WIDTH) |lane| {
                const epoch = packed_epochs[lane];
                // All epochs are valid in complete batches, so no need to check for padding
                const tweak_encoding = (@as(u128, epoch) << 24) | (@as(u128, chain_index) << 16) | (@as(u128, pos_in_chain) << 8) | field.TWEAK_SEPARATOR_FOR_CHAIN_HASH;
                const tweak = tweakToFieldElements(tweak_encoding);
                tweaks_slice[step][0][lane] = tweak[0].value;
                tweaks_slice[step][1][lane] = tweak[1].value;
            }
        }

        // Walk chain for chain_length - 1 steps
        for (0..num_steps) |step| {
            const pos_in_chain = @as(u8, @intCast(step + 1));

            // OPTIMIZATION: Use pre-computed tweaks directly without intermediate conversion
            // The tweaks are already stored as arrays, convert to PackedF directly
            const packed_tweaks = [2]simd_utils.PackedF{
                simd_utils.PackedF{ .values = tweaks_slice[step][0] },
                simd_utils.PackedF{ .values = tweaks_slice[step][1] },
            };

            // Apply SIMD hash to advance chain for all epochs simultaneously
            // Write directly to stack buffer to avoid allocation
            try self.applyPoseidonChainTweakHashSIMDWithTweaks(
                simd_poseidon2, // Pass reusable instance
                packed_chains[chain_index],
                packed_epochs,
                @as(u8, @intCast(chain_index)),
                pos_in_chain,
                packed_parameter,
                packed_tweaks, // Use pre-computed tweaks
                packed_next_stack[0..hash_len], // Write to stack buffer
            );

            // Update packed chain state
            // OPTIMIZATION: Use @memcpy for efficient copying (hash_len is small, typically 8)
            @memcpy(packed_chains[chain_index][0..hash_len], packed_next_stack[0..hash_len]);
        }
    }

    /// Context for parallel pair processing
    const PairProcessContext = struct {
        scheme: *GeneralizedXMSSSignatureScheme,
        nodes: [][8]FieldElement,
        parents: [][8]FieldElement,
        parent_start: usize,
        current_level: usize,
        parameter: [5]FieldElement,
        hash_len: usize,
        index: std.atomic.Value(usize),
        error_flag: std.atomic.Value(bool),
        error_mutex: std.Thread.Mutex,
        stored_error: ?anyerror,
    };

    /// Batch hash multiple tree node pairs together (reduces function call overhead)
    /// Processes pairs in batches of 4 for better SIMD utilization
    inline fn batchHashTreePairs(
        self: *GeneralizedXMSSSignatureScheme,
        nodes: [][8]FieldElement,
        parents: [][8]FieldElement,
        parent_start: usize,
        current_level: usize,
        parameter: [5]FieldElement,
        start_idx: usize,
        end_idx: usize,
    ) !void {
        const hash_len = self.lifetime_params.hash_len_fe;
        const SIMD_WIDTH = simd_utils.SIMD_WIDTH;
        const batch_size = SIMD_WIDTH; // Process SIMD_WIDTH pairs at a time for SIMD optimization

        // OPTIMIZATION: Create Poseidon2SIMD instance once and reuse it for all batches
        // This avoids creating a new instance for each batch (Poseidon2SIMD.init is lightweight but still has overhead)
        var simd_poseidon2 = poseidon2_simd.Poseidon2SIMD.init(self.allocator, self.poseidon2);

        var i = start_idx;
        // Use SIMD for batches, scalar for remaining pairs
        while (i + batch_size <= end_idx) : (i += batch_size) {
            // Process batch of SIMD_WIDTH pairs together using SIMD
            try self.batchHashTreePairsSIMDWithInstance(
                &simd_poseidon2, // Pass reusable instance
                nodes,
                parents,
                parent_start,
                current_level,
                parameter,
                i,
                i + batch_size,
            );
        }

        // Process remaining pairs with scalar
        while (i < end_idx) : (i += 1) {
            const left_idx = i * 2;
            const right_idx = i * 2 + 1;

            const left = nodes[left_idx];
            const right = nodes[right_idx];

            const left_slice = left[0..hash_len];
            const right_slice = right[0..hash_len];

            const parent_pos = @as(u32, @intCast(parent_start + i));

            // Debug output removed for performance

            // Debug: For bottom tree 0, epoch 0, level 0, first parent (i=0), log the hash inputs
            if (build_opts.enable_debug_logs and current_level == 0 and i == 0 and parent_start == 0) {
                log.debugPrint("ZIG_TREEBUILD_HASH: Bottom tree 0, epoch 0, level 0, parent_pos 0: left[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical), right[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical), level={}, pos_in_level={}, param[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical)\n", .{ left_slice[0].value, left_slice[0].toCanonical(), right_slice[0].value, right_slice[0].toCanonical(), current_level, parent_pos, parameter[0].value, parameter[0].toCanonical() });
            }
            // Debug: For bottom tree 0, log all hashes for levels 0-3 (for 2^8 lifetime debugging)
            if (build_opts.enable_debug_logs and parent_start == 0 and current_level <= 3) {
                log.debugPrint("ZIG_TREEBUILD_HASH: Bottom tree 0, level {}, i={}, parent_pos {}: left[0]=0x{x:0>8}, right[0]=0x{x:0>8}, left_idx={}, right_idx={}\n", .{ current_level, i, parent_pos, left_slice[0].value, right_slice[0].value, left_idx, right_idx });
            }

            // Debug: For bottom tree 0, level 0, i=0, log inputs before hash call
            if (build_opts.enable_debug_logs and current_level == 0 and i == 0 and parent_start == 0) {
                log.debugPrint("ZIG_TREEBUILD_HASH_INPUT: Bottom tree 0, level 0, i=0: left[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical), right[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical), level={}, pos_in_level={}, param[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical)\n", .{ left_slice[0].value, left_slice[0].toCanonical(), right_slice[0].value, right_slice[0].toCanonical(), current_level, parent_pos, parameter[0].value, parameter[0].toCanonical() });
            }

            const hash_result = try self.applyPoseidonTreeTweakHashWithSeparateInputs(
                left_slice,
                right_slice,
                @as(u8, @intCast(current_level)),
                parent_pos,
                parameter,
            );
            defer self.allocator.free(hash_result);

            // Debug output removed for performance
            // Debug: For bottom tree 0, log all hash results for levels 0-3 (for 2^8 lifetime debugging)
            if (build_opts.enable_debug_logs and parent_start == 0 and current_level <= 3) {
                log.debugPrint("ZIG_TREEBUILD_HASH: Bottom tree 0, level {}, i={}, parent_pos {}: left[0]=0x{x:0>8}, right[0]=0x{x:0>8}, parent[0]=0x{x:0>8}\n", .{ current_level, i, parent_pos, left_slice[0].value, right_slice[0].value, hash_result[0].value });
            }
            // Debug: For bottom tree 0, level 1, i=0, log the result (should match node 0's result)
            if (build_opts.enable_debug_logs and parent_start == 0 and current_level == 1 and i == 0) {
                log.debugPrint("ZIG_TREEBUILD_HASH: Bottom tree 0, level 1, i=0, parent_pos {}: parent[0]=0x{x:0>8} (should match node 0's result)\n", .{ parent_pos, hash_result[0].value });
            }
            // Debug: For first top tree hash (level=16, i=0, parent_start=0), log the result
            if (build_opts.enable_debug_logs and current_level == 16 and i == 0 and parent_start == 0) {
                log.debugPrint("ZIG_TREEBUILD_HASH: First top tree hash (level=16, i=0, parent_pos=0): left[0]=0x{x:0>8}, right[0]=0x{x:0>8}, parent[0]=0x{x:0>8}\n", .{ left_slice[0].value, right_slice[0].value, hash_result[0].value });
            }

            @memcpy(parents[i][0..hash_len], hash_result[0..hash_len]);
            @memset(parents[i][hash_len..8], FieldElement{ .value = 0 });
        }
    }

    /// Batch hash tree pairs using SIMD Poseidon2-24 (with reusable instance)
    inline fn batchHashTreePairsSIMDWithInstance(
        self: *GeneralizedXMSSSignatureScheme,
        simd_poseidon2: *poseidon2_simd.Poseidon2SIMD, // Reusable instance
        nodes: [][8]FieldElement,
        parents: [][8]FieldElement,
        parent_start: usize,
        current_level: usize,
        parameter: [5]FieldElement,
        start_idx: usize,
        end_idx: usize,
    ) !void {
        const hash_len = self.lifetime_params.hash_len_fe;
        const SIMD_WIDTH = simd_utils.SIMD_WIDTH;
        const field = @import("../../core/field.zig");

        // OPTIMIZATION: Use stack-allocated arrays and optimize packing
        // Format: parameter (5) + tweak (2) + left (hash_len) + right (hash_len)
        const total_input_len = 5 + 2 + hash_len + hash_len;
        const tweak_level = @as(u8, @intCast(current_level)) + 1;
        const p: u128 = 2130706433; // KoalaBear field modulus

        // OPTIMIZATION: Stack-allocate packed input (max size known: 5+2+8+8=23)
        // OPTIMIZATION: Align for SIMD (16-byte for NEON/SSE, 32-byte for AVX-512)
        const align_bytes_input = if (SIMD_WIDTH == 8) 32 else 16;
        var packed_input: [23]simd_utils.PackedF align(align_bytes_input) = undefined;
        const packed_input_slice = packed_input[0..total_input_len];

        // OPTIMIZATION: Pack directly without intermediate buffers (reduce memory overhead)
        const actual_batch_size = end_idx - start_idx;
        var input_idx: usize = 0;

        // OPTIMIZATION: Pack parameter using @splat (same value in all lanes) - direct packing
        for (0..5) |p_idx| {
            const param_val = parameter[p_idx].value;
            packed_input_slice[input_idx] = simd_utils.PackedF{ .values = @splat(param_val) };
            input_idx += 1;
        }

        // OPTIMIZATION: Compute tweaks and pack directly (avoid intermediate buffers)
        // Convert tweak values to Montgomery form (matching scalar version)
        var tweak_values_0: [SIMD_WIDTH]u32 = undefined;
        var tweak_values_1: [SIMD_WIDTH]u32 = undefined;
        for (0..actual_batch_size) |lane| {
            const pair_idx = start_idx + lane;
            const tweak_bigint = (@as(u128, tweak_level) << 40) | (@as(u128, @intCast(parent_start + pair_idx)) << 8) | field.TWEAK_SEPARATOR_FOR_TREE_HASH;
            // Convert canonical u32 to Montgomery form (matching scalar version: FieldElement.fromCanonical)
            const tweak_canonical_0 = @as(u32, @intCast(tweak_bigint % p));
            const tweak_canonical_1 = @as(u32, @intCast((tweak_bigint / p) % p));
            const tweak_fe_0 = FieldElement.fromCanonical(tweak_canonical_0);
            const tweak_fe_1 = FieldElement.fromCanonical(tweak_canonical_1);
            tweak_values_0[lane] = tweak_fe_0.value; // Use Montgomery form
            tweak_values_1[lane] = tweak_fe_1.value; // Use Montgomery form
        }
        // Pad remaining lanes with zeros
        @memset(tweak_values_0[actual_batch_size..], 0);
        @memset(tweak_values_1[actual_batch_size..], 0);

        // Pack tweaks
        packed_input_slice[input_idx] = simd_utils.PackedF{ .values = tweak_values_0 };
        input_idx += 1;
        packed_input_slice[input_idx] = simd_utils.PackedF{ .values = tweak_values_1 };
        input_idx += 1;

        // OPTIMIZATION: Pack left and right inputs directly (transpose on-the-fly, no intermediate buffer)
        for (0..hash_len) |h_idx| {
            var left_vals: [SIMD_WIDTH]u32 = undefined;
            for (0..actual_batch_size) |lane| {
                const pair_idx = start_idx + lane;
                const left_idx = pair_idx * 2;
                left_vals[lane] = nodes[left_idx][h_idx].value;
            }
            // Pad remaining lanes
            @memset(left_vals[actual_batch_size..], 0);
            packed_input_slice[input_idx] = simd_utils.PackedF{ .values = left_vals };
            input_idx += 1;
        }
        for (0..hash_len) |h_idx| {
            var right_vals: [SIMD_WIDTH]u32 = undefined;
            for (0..actual_batch_size) |lane| {
                const pair_idx = start_idx + lane;
                const right_idx = pair_idx * 2 + 1;
                right_vals[lane] = nodes[right_idx][h_idx].value;
            }
            // Pad remaining lanes
            @memset(right_vals[actual_batch_size..], 0);
            packed_input_slice[input_idx] = simd_utils.PackedF{ .values = right_vals };
            input_idx += 1;
        }

        // Debug output removed for performance

        // Use SIMD Poseidon2-24 compression
        // Use stack-allocated buffer instead of heap allocation
        // hash_len is at most 8, so 8 PackedF = 128 bytes (safe for stack)
        // OPTIMIZATION: Align for SIMD (16-byte for NEON/SSE, 32-byte for AVX-512)
        const align_bytes_results = if (SIMD_WIDTH == 8) 32 else 16;
        var packed_hash_results_stack: [8]simd_utils.PackedF align(align_bytes_results) = undefined;
        // OPTIMIZATION: Use passed instance instead of creating new one
        try simd_poseidon2.compress24SIMD(packed_input_slice, hash_len, packed_hash_results_stack[0..hash_len]);

        // Extract results for each lane
        // OPTIMIZATION: Use @memcpy for efficient copying where possible
        for (0..actual_batch_size) |lane| {
            const pair_idx = start_idx + lane;
            // Copy hash_len elements
            for (0..hash_len) |h_idx| {
                parents[pair_idx][h_idx] = FieldElement{ .value = packed_hash_results_stack[h_idx].values[lane] };
            }
            // Zero-pad remaining elements
            @memset(parents[pair_idx][hash_len..8], FieldElement{ .value = 0 });

            // Debug output removed for performance
        }
    }

    /// Worker function for parallel pair processing with optimized batch memory operations
    fn pairProcessWorker(ctx: *PairProcessContext) void {
        const total = ctx.parents.len;

        while (true) {
            const i = ctx.index.fetchAdd(1, .monotonic);
            if (i >= total) break;

            const left_idx = i * 2;
            const right_idx = i * 2 + 1;

            const left = ctx.nodes[left_idx];
            const right = ctx.nodes[right_idx];

            const left_slice = left[0..ctx.hash_len];
            const right_slice = right[0..ctx.hash_len];

            const parent_pos = @as(u32, @intCast(ctx.parent_start + i));
            const hash_result = ctx.scheme.applyPoseidonTreeTweakHashWithSeparateInputs(
                left_slice,
                right_slice,
                @as(u8, @intCast(ctx.current_level)),
                parent_pos,
                ctx.parameter,
            ) catch |err| {
                ctx.error_mutex.lock();
                defer ctx.error_mutex.unlock();
                if (ctx.stored_error == null) {
                    ctx.stored_error = err;
                }
                ctx.error_flag.store(true, .monotonic);
                return;
            };
            defer ctx.scheme.allocator.free(hash_result);

            // Batch copy result efficiently using memcpy (reduces function call overhead)
            @memcpy(ctx.parents[i][0..ctx.hash_len], hash_result[0..ctx.hash_len]);
            @memset(ctx.parents[i][ctx.hash_len..8], FieldElement{ .value = 0 });
        }
    }

    /// Process all pairs in parallel (matching Rust par_chunks_exact(2))
    fn processPairsInParallel(
        self: *GeneralizedXMSSSignatureScheme,
        nodes: [][8]FieldElement,
        parents: [][8]FieldElement,
        parent_start: usize,
        current_level: usize,
        parameter: [5]FieldElement,
    ) !void {
        const parents_len = parents.len;
        const hash_len = self.lifetime_params.hash_len_fe;

        // Use parallel processing for large workloads (matching Rust behavior)
        // Rust uses rayon's par_chunks_exact(2) which processes pairs in parallel
        // but maintains deterministic order
        const num_cpus = std.Thread.getCpuCount() catch 1;
        const min_parallel_size = 64; // Threshold for parallel processing

        if (parents_len < min_parallel_size or num_cpus <= 1) {
            // Sequential processing with batch hash operations for small workloads
            // Batch processing reduces function call overhead
            try self.batchHashTreePairs(nodes, parents, parent_start, current_level, parameter, 0, parents_len);
        } else {
            // Parallel processing for large workloads
            var ctx = PairProcessContext{
                .scheme = self,
                .nodes = nodes,
                .parents = parents,
                .parent_start = parent_start,
                .current_level = current_level,
                .parameter = parameter,
                .hash_len = hash_len,
                .index = std.atomic.Value(usize).init(0),
                .error_flag = std.atomic.Value(bool).init(false),
                .error_mutex = .{},
                .stored_error = null,
            };

            const num_threads = @min(num_cpus, parents_len);
            // OPTIMIZATION: Use stack allocation for small thread counts
            if (num_threads <= 16) {
                var threads_stack: [16]std.Thread = undefined;
                const threads = threads_stack[0..num_threads];

                // Spawn worker threads
                for (0..num_threads) |t| {
                    threads[t] = try std.Thread.spawn(.{}, pairProcessWorker, .{&ctx});
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            } else {
                var threads = try self.allocator.alloc(std.Thread, num_threads);
                defer self.allocator.free(threads);

                // Spawn worker threads
                for (0..num_threads) |t| {
                    threads[t] = try std.Thread.spawn(.{}, pairProcessWorker, .{&ctx});
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            }

            // Check for errors
            if (ctx.error_flag.load(.monotonic)) {
                ctx.error_mutex.lock();
                defer ctx.error_mutex.unlock();
                if (ctx.stored_error) |err| {
                    return err;
                }
                return error.UnknownError;
            }
        }
    }

    /// Process a batch of pairs (thread worker function)
    fn processPairBatch(
        self: *GeneralizedXMSSSignatureScheme,
        nodes: [][8]FieldElement,
        parents: [][8]FieldElement,
        parent_start: usize,
        current_level: usize,
        parameter: [5]FieldElement,
        start_idx: usize,
        end_idx: usize,
    ) void {
        for (start_idx..end_idx) |i| {
            // Hash two children together (matching Rust exactly)
            const left_idx = i * 2;
            const right_idx = i * 2 + 1;

            const left = nodes[left_idx];
            const right = nodes[right_idx];

            // Convert arrays to slices for hashing
            const left_slice = left[0..];
            const right_slice = right[0..];

            // Use tree tweak for this level and position (matching Rust exactly)
            const parent_pos = @as(u32, @intCast(parent_start + i));
            const hash_result = self.applyPoseidonTreeTweakHashWithSeparateInputs(left_slice, right_slice, @as(u8, @intCast(current_level)), parent_pos, parameter) catch {
                // Handle error - in a real implementation, we'd need proper error handling
                return;
            };
            defer self.allocator.free(hash_result);

            // Copy the result to the parents array (all 8 elements)
            @memcpy(parents[i][0..], hash_result[0..8]);
        }
    }

    /// Apply Poseidon2 tree tweak hash with separate left/right inputs (matching Rust exactly)
    pub fn applyPoseidonTreeTweakHashWithSeparateInputs(
        self: *GeneralizedXMSSSignatureScheme,
        left: []const FieldElement,
        right: []const FieldElement,
        level: u8,
        pos_in_level: u32,
        parameter: [5]FieldElement,
    ) ![]FieldElement {
        // Inputs are expected canonical; Poseidon layer handles Montgomery internally.
        // Compute tree tweak: ((level + 1 as u128) << 40) | ((pos_in_level as u128) << 8) | TWEAK_SEPARATOR_FOR_TREE_HASH
        // Match Rust: let tweak_level = (level as u8) + 1;
        const field = @import("../../core/field.zig");
        const tweak_level = level + 1;
        const tweak_bigint = (@as(u128, tweak_level) << 40) | (@as(u128, pos_in_level) << 8) | field.TWEAK_SEPARATOR_FOR_TREE_HASH;

        // Create a unique identifier for this hash call based on inputs (for matching across build/verify)
        // Use first element of left, right, and param[0] as a simple identifier
        const call_id = left[0].value ^ right[0].value ^ parameter[0].value ^ @as(u32, @intCast(level)) ^ @as(u32, @intCast(pos_in_level));

        // Debug: log hash call for level 0, pos 8 (epoch 16 bottom tree)
        if (level == 0 and pos_in_level == 8 and left.len > 0 and right.len > 0) {
            log.print("ZIG_HASH_CALL: level={} pos={} tweak=0x{x} param[0]=0x{x:0>8} left[0]=0x{x:0>8} right[0]=0x{x:0>8} left.len={} right.len={} call_id=0x{x:0>8} left_all=", .{ level, pos_in_level, tweak_bigint, parameter[0].value, left[0].value, right[0].value, left.len, right.len, call_id });
            for (left) |fe| log.print("0x{x:0>8} ", .{fe.value});
            log.print("right_all=", .{});
            for (right) |fe| log.print("0x{x:0>8} ", .{fe.value});
            log.print("\n", .{});
        }

        log.print("DEBUG: Tree tweak level={} pos={} -> 0x{x}\n", .{ tweak_level, pos_in_level, tweak_bigint });

        // Convert to 2 field elements using base-p representation
        const p: u128 = 2130706433; // KoalaBear field modulus
        const tweak = [_]FieldElement{
            FieldElement.fromCanonical(@intCast(tweak_bigint % p)),
            FieldElement.fromCanonical(@intCast((tweak_bigint / p) % p)),
        };

        // Debug: print tweak field elements
        log.print("DEBUG: Sponge tweak_fe: [0x{x}, 0x{x}]\n", .{ tweak[0].value, tweak[1].value });

        // Prepare combined input: parameter + tweak + left + right (matching Rust exactly)
        const total_input_len = 5 + 2 + left.len + right.len;
        var combined_input = try self.allocator.alloc(FieldElement, total_input_len);
        defer self.allocator.free(combined_input);

        var input_index: usize = 0;

        // Add parameter elements (canonical)
        for (0..5) |i| {
            combined_input[input_index] = parameter[i];
            input_index += 1;
        }

        // Add tweak elements (canonical)
        for (tweak) |t| {
            combined_input[input_index] = t;
            input_index += 1;
        }

        // Add left elements
        for (left) |fe| {
            combined_input[input_index] = fe;
            input_index += 1;
        }

        // Add right elements
        for (right) |fe| {
            combined_input[input_index] = fe;
            input_index += 1;
        }

        // Debug output removed for performance

        // Use Poseidon2-24 compress (feed-forward) with zero-padding to width 24,
        // then take the first hash_len_fe elements (matching Rust poseidon_compress::<_, 24, HASH_LEN>)
        // Note: For single inputs, use scalar version for correctness
        // SIMD can be used when batching multiple tree nodes (future optimization)
        const hash_len = self.lifetime_params.hash_len_fe;
        var padded: [24]FieldElement = [_]FieldElement{FieldElement{ .value = 0 }} ** 24;
        const simd_width = 4;
        var i: usize = 0;
        while (i + simd_width <= combined_input.len and i + simd_width <= padded.len) : (i += simd_width) {
            // Copy 4 elements at once (SIMD-friendly, better cache performance)
            padded[i] = combined_input[i];
            padded[i + 1] = combined_input[i + 1];
            padded[i + 2] = combined_input[i + 2];
            padded[i + 3] = combined_input[i + 3];
        }
        // Copy remaining elements
        while (i < combined_input.len and i < padded.len) : (i += 1) {
            padded[i] = combined_input[i];
        }
        // compress requires comptime output_len, so use max (8) and slice to hash_len_fe
        const full_out = try self.poseidon2.compress(padded, 8);

        // Debug: log hash result for level 0, pos 8 (epoch 16 bottom tree)
        if (level == 0 and pos_in_level == 8 and left.len > 0 and right.len > 0) {
            log.print("ZIG_HASH_RESULT: level={} pos={} result[0]=0x{x:0>8} call_id=0x{x:0>8}\n", .{ level, pos_in_level, full_out[0].value, call_id });
        }

        // DETAILED HASH LOGGING
        log.print("DEBUG: Hash input ({} elements): ", .{combined_input.len});
        for (combined_input, 0..) |fe, idx| {
            log.print("{}:0x{x}", .{ idx, fe.value });
            if (idx < combined_input.len - 1) log.print(", ", .{});
        }
        log.print("\n", .{});
        log.print("DEBUG: Hash output (first {} of {} elements): ", .{ hash_len, full_out.len });
        for (0..hash_len) |idx| {
            log.print("{}:0x{x}", .{ idx, full_out[idx].value });
            if (idx < hash_len - 1) log.print(", ", .{});
        }
        log.print("\n", .{});

        // Batch copy result efficiently using memcpy (reduces function call overhead)
        const result = try self.allocator.alloc(FieldElement, hash_len);
        @memcpy(result[0..hash_len], full_out[0..hash_len]);
        return result;
    }

    /// Apply Poseidon2 tree tweak hash (matching Rust PoseidonTweakHash for tree hashing)
    pub fn applyPoseidonTreeTweakHash(
        self: *GeneralizedXMSSSignatureScheme,
        input: []const FieldElement,
        level: u8,
        pos_in_level: u32,
        parameter: [5]FieldElement,
    ) ![]FieldElement {
        // Compute tree tweak: ((level + 1 as u128) << 40) | ((pos_in_level as u128) << 8) | 0x01
        // Match Rust: let tweak_level = (level as u8) + 1;
        const tweak_level = level + 1;
        const tweak_bigint = (@as(u128, tweak_level) << 40) | (@as(u128, pos_in_level) << 8) | 0x01;
        log.print("DEBUG: Tree tweak level={} pos={} -> 0x{x}\n", .{ tweak_level, pos_in_level, tweak_bigint });

        // Convert to 2 field elements using base-p representation
        const p: u128 = 2130706433; // KoalaBear field modulus
        const tweak = [_]FieldElement{
            FieldElement.fromCanonical(@intCast(tweak_bigint % p)),
            FieldElement.fromCanonical(@intCast((tweak_bigint / p) % p)),
        };

        // Prepare combined input: parameter + tweak + message
        const total_input_len = 5 + 2 + input.len;
        var combined_input = try self.allocator.alloc(FieldElement, total_input_len);
        defer self.allocator.free(combined_input);

        var input_index: usize = 0;

        // Add parameter elements
        for (0..5) |i| {
            combined_input[input_index] = parameter[i];
            input_index += 1;
        }

        // Add tweak elements
        for (tweak) |t| {
            combined_input[input_index] = t;
            input_index += 1;
        }

        // Add message elements
        for (input) |fe| {
            combined_input[input_index] = fe;
            input_index += 1;
        }

        // Apply Poseidon2-24 for tree hashing (matching Rust implementation)
        const hash_result = try self.poseidon2.hashFieldElements(self.allocator, combined_input);
        defer self.allocator.free(hash_result);

        // DETAILED HASH LOGGING: Log input and output for debugging
        log.print("DEBUG: Tree Hash input ({} elements): ", .{combined_input.len});
        for (combined_input, 0..) |fe, i| {
            log.print("{}:0x{x}", .{ i, fe.value });
            if (i < combined_input.len - 1) log.print(", ", .{});
        }
        log.print("\n", .{});

        log.print("DEBUG: Tree Hash output ({} elements): ", .{hash_result.len});
        for (hash_result, 0..) |fe, i| {
            log.print("{}:0x{x}", .{ i, fe.value });
            if (i < hash_result.len - 1) log.print(", ", .{});
        }
        log.print("\n", .{});

        // Return result with hash_len_fe elements (8 for tree hashing)
        const result = try self.allocator.alloc(FieldElement, self.lifetime_params.hash_len_fe);
        for (0..self.lifetime_params.hash_len_fe) |i| {
            result[i] = hash_result[i];
        }

        return result;
    }

    /// Hash chain ends using Poseidon2 by reducing all elements pairwise until one remains
    pub fn hashChainEnds(self: *GeneralizedXMSSSignatureScheme, chain_ends: []FieldElement, parameter: [5]FieldElement) !FieldElement {
        if (chain_ends.len == 0) return error.InvalidInput;

        var current = try self.allocator.alloc(FieldElement, chain_ends.len);
        defer self.allocator.free(current);
        @memcpy(current, chain_ends);

        var cur_len = chain_ends.len;
        while (cur_len > 1) {
            const next_len = (cur_len + 1) / 2;
            var next = try self.allocator.alloc(FieldElement, next_len);
            defer self.allocator.free(next);

            var i: usize = 0;
            var out_idx: usize = 0;
            while (i < cur_len) : (i += 2) {
                if (i + 1 < cur_len) {
                    // Hash the pair [current[i], current[i+1]]
                    const pair = [_]FieldElement{ current[i], current[i + 1] };
                    const h = try self.applyPoseidonTweakHash(&pair, 0, 0, parameter);
                    defer self.allocator.free(h);
                    next[out_idx] = h[0];
                } else {
                    // Odd tail: carry forward
                    next[out_idx] = current[i];
                }
                out_idx += 1;
            }

            // Move to next level
            self.allocator.free(current);
            current = try self.allocator.alloc(FieldElement, next_len);
            @memcpy(current, next);
            cur_len = next_len;
        }

        return current[0];
    }

    /// Reduce chain ends into an 8-wide domain by pairwise hashing until 8 remain
    pub fn hashChainEndsToDomain(self: *GeneralizedXMSSSignatureScheme, chain_ends: []FieldElement, parameter: [5]FieldElement) ![8]FieldElement {
        if (chain_ends.len == 0) return error.InvalidInput;

        var current = try self.allocator.alloc(FieldElement, chain_ends.len);
        defer self.allocator.free(current);
        @memcpy(current, chain_ends);

        var cur_len = chain_ends.len;
        while (cur_len > 8) {
            const next_len = (cur_len + 1) / 2;
            var next = try self.allocator.alloc(FieldElement, next_len);
            defer self.allocator.free(next);

            var i: usize = 0;
            var out_idx: usize = 0;
            while (i < cur_len) : (i += 2) {
                if (i + 1 < cur_len) {
                    const pair = [_]FieldElement{ current[i], current[i + 1] };
                    const h = try self.applyPoseidonTweakHash(&pair, 0, 0, parameter);
                    defer self.allocator.free(h);
                    next[out_idx] = h[0];
                } else {
                    next[out_idx] = current[i];
                }
                out_idx += 1;
            }

            self.allocator.free(current);
            current = try self.allocator.alloc(FieldElement, next_len);
            @memcpy(current, next);
            cur_len = next_len;
        }

        var domain: [8]FieldElement = undefined;
        // If fewer than 8 remain (shouldn't happen with 64), pad with zeros
        var i: usize = 0;
        while (i < 8) : (i += 1) {
            domain[i] = if (i < cur_len) current[i] else FieldElement{ .value = 0 };
        }
        return domain;
    }

    /// Reduce 64 chain domains ([8] each) into a single leaf domain using Poseidon sponge
    /// This matches Rust's TH::apply when message.len() > 2 (sponge mode)
    /// Returns hash_len_fe elements (7 for lifetime 2^18, 8 for lifetime 2^8)
    /// OPTIMIZATION: Accepts output buffer to avoid per-call allocation
    pub inline fn reduceChainDomainsToLeafDomain(
        self: *GeneralizedXMSSSignatureScheme,
        chain_domains_in: [][8]FieldElement,
        parameter: [5]FieldElement,
        epoch: u32,
        output: []FieldElement, // Pre-allocated output buffer (must be at least hash_len_fe)
    ) !void {
        if (chain_domains_in.len == 0) return error.InvalidInput;

        // Implement the sponge mode matching Rust exactly:
        // 1. Flatten all domains: message.iter().flatten()
        // 2. Create domain separator from [PARAMETER_LEN, TWEAK_LEN, NUM_CHUNKS, HASH_LEN]
        // 3. Use poseidon_sponge with width 24, capacity from domain separator

        const PARAMETER_LEN: u32 = @intCast(self.lifetime_params.parameter_len);
        const TWEAK_LEN: u32 = @intCast(self.lifetime_params.tweak_len_fe);
        const NUM_CHUNKS: u32 = @intCast(chain_domains_in.len); // dimension (64)
        const HASH_LEN: u32 = @intCast(self.lifetime_params.hash_len_fe);

        // Flatten all domains into a single slice (matching Rust: message.iter().flatten())
        // Only use hash_len_fe elements from each domain (7 for lifetime 2^18, 8 for lifetime 2^8)
        const hash_len = self.lifetime_params.hash_len_fe;
        const flattened_len = chain_domains_in.len * hash_len;
        // Use stack allocation instead of heap
        // Max size: 64 chains × 8 elements = 512 FieldElements = 2KB (safe for stack)
        var flattened_input_stack: [512]FieldElement = undefined;
        const flattened_input = flattened_input_stack[0..flattened_len];

        // SIMD-optimized flattening: use @Vector for copying when hash_len >= 4
        // Batch flattening: use memcpy for efficient copying (reduces function call overhead)
        var flat_idx: usize = 0;
        for (chain_domains_in) |domain| {
            // Batch copy hash_len elements using memcpy (faster than loop)
            @memcpy(flattened_input[flat_idx .. flat_idx + hash_len], domain[0..hash_len]);
            flat_idx += hash_len;
        }

        // Create tree tweak: level=0, pos_in_level=epoch (matching Rust: TH::tree_tweak(0, epoch))
        const field = @import("../../core/field.zig");
        const tweak_level: u8 = 0;
        const tweak_bigint = (@as(u128, tweak_level) << 40) | (@as(u128, epoch) << 8) | field.TWEAK_SEPARATOR_FOR_TREE_HASH;

        // Convert tweak to 2 field elements using base-p representation
        const p: u128 = 2130706433; // KoalaBear field modulus
        const tweak = [_]FieldElement{
            FieldElement.fromCanonical(@intCast(tweak_bigint % p)),
            FieldElement.fromCanonical(@intCast((tweak_bigint / p) % p)),
        };

        // Create domain separator from lengths (matching Rust's poseidon_safe_domain_separator)
        const DOMAIN_PARAMETERS_LENGTH: usize = 4;
        const domain_params: [DOMAIN_PARAMETERS_LENGTH]u32 = [4]u32{ PARAMETER_LEN, TWEAK_LEN, NUM_CHUNKS, HASH_LEN };

        // Combine params into a single number in base 2^32 (matching Rust)
        var acc: u128 = 0;
        for (domain_params) |param| {
            acc = (acc << 32) | (@as(u128, param));
        }

        // Compute base-p decomposition to 24 elements (matching Rust)
        // Rust uses F::from_u64(digit) which converts to Montgomery, so we need to do the same
        // (using p already declared above for tweak computation)
        const Poseidon24 = @import("../../poseidon2/poseidon2.zig").Poseidon2KoalaBear24Plonky3;
        const F = Poseidon24.Field;
        var input_24_monty: [24]F = undefined;
        var remaining = acc;
        for (0..24) |i| {
            const digit = remaining % p;
            input_24_monty[i] = F.fromU32(@as(u32, @intCast(digit))); // Convert to Montgomery (matching Rust F::from_u64)
            remaining /= p;
        }

        // Use poseidon_compress directly with Montgomery values (matching Rust's poseidon_compress)
        // Rust's poseidon_compress takes &[F] (Montgomery) and returns [F; OUT_LEN] (Montgomery)
        const CAPACITY: usize = self.lifetime_params.capacity; // From lifetime_params (9 for lifetime 2^8, 2^18, 2^32)
        var padded_input_monty: [24]F = undefined;
        @memcpy(&padded_input_monty, &input_24_monty);

        // Apply permutation
        Poseidon24.permutation(&padded_input_monty);

        // Feed-forward: Add the input back into the state element-wise (matching Rust's poseidon_compress)
        for (0..24) |i| {
            padded_input_monty[i] = padded_input_monty[i].add(input_24_monty[i]);
        }

        // Extract capacity_value in Montgomery form (matching Rust's return type [F; OUT_LEN])
        // Use stack allocation instead of heap
        // CAPACITY is 9, so 9 elements = 36 bytes (safe for stack)
        var capacity_value_monty_stack: [9]F = undefined;
        const capacity_value_monty = capacity_value_monty_stack[0..CAPACITY];
        for (0..CAPACITY) |i| {
            capacity_value_monty[i] = padded_input_monty[i];
        }

        // Debug: log capacity_value (in Montgomery form, print as canonical for comparison)
        log.print("ZIG_SPONGE_DEBUG: Capacity value ({} elements, canonical): ", .{CAPACITY});
        for (capacity_value_monty) |fe| {
            log.print("0x{x:0>8} ", .{fe.toU32()});
        }
        log.print("\n", .{});
        log.print("DEBUG: Sponge capacity_value ({} elements, Montgomery->canonical): ", .{CAPACITY});
        for (capacity_value_monty, 0..) |fe, i| {
            log.print("{}:0x{x}", .{ i, fe.toU32() });
            if (i < CAPACITY - 1) log.print(", ", .{});
        }
        log.print("\n", .{});

        // Combine parameter + tweak + flattened input (matching Rust's poseidon_sponge input)
        // Rust passes everything in Montgomery form directly to poseidon_sponge
        // Chain ends are already in Montgomery form (from Poseidon2-16 compress)
        // Parameter and tweak need to be converted to Montgomery
        const combined_input_len = self.lifetime_params.parameter_len + self.lifetime_params.tweak_len_fe + flattened_len;

        // Debug: print lengths
        log.print("ZIG_SPONGE_DEBUG: Lengths - parameter_len={}, tweak_len_fe={}, flattened_len={}, combined_input_len={}\n", .{ self.lifetime_params.parameter_len, self.lifetime_params.tweak_len_fe, flattened_len, combined_input_len });

        // Use stack allocation instead of heap
        // Max size: 5 (param) + 2 (tweak) + 512 (flattened) = 519 elements = ~2KB (safe for stack)
        var combined_input_monty_stack: [600]F = undefined; // 600 to allow for padding
        const combined_input_monty = combined_input_monty_stack[0..combined_input_len];

        var input_idx: usize = 0;
        // Add parameter (values already stored in Montgomery form)
        for (parameter) |fe| {
            combined_input_monty[input_idx] = F{ .value = fe.value };
            input_idx += 1;
        }
        // Add tweak (already in Montgomery form)
        for (tweak) |fe| {
            combined_input_monty[input_idx] = F{ .value = fe.value };
            input_idx += 1;
        }
        // Add flattened input (chain ends - already in Montgomery form from chain walking)
        // Chain ends are stored as Montgomery u32 values in FieldElement.value
        // We need to create F directly from the Montgomery value (not convert canonical to Montgomery)
        for (flattened_input) |fe| {
            // fe.value is already in Montgomery form (from chain walking)
            // Create F directly with this Montgomery value (F{ .value = ... } creates F with Montgomery value)
            combined_input_monty[input_idx] = F{ .value = fe.value };
            input_idx += 1;
        }

        // Debug: print first RATE elements of combined input (in canonical form for comparison)
        log.print("ZIG_SPONGE_DEBUG: Combined input (first {} elements, canonical): ", .{@min(15, combined_input_monty.len)});
        for (0..@min(15, combined_input_monty.len)) |i| {
            log.print("0x{x:0>8} ", .{combined_input_monty[i].toU32()});
        }
        log.print("\n", .{});
        log.print("DEBUG: Sponge combined_input head RATE ({}): ", .{15});
        for (0..@min(15, combined_input_monty.len)) |i| {
            log.print("{}:0x{x}", .{ i, combined_input_monty[i].toU32() });
            if (i + 1 < @min(15, combined_input_monty.len)) log.print(", ", .{});
        }
        log.print("\n", .{});

        // Apply Poseidon2-24 sponge (matching Rust's poseidon_sponge)
        const WIDTH: usize = 24;
        const RATE: usize = WIDTH - CAPACITY; // 24 - capacity
        const OUTPUT_LEN: usize = self.lifetime_params.hash_len_fe; // Domain size (7 for lifetime 2^18, 8 for lifetime 2^8)

        // Pad input to multiple of rate (matching Rust's input_vector.resize)
        const input_remainder = combined_input_monty.len % RATE;
        const extra_elements = if (input_remainder == 0) 0 else (RATE - input_remainder) % RATE;
        const padded_input_len = combined_input_monty.len + extra_elements;
        // Use stack allocation instead of heap
        // Max size: 519 + 15 (max padding) = 534 elements = ~2KB (safe for stack)
        var padded_input_stack: [600]F = undefined;
        const padded_input = padded_input_stack[0..padded_input_len];
        @memcpy(padded_input[0..combined_input_monty.len], combined_input_monty);
        // Pad with zeros (in Montgomery form)
        for (combined_input_monty.len..padded_input.len) |i| {
            padded_input[i] = F.zero; // Zero in Montgomery is still zero
        }

        // Initialize state: capacity in capacity part, zeros in rate part
        // Use Montgomery form throughout (matching Rust's KoalaBear which uses Montgomery internally)
        // capacity_value_monty is already in Montgomery form (from poseidon_compress)
        var state: [WIDTH]F = undefined;

        // Initialize rate part with zeros, capacity part with capacity_value (both in Montgomery)
        // Rust: state[rate..].copy_from_slice(capacity_value) means state[15..24] = capacity_value[0..9]
        for (0..RATE) |i| {
            state[i] = F.zero; // Zero in Montgomery is still zero
        }
        // state[RATE + i] = capacity_value_monty[i]
        //   state[15 + 0] = capacity_value[0] -> state[15] = capacity_value[0] ✓
        //   state[15 + 1] = capacity_value[1] -> state[16] = capacity_value[1] ✓
        //   ...
        //   state[15 + 8] = capacity_value[8] -> state[23] = capacity_value[8] ✓
        for (0..CAPACITY) |i| {
            state[RATE + i] = capacity_value_monty[i]; // Already in Montgomery form
        }

        // Debug: verify capacity placement immediately after initialization
        log.print("ZIG_SPONGE_DEBUG: Verify state[15] = capacity[0]: state[15]=0x{x:0>8} capacity[0]=0x{x:0>8}\n", .{ state[15].toU32(), capacity_value_monty[0].toU32() });
        log.print("ZIG_SPONGE_DEBUG: Verify state[23] = capacity[8]: state[23]=0x{x:0>8} capacity[8]=0x{x:0>8}\n", .{ state[23].toU32(), capacity_value_monty[8].toU32() });

        // Debug: print initial state (after initialization, before absorption)
        log.print("ZIG_SPONGE_DEBUG: Initial state (canonical): ", .{});
        for (0..WIDTH) |i| {
            log.print("0x{x:0>8} ", .{state[i].toU32()});
        }
        log.print("\n", .{});
        log.print("ZIG_SPONGE_DEBUG: Initial state (canonical): ", .{});
        for (0..WIDTH) |i| {
            log.print("{}:0x{x}", .{ i, state[i].toU32() });
            if (i < WIDTH - 1) log.print(", ", .{});
        }
        log.print("\n", .{});

        // Absorb: process padded input in chunks of RATE (matching Rust's poseidon_sponge)
        // Rust's KoalaBear uses Montgomery form internally, so convert canonical input to Montgomery before adding
        log.print("DEBUG: Sponge padded_input_len={} rate={}\n", .{ padded_input.len, RATE });
        var chunk_start: usize = 0;
        var chunk_num: usize = 0;
        const total_chunks = (padded_input.len + RATE - 1) / RATE;
        log.print("ZIG_SPONGE_DEBUG: Total chunks: {}\n", .{total_chunks});

        while (chunk_start < padded_input.len) {
            const chunk_end = @min(chunk_start + RATE, padded_input.len);
            const actual_chunk_len = chunk_end - chunk_start;

            // Debug: print input values for first few and last few chunks (in canonical form for comparison)
            if (chunk_num < 3 or chunk_num >= total_chunks - 3) {
                log.print("ZIG_SPONGE_DEBUG: Input chunk {} (first {} elements, canonical): ", .{ chunk_num, @min(8, actual_chunk_len) });
                for (0..@min(8, actual_chunk_len)) |i| {
                    log.print("0x{x:0>8} ", .{padded_input[chunk_start + i].toU32()});
                }
                log.print("\n", .{});
            }

            // Add chunk to rate part of state (state[0..RATE])
            // Input is already in Montgomery form, so add directly (matching Rust's state[i] += chunk[i])
            for (0..actual_chunk_len) |i| {
                state[i] = state[i].add(padded_input[chunk_start + i]);
            }

            // Debug: print state after adding chunk (before permutation) for first few and last few chunks
            if (chunk_num < 3 or chunk_num >= total_chunks - 3) {
                log.print("ZIG_SPONGE_DEBUG: State after adding chunk {} (before perm, first 8): ", .{chunk_num});
                for (0..@min(8, WIDTH)) |i| {
                    log.print("0x{x:0>8} ", .{state[i].toU32()});
                }
                log.print("\n", .{});
            }

            // Permute state (matching Rust's perm.permute_mut(&mut state))
            // Rust's permute_mut calls: external_layer.permute_state_initial (includes MDS light + 4 rounds)
            //                           internal_layer.permute_state (23 rounds)
            //                           external_layer.permute_state_terminal (4 rounds)
            // So we should use the full permutation WITH MDS light (matching Rust)
            Poseidon24.permutation(state[0..]);

            // Debug: print state after permutation for first few and last few chunks
            if (chunk_num < 3 or chunk_num >= total_chunks - 3) {
                log.print("ZIG_SPONGE_DEBUG: State after chunk {} perm (first 8): ", .{chunk_num});
                for (0..@min(8, WIDTH)) |i| {
                    log.print("0x{x:0>8} ", .{state[i].toU32()});
                }
                log.print("\n", .{});
            }

            chunk_start = chunk_end;
            chunk_num += 1;
        }

        // Debug: print state after all absorptions (before squeeze)
        log.print("ZIG_SPONGE_DEBUG: State after all absorptions (canonical): ", .{});
        for (0..WIDTH) |i| {
            log.print("0x{x:0>8} ", .{state[i].toU32()});
        }
        log.print("\n", .{});

        // Squeeze: extract OUTPUT_LEN elements from rate part (matching Rust's squeeze exactly)
        // Rust's squeeze: while out.len() < OUT_LEN { out.extend_from_slice(&state[..rate]); perm.permute_mut(&mut state); }
        // Since OUTPUT_LEN=8 < RATE=15, it reads 15 elements, then permutes, then takes first 8
        var out: std.ArrayList(F) = .{};
        defer out.deinit(self.allocator);

        while (out.items.len < OUTPUT_LEN) {
            // Read from state[0..rate] (15 elements)
            try out.appendSlice(self.allocator, state[0..RATE]);
            // Debug: print state before squeeze permutation
            log.print("ZIG_SPONGE_DEBUG: State before squeeze perm (canonical): ", .{});
            for (0..WIDTH) |i| {
                log.print("{}:0x{x}", .{ i, state[i].toU32() });
                if (i < WIDTH - 1) log.print(", ", .{});
            }
            log.print("\n", .{});
            // Permute state (matching Rust's perm.permute_mut(&mut state))
            // Rust's permute_mut includes MDS light (in permute_state_initial)
            Poseidon24.permutation(state[0..]);
            // Debug: print state after squeeze permutation
            log.print("ZIG_SPONGE_DEBUG: State after squeeze perm (canonical): ", .{});
            for (0..WIDTH) |i| {
                log.print("{}:0x{x}", .{ i, state[i].toU32() });
                if (i < WIDTH - 1) log.print(", ", .{});
            }
            log.print("\n", .{});
        }

        // OPTIMIZATION: Write directly to output buffer instead of allocating
        // Take first OUTPUT_LEN elements (matching Rust's &out[0..OUT_LEN])
        std.debug.assert(output.len >= OUTPUT_LEN);
        for (0..OUTPUT_LEN) |i| {
            output[i] = FieldElement.fromMontgomery(out.items[i].value);
        }

        log.print("DEBUG: Sponge leaf domain ({} elements): ", .{OUTPUT_LEN});
        for (output[0..OUTPUT_LEN], 0..) |fe, i| {
            log.print("{}:0x{x}", .{ i, fe.value });
            if (i < OUTPUT_LEN - 1) log.print(", ", .{});
        }
        log.print("\n", .{});
    }

    /// SIMD version: Reduce chain domains to leaf domains for all epochs in a batch simultaneously
    /// This matches Rust's poseidon_sponge with PackedF inputs
    /// Processes SIMD_WIDTH epochs in parallel using SIMD operations
    /// packed_chains: [num_chains][hash_len]PackedF - chain ends for all epochs (vertical packing)
    /// packed_epochs: @Vector(SIMD_WIDTH, u32) - epochs in this batch
    /// packed_parameter: [5]PackedF - parameter broadcast to all lanes
    /// output: [SIMD_WIDTH][hash_len]FieldElement - output buffer for all epochs
    inline fn reduceChainDomainsToLeafDomainSIMD(
        self: *GeneralizedXMSSSignatureScheme,
        simd_poseidon2: *poseidon2_simd.Poseidon2SIMD,
        packed_chains: [][]simd_utils.PackedF, // [num_chains][hash_len]PackedF
        packed_epochs: @Vector(simd_utils.SIMD_WIDTH, u32),
        packed_parameter: [5]simd_utils.PackedF,
        output: [][8]FieldElement, // [SIMD_WIDTH][8]FieldElement - output buffer
    ) !void {
        const SIMD_WIDTH = simd_utils.SIMD_WIDTH;
        const PARAMETER_LEN: u32 = @intCast(self.lifetime_params.parameter_len);
        const TWEAK_LEN: u32 = @intCast(self.lifetime_params.tweak_len_fe);
        const NUM_CHUNKS: u32 = @intCast(packed_chains.len);
        const HASH_LEN: u32 = @intCast(self.lifetime_params.hash_len_fe);
        const hash_len = self.lifetime_params.hash_len_fe;
        const field = @import("../../core/field.zig");
        const p: u128 = 2130706433; // KoalaBear field modulus

        // STEP 1: Generate tree tweaks for all epochs in batch
        // Each lane gets a tweak specific to its epoch
        var packed_tree_tweak: [2]simd_utils.PackedF = undefined;
        for (0..TWEAK_LEN) |t_idx| {
            var tweak_values: [SIMD_WIDTH]u32 = undefined;
            for (0..SIMD_WIDTH) |lane| {
                const epoch = packed_epochs[lane];
                const tweak_level: u8 = 0;
                const tweak_bigint = (@as(u128, tweak_level) << 40) | (@as(u128, epoch) << 8) | field.TWEAK_SEPARATOR_FOR_TREE_HASH;
                const tweak_fe = if (t_idx == 0)
                    FieldElement.fromCanonical(@intCast(tweak_bigint % p))
                else
                    FieldElement.fromCanonical(@intCast((tweak_bigint / p) % p));
                tweak_values[lane] = tweak_fe.value; // Already in Montgomery form
            }
            packed_tree_tweak[t_idx] = simd_utils.PackedF{ .values = tweak_values };
        }

        // STEP 2: Create domain separator (same for all epochs)
        // Rust computes this with PackedF, but since all lanes have same value, scalar+broadcast should be equivalent
        // However, to match Rust exactly, we should compute with SIMD
        const DOMAIN_PARAMETERS_LENGTH: usize = 4;
        const domain_params: [DOMAIN_PARAMETERS_LENGTH]u32 = [4]u32{ PARAMETER_LEN, TWEAK_LEN, NUM_CHUNKS, HASH_LEN };
        var acc: u128 = 0;
        for (domain_params) |param| {
            acc = (acc << 32) | (@as(u128, param));
        }
        const Poseidon24 = @import("../../poseidon2/poseidon2.zig").Poseidon2KoalaBear24Plonky3;
        const F = Poseidon24.Field;
        var input_24_monty: [24]F = undefined;
        var remaining = acc;
        for (0..24) |i| {
            const digit = remaining % p;
            input_24_monty[i] = F.fromU32(@as(u32, @intCast(digit)));
            remaining /= p;
        }
        var padded_input_monty: [24]F = undefined;
        @memcpy(&padded_input_monty, &input_24_monty);
        Poseidon24.permutation(&padded_input_monty);
        for (0..24) |i| {
            padded_input_monty[i] = padded_input_monty[i].add(input_24_monty[i]);
        }
        const CAPACITY: usize = self.lifetime_params.capacity;
        var capacity_value_monty_stack: [9]F = undefined;
        const capacity_value_monty = capacity_value_monty_stack[0..CAPACITY];
        for (0..CAPACITY) |i| {
            capacity_value_monty[i] = padded_input_monty[i];
        }

        // Pack capacity value to all SIMD lanes (same for all epochs)
        // since all lanes have the same value. But let's verify this is correct.
        var packed_capacity_value: [9]simd_utils.PackedF = undefined;
        for (0..CAPACITY) |i| {
            packed_capacity_value[i] = simd_utils.PackedF{ .values = @splat(capacity_value_monty[i].value) };
        }

        // STEP 4: Assemble packed leaf input: [parameter | tree_tweak | all_chain_ends]
        // Layout matches Rust: packed_parameter.iter().chain(packed_tree_tweak.iter()).chain(packed_chains.iter().flatten())
        const packed_leaf_input_len = PARAMETER_LEN + TWEAK_LEN + (NUM_CHUNKS * HASH_LEN);
        var packed_leaf_input_stack: [600]simd_utils.PackedF = undefined; // Max: 5 + 2 + 64*8 = 519
        const packed_leaf_input = packed_leaf_input_stack[0..packed_leaf_input_len];
        var input_idx: usize = 0;

        // Add parameter (already packed)
        @memcpy(packed_leaf_input[input_idx .. input_idx + PARAMETER_LEN], &packed_parameter);
        input_idx += PARAMETER_LEN;

        // Add tree tweak (packed)
        @memcpy(packed_leaf_input[input_idx .. input_idx + TWEAK_LEN], &packed_tree_tweak);
        input_idx += TWEAK_LEN;

        // Add all chain ends (already packed)
        for (packed_chains) |packed_chain| {
            @memcpy(packed_leaf_input[input_idx .. input_idx + hash_len], packed_chain[0..hash_len]);
            input_idx += hash_len;
        }

        // STEP 5: Apply SIMD sponge hash (processes all epochs simultaneously)
        const WIDTH: usize = 24;
        const RATE: usize = WIDTH - CAPACITY;

        // Pad input to multiple of rate (matching Rust: (rate - (input.len() % rate)) % rate)
        const original_input_len = packed_leaf_input.len;
        const extra_elements = (RATE - (original_input_len % RATE)) % RATE;
        const padded_input_len = original_input_len + extra_elements;
        // OPTIMIZATION: Align for SIMD (16-byte for NEON/SSE, 32-byte for AVX-512)
        const align_bytes_input = if (SIMD_WIDTH == 8) 32 else 16;
        var padded_leaf_input_stack: [600]simd_utils.PackedF align(align_bytes_input) = undefined;
        const padded_leaf_input = padded_leaf_input_stack[0..padded_input_len];
        @memcpy(padded_leaf_input[0..original_input_len], packed_leaf_input);
        // Pad with zeros
        for (original_input_len..padded_input_len) |i| {
            padded_leaf_input[i] = simd_utils.PackedF{ .values = @splat(@as(u32, 0)) };
        }

        // Initialize state: capacity in capacity part, zeros in rate part
        // OPTIMIZATION: Initialize all to zero first, then set capacity part
        // OPTIMIZATION: Explicitly align for SIMD (16-byte for NEON/SSE, 32-byte for AVX-512)
        // Note: SIMD_WIDTH is already declared in function scope above
        const align_bytes = if (SIMD_WIDTH == 8) 32 else 16;
        var packed_state: [24]simd_utils.PackedF align(align_bytes) = undefined;
        const zero_packed = simd_utils.PackedF{ .values = @splat(@as(u32, 0)) };
        // Initialize all elements to zero (loop unrolled for efficiency)
        for (&packed_state) |*elem| {
            elem.* = zero_packed;
        }
        // Set capacity part
        for (0..CAPACITY) |i| {
            packed_state[RATE + i] = packed_capacity_value[i];
        }

        // Absorb: process padded input in chunks of RATE
        // OPTIMIZATION: Pre-compute prime vector outside loop
        const prime_vec: @Vector(simd_utils.SIMD_WIDTH, u32) = @splat(KOALABEAR_PRIME);
        var chunk_start: usize = 0;
        while (chunk_start < padded_leaf_input.len) {
            const chunk_end = @min(chunk_start + RATE, padded_leaf_input.len);
            const actual_chunk_len = chunk_end - chunk_start;

            // Add chunk to rate part of state (SIMD addition with modular reduction)
            // OPTIMIZED: Batch process additions with SIMD modular reduction
            // Use modular addition to match Rust's field addition behavior
            // Rust's PackedF addition uses field addition which includes modular reduction
            // Algorithm: sum = a +% b; if sum >= p then sum -= p
            // In Montgomery form, values are in [0, 2p), so after addition they're in [0, 4p)
            // We only need one subtraction since 4p - p = 3p < 2^32 (p = 0x7f000001)
            for (0..actual_chunk_len) |i| {
                var sum = packed_state[i].values +% padded_leaf_input[chunk_start + i].values;
                // SIMD modular reduction: if sum >= PRIME then sum -= PRIME
                // Use SIMD comparison and conditional subtraction (single instruction on most CPUs)
                const ge_mask = sum >= prime_vec;
                sum = @select(u32, ge_mask, sum -% prime_vec, sum);
                packed_state[i] = simd_utils.PackedF{ .values = sum };
            }

            // Permute state using SIMD Poseidon2-24 (permutation only, no feed-forward for sponge)
            simd_poseidon2.permute24SIMDFromPackedF(&packed_state);

            chunk_start = chunk_end;
        }

        // Squeeze: extract OUTPUT_LEN elements from rate part (matching Rust's squeeze exactly)
        // OPTIMIZED: Since hash_len <= 8 and RATE = 15, we typically only need one iteration
        // Rust: while out.len() < OUT_LEN { out.extend_from_slice(&state[..rate]); perm.permute_mut(&mut state); }
        // Use stack allocation since we know max size needed (hash_len <= 8, RATE = 15, so max 15 elements)
        var packed_out_stack: [15]simd_utils.PackedF = undefined;
        var packed_out_len: usize = 0;

        // OPTIMIZATION: Unroll first iteration since hash_len <= 8 < RATE = 15
        // This avoids the loop overhead for the common case
        if (hash_len > 0) {
            // First iteration: read min(hash_len, RATE) elements
            const first_read_len = @min(hash_len, RATE);
            @memcpy(packed_out_stack[0..first_read_len], packed_state[0..first_read_len]);
            packed_out_len = first_read_len;

            // Only permute if we need more output (unlikely but matches Rust's behavior)
            if (packed_out_len < hash_len) {
                simd_poseidon2.permute24SIMDFromPackedF(&packed_state);
                // Second iteration: read remaining elements (shouldn't happen for hash_len <= 8)
                const remaining_len = hash_len - packed_out_len;
                @memcpy(packed_out_stack[packed_out_len .. packed_out_len + remaining_len], packed_state[0..remaining_len]);
                packed_out_len = hash_len;
            }
        }

        // STEP 6: Unpack results to output buffer (truncate to OUTPUT_LEN, matching Rust's slice[0..OUT_LEN])
        // Rust truncates to OUT_LEN: let slice = &out[0..OUT_LEN];
        for (0..SIMD_WIDTH) |lane| {
            for (0..hash_len) |h| {
                output[lane][h] = FieldElement.fromMontgomery(packed_out_stack[h].values[lane]);
            }
            for (hash_len..8) |h| {
                output[lane][h] = FieldElement.zero();
            }
        }
    }

    /// Build bottom tree from leaf hashes and return as array of 8 field elements
    /// This matches the Rust HashSubTree::new_subtree algorithm exactly
    pub fn buildBottomTree(self: *GeneralizedXMSSSignatureScheme, leaf_hashes: []FieldElement, parameter: [5]FieldElement, bottom_tree_index: usize) ![8]FieldElement {
        // For bottom trees: build full 8-layer tree (0->8), then truncate to 4 layers (0->4)
        // This matches Rust: new_subtree builds 0->8, then truncates to 0->4
        const full_depth = 8; // Build full 8-layer tree like Rust
        // TODO: Implement proper truncation to 4 layers like Rust
        const lowest_layer = 0;
        const start_index = bottom_tree_index * 16; // Each bottom tree has 16 leaves

        log.print("DEBUG: Building bottom tree from layer {} to layer {}\n", .{ lowest_layer, full_depth });
        log.print("DEBUG: Starting with {} leaf hashes\n", .{leaf_hashes.len});

        // Convert single field elements to arrays of 8 field elements
        var leaf_nodes = try self.allocator.alloc([8]FieldElement, leaf_hashes.len);
        defer self.allocator.free(leaf_nodes);

        for (0..leaf_hashes.len) |i| {
            // Convert single field element to array of 8 field elements
            // First element is the actual value, rest are zeros
            leaf_nodes[i][0] = leaf_hashes[i];
            for (1..8) |j| {
                leaf_nodes[i][j] = FieldElement{ .value = 0 };
            }
        }

        // Start with the lowest layer, padded accordingly (matching Rust HashTreeLayer::padded)
        // Use dummy RNG for bottom trees (matching Rust implementation)
        // Rust uses StdRng::seed_from_u64(0) for bottom trees because they're full and padding is removed
        // This allows parallel building without affecting RNG determinism
        var dummy_rng = std.Random.DefaultPrng.init(0);
        const dummy_rng_random = dummy_rng.random();
        const initial_padded = try self.padLayerWithRng(leaf_nodes, start_index, &dummy_rng_random);

        log.print("DEBUG: Initial padding: {} nodes (start_index: {})\n", .{ initial_padded.nodes.len, initial_padded.start_index });

        // Build tree layer by layer (matching Rust exactly)
        // Track all layers for proper truncation
        var layers: std.ArrayList(PaddedLayer) = .{};
        defer {
            for (layers.items) |layer| self.allocator.free(layer.nodes);
            layers.deinit(self.allocator);
        }

        var current_layer = initial_padded;
        var current_level: usize = lowest_layer;

        while (current_level < full_depth) {
            const next_level = current_level + 1;

            log.print("DEBUG: Zig Layer {} -> {}: {} nodes (start_index: {})\n", .{ current_level, next_level, current_layer.nodes.len, current_layer.start_index });

            // Parent layer starts at half the previous start index (matching Rust)
            const parent_start = current_layer.start_index >> 1;

            // Compute all parents by pairing children two-by-two (matching Rust par_chunks_exact(2))
            const parents_len = current_layer.nodes.len / 2; // This is guaranteed to be exact due to padding
            const parents = try self.allocator.alloc([8]FieldElement, parents_len);

            log.print("DEBUG: Processing {} nodes to get {} parents\n", .{ current_layer.nodes.len, parents_len });

            // Process all pairs in parallel (matching Rust par_chunks_exact(2))
            try self.processPairsInParallel(current_layer.nodes, parents, parent_start, current_level, parameter);

            // Free the current layer before creating the new one
            self.allocator.free(current_layer.nodes);

            // Add the new layer with padding so next iteration also has even start and length (matching Rust)
            // Use dummy RNG for bottom trees (matching Rust implementation)
            const new_layer = try self.padLayerWithRng(parents, parent_start, &dummy_rng_random);
            self.allocator.free(parents);

            current_layer = new_layer;

            log.print("DEBUG: After padding: {} nodes (start_index: {})\n", .{ current_layer.nodes.len, current_layer.start_index });

            // Store this layer for truncation
            // We need to store a copy of the layer, not the original
            const layer_copy = PaddedLayer{
                .nodes = try self.allocator.alloc([8]FieldElement, current_layer.nodes.len),
                .start_index = current_layer.start_index,
            };
            @memcpy(layer_copy.nodes, current_layer.nodes);
            try layers.append(self.allocator, layer_copy);

            current_level = next_level;
        }

        // Truncate to final_depth = 4 layers like Rust does
        // Rust truncates to depth/2 = 4 layers and gets root from layer 4
        // According to Rust: bottom_tree_root = bottom_tree.layers[depth / 2].nodes[bottom_tree_index % 2]
        // where depth = 8, so depth/2 = 4, and we need the root from layer 4

        // Get the root from layer 4
        // layers.items[0] = result of layer 0->1 (i.e., layer 1)
        // layers.items[1] = layer 2
        // layers.items[2] = layer 3
        // layers.items[3] = layer 4
        // layers.items[4] = layer 5
        // So layer 4 is at index 3, not 4!
        const target_layer_index = (full_depth / 2) - 1; // 8 / 2 - 1 = 3
        log.print("DEBUG: Looking for root in layer {} (stored layers: {})\n", .{ target_layer_index + 1, layers.items.len });

        if (target_layer_index >= layers.items.len) {
            log.print("ERROR: target_layer_index {} >= layers.len {}\n", .{ target_layer_index, layers.items.len });
            // Fallback to current layer
            const truncated_root = current_layer.nodes[0];
            log.print("DEBUG: Using fallback root from current layer: {any}\n", .{truncated_root});
            self.allocator.free(current_layer.nodes);
            return truncated_root;
        }

        const target_layer = layers.items[target_layer_index];
        log.print("DEBUG: Target layer has {} nodes\n", .{target_layer.nodes.len});

        // Get the root from the correct position in layer 4
        // Rust uses: bottom_tree_index % 2 to select which of the 2 nodes to use
        const root_index = bottom_tree_index % 2;
        const truncated_root = target_layer.nodes[root_index];
        log.print("DEBUG: Final bottom tree root array (truncated from 8-layer to 4-layer, from layer {}): {any}\n", .{ target_layer_index, truncated_root });

        // Free the final layer
        self.allocator.free(current_layer.nodes);

        return truncated_root;
    }

    /// Build top tree from bottom tree roots
    fn buildTopTree(
        self: *GeneralizedXMSSSignatureScheme,
        bottom_tree_roots: [][8]FieldElement,
        parameter: [5]FieldElement,
        start_bottom_tree_index: usize,
    ) !*HashSubTree {
        const root_array = try self.buildTopTreeAsArray(bottom_tree_roots, parameter, start_bottom_tree_index);
        // Use the entire array as the root for the HashSubTree
        return try HashSubTree.init(self.allocator, root_array);
    }

    fn computePathFromLayers(
        self: *GeneralizedXMSSSignatureScheme,
        layers: []const PaddedLayer,
        position_initial: u32,
    ) ![][8]FieldElement {
        var co_path = try self.allocator.alloc([8]FieldElement, layers.len);
        var co_len: usize = 0;
        var current_position = position_initial;
        var l: usize = 0;
        // For bottom trees: depth = log_lifetime / 2, so we should walk depth levels
        // For top trees: depth = log_lifetime / 2 as well
        // Stop when we've walked through all non-root layers (layers.len - 1) or when we hit root
        while (l < layers.len) : (l += 1) {
            const layer = layers[l];
            // Stop if we've reached the root layer (1 or fewer nodes)
            // But also check if this is the last layer - if so, it's the root
            if (layer.nodes.len <= 1 or l >= layers.len - 1) break;

            const sibling_position: u32 = current_position ^ 0x01;
            const sibling_index_in_vec_u32: u32 = sibling_position - @as(u32, @intCast(layer.start_index));
            const sibling_index = @as(usize, @intCast(sibling_index_in_vec_u32));

            // Debug: For epoch 0, bottom tree 0, first layer, log the path computation
            if (position_initial == 0 and l == 0 and layer.start_index == 0) {
                log.debugPrint("ZIG_PATH_DEBUG: Epoch 0, layer 0: current_position={}, sibling_position={}, layer.start_index={}, sibling_index={}, layer.nodes.len={}, sibling_node[0]=0x{x:0>8}\n", .{ current_position, sibling_position, layer.start_index, sibling_index, layer.nodes.len, layer.nodes[sibling_index][0].value });
            }
            // Debug: For epoch 0, bottom tree 0, layer 1, log the path computation
            if (position_initial == 0 and l == 1 and layer.start_index == 0) {
                log.debugPrint("ZIG_PATH_DEBUG: Epoch 0, layer 1: current_position={}, sibling_position={}, layer.start_index={}, sibling_index={}, layer.nodes.len={}, sibling_node[0]=0x{x:0>8}\n", .{ current_position, sibling_position, layer.start_index, sibling_index, layer.nodes.len, layer.nodes[sibling_index][0].value });
            }

            // Debug logging for epoch 16, bottom tree (layers.len == 4)
            if (position_initial == 16 and layers.len == 4 and l == 0) {
                const current_node_index = current_position - @as(u32, @intCast(layer.start_index));
                const current_node_val = if (current_node_index < layer.nodes.len) layer.nodes[@as(usize, @intCast(current_node_index))][0].value else 0;
                log.print("ZIG_SIGN: computePathFromLayers epoch {} layer {}: current_position={}, sibling_position={}, layer.start_index={}, sibling_index={}, layer.nodes.len={}, current_node[0]=0x{x:0>8}, sibling_node[0]=0x{x:0>8}\n", .{ position_initial, l, current_position, sibling_position, layer.start_index, sibling_index, layer.nodes.len, current_node_val, layer.nodes[sibling_index][0].value });
            }

            // Debug logging for top tree path computation
            if (layers.len > 4) { // Likely top tree (more layers than bottom tree)
                log.print("ZIG_SIGN_DEBUG: computePathFromLayers layer {}: current_position={}, sibling_position={}, layer.start_index={}, sibling_index={}, layer.nodes.len={}\n", .{ l, current_position, sibling_position, layer.start_index, sibling_index, layer.nodes.len });
            }

            if (sibling_index >= layer.nodes.len) {
                log.print("ZIG_SIGN_DEBUG: ERROR: sibling_index {} >= layer.nodes.len {} (layer {}, start_index={}, current_position={})\n", .{ sibling_index, layer.nodes.len, l, layer.start_index, current_position });
                return error.InvalidPathComputation;
            }

            co_path[co_len] = layer.nodes[sibling_index];
            co_len += 1;
            current_position >>= 1;
        }

        const out = try self.allocator.alloc([8]FieldElement, co_len);
        @memcpy(out, co_path[0..co_len]);
        self.allocator.free(co_path);
        return out;
    }

    fn buildBottomTreeLayers(
        self: *GeneralizedXMSSSignatureScheme,
        leaf_hashes: []FieldElement,
        parameter: [5]FieldElement,
        bottom_tree_index: usize,
    ) ![]PaddedLayer {
        const full_depth = 8;
        const start_index = bottom_tree_index * 16;

        var layers: std.ArrayList(PaddedLayer) = .{};
        errdefer {
            for (layers.items) |pl| self.allocator.free(pl.nodes);
            layers.deinit(self.allocator);
        }

        var leaf_nodes = try self.allocator.alloc([8]FieldElement, leaf_hashes.len);
        defer self.allocator.free(leaf_nodes);
        for (0..leaf_hashes.len) |i| {
            leaf_nodes[i][0] = leaf_hashes[i];
            for (1..8) |j| leaf_nodes[i][j] = FieldElement{ .value = 0 };
        }

        // Use dummy RNG for bottom trees (matching Rust implementation)
        // Rust uses StdRng::seed_from_u64(0) for bottom trees because they're full and padding is removed
        // This allows parallel building without affecting RNG determinism
        var dummy_rng = std.Random.DefaultPrng.init(0);
        const dummy_rng_random = dummy_rng.random();
        var current_layer = try self.padLayerWithRng(leaf_nodes, start_index, &dummy_rng_random);
        try layers.append(self.allocator, .{ .nodes = try self.allocator.alloc([8]FieldElement, current_layer.nodes.len), .start_index = current_layer.start_index });
        @memcpy(layers.items[layers.items.len - 1].nodes, current_layer.nodes);

        // Debug: For bottom tree 0, verify that layer.nodes[1] matches the leaf domain for epoch 1
        if (bottom_tree_index == 0 and current_layer.nodes.len > 1) {
            log.debugPrint("ZIG_TREEBUILD_LAYER: Bottom tree 0, first layer: nodes[0][0]=0x{x:0>8}, nodes[1][0]=0x{x:0>8}\n", .{ current_layer.nodes[0][0].value, current_layer.nodes[1][0].value });
        }

        var current_level: usize = 0;
        while (current_level < full_depth) : (current_level += 1) {
            const parent_start = current_layer.start_index >> 1;
            const parents_len = current_layer.nodes.len / 2;
            const parents = try self.allocator.alloc([8]FieldElement, parents_len);
            try self.processPairsInParallel(current_layer.nodes, parents, parent_start, current_level, parameter);
            self.allocator.free(current_layer.nodes);
            const new_layer = try self.padLayerWithRng(parents, parent_start, &dummy_rng_random);
            self.allocator.free(parents);
            current_layer = new_layer;
            try layers.append(self.allocator, .{ .nodes = try self.allocator.alloc([8]FieldElement, current_layer.nodes.len), .start_index = current_layer.start_index });
            @memcpy(layers.items[layers.items.len - 1].nodes, current_layer.nodes);
        }

        self.allocator.free(current_layer.nodes);

        return layers.toOwnedSlice(self.allocator);
    }

    inline fn buildBottomTreeLayersFromLeafDomains(
        self: *GeneralizedXMSSSignatureScheme,
        leaf_nodes_in: [][8]FieldElement,
        parameter: [5]FieldElement,
        bottom_tree_index: usize,
    ) ![]PaddedLayer {
        // Bottom trees should have depth = log_lifetime / 2 = 4 for lifetime 2^8
        // Rust builds full_depth = 8 but truncates to depth/2 = 4
        // We should only build 4 layers to match the actual bottom tree structure
        const full_depth = self.lifetime_params.log_lifetime / 2; // 4 for lifetime 2^8
        const leafs_per_bottom_tree = @as(usize, 1) << @intCast(self.lifetime_params.log_lifetime / 2);
        const start_index = bottom_tree_index * leafs_per_bottom_tree;

        var layers: std.ArrayList(PaddedLayer) = .{};
        errdefer {
            for (layers.items) |pl| self.allocator.free(pl.nodes);
            layers.deinit(self.allocator);
        }

        // Use dummy RNG for bottom trees (matching Rust implementation)
        // Rust uses StdRng::seed_from_u64(0) for bottom trees because they're full and padding is removed
        // This allows parallel building without affecting RNG determinism
        var dummy_rng = std.Random.DefaultPrng.init(0);
        const dummy_rng_random = dummy_rng.random();

        // Pass leaf_nodes_in directly to padLayer
        // padLayer allocates its own array and copies the input, so we don't need an intermediate copy
        var current_layer = try self.padLayerWithRng(leaf_nodes_in, start_index, &dummy_rng_random);
        try layers.append(self.allocator, .{ .nodes = try self.allocator.alloc([8]FieldElement, current_layer.nodes.len), .start_index = current_layer.start_index });
        @memcpy(layers.items[layers.items.len - 1].nodes, current_layer.nodes);

        // Debug: log leaf nodes for epoch 16 (bottom tree 1)
        // For bottom tree 1, epoch 16 is at index 0 (epoch - epoch_range_start = 16 - 16 = 0)
        if (bottom_tree_index == 1 and current_layer.nodes.len >= 2) {
            const epoch_16_index = 0; // epoch 16 is the first epoch in bottom tree 1
            if (current_layer.nodes.len > epoch_16_index) {
                log.print("ZIG_BUILDTREE: Bottom tree {} level 0 layer: start_index={}, epoch_16_index={}, nodes[{}][0]=0x{x:0>8}, nodes[{}][0]=0x{x:0>8}\n", .{ bottom_tree_index, current_layer.start_index, epoch_16_index, epoch_16_index, current_layer.nodes[epoch_16_index][0].value, epoch_16_index + 1, current_layer.nodes[epoch_16_index + 1][0].value });
            }
        }

        var current_level: usize = 0;
        while (current_level < full_depth) : (current_level += 1) {
            // This matches Rust's behavior: prev = &layers[level - lowest_layer]
            // At the start of each iteration, current_layer is the previous layer
            const prev_layer_start_index = current_layer.start_index;
            const parent_start = prev_layer_start_index >> 1;
            const parents_len = current_layer.nodes.len / 2;
            const parents = try self.allocator.alloc([8]FieldElement, parents_len);
            try self.processPairsInParallel(current_layer.nodes, parents, parent_start, current_level, parameter);
            self.allocator.free(current_layer.nodes);
            const new_layer = try self.padLayer(parents, parent_start);
            self.allocator.free(parents);
            current_layer = new_layer;
            try layers.append(self.allocator, .{ .nodes = try self.allocator.alloc([8]FieldElement, current_layer.nodes.len), .start_index = current_layer.start_index });
            @memcpy(layers.items[layers.items.len - 1].nodes, current_layer.nodes);
        }

        self.allocator.free(current_layer.nodes);

        return layers.toOwnedSlice(self.allocator);
    }

    fn buildTopTreeLayers(
        self: *GeneralizedXMSSSignatureScheme,
        roots_of_bottom_trees: [][8]FieldElement,
        parameter: [5]FieldElement,
        start_bottom_tree_index: usize,
    ) ![]PaddedLayer {
        // start_bottom_tree_index is used as start_index for top tree layers
        const log_lifetime = self.lifetime_params.log_lifetime;
        const lowest_layer = log_lifetime / 2;
        const depth = log_lifetime;
        // Use the actual start_bottom_tree_index for top tree layers
        // Rust's combined_path uses epoch directly, and the top tree layers must use
        // the actual bottom tree indices (start_bottom_tree_index) so that path computation
        // can use bottom_tree_index directly (absolute position)
        const start_index = start_bottom_tree_index;

        var layers: std.ArrayList(PaddedLayer) = .{};
        errdefer {
            for (layers.items) |pl| self.allocator.free(pl.nodes);
            layers.deinit(self.allocator);
        }

        const lowest_layer_nodes = try self.allocator.alloc([8]FieldElement, roots_of_bottom_trees.len);
        defer self.allocator.free(lowest_layer_nodes);
        @memcpy(lowest_layer_nodes, roots_of_bottom_trees);

        var current_layer = try self.padLayer(lowest_layer_nodes, start_index);
        try layers.append(self.allocator, .{ .nodes = try self.allocator.alloc([8]FieldElement, current_layer.nodes.len), .start_index = current_layer.start_index });
        @memcpy(layers.items[layers.items.len - 1].nodes, current_layer.nodes);

        var current_level: usize = lowest_layer;
        while (current_level < depth) : (current_level += 1) {
            const parent_start = current_layer.start_index >> 1;
            const parents_len = current_layer.nodes.len / 2;
            const parents = try self.allocator.alloc([8]FieldElement, parents_len);

            if (current_level == lowest_layer and parents_len > 0) {
                log.debugPrint("ZIG_KEYGEN_DEBUG: First top tree level (level={}): hashing nodes[0] and nodes[1] with parent_start={}, parent_pos={}\n", .{ current_level, parent_start, parent_start + 0 });
                log.debugPrint("ZIG_KEYGEN_DEBUG:   nodes[0][0]=0x{x:0>8} (root of bottom tree {})\n", .{ current_layer.nodes[0][0].value, start_index });
                if (current_layer.nodes.len > 1) {
                    log.debugPrint("ZIG_KEYGEN_DEBUG:   nodes[1][0]=0x{x:0>8} (root of bottom tree {})\n", .{ current_layer.nodes[1][0].value, start_index + 1 });
                }
            }

            try self.processPairsInParallel(current_layer.nodes, parents, parent_start, current_level, parameter);
            self.allocator.free(current_layer.nodes);
            // Use padLayer (with mutex) for top tree to ensure deterministic RNG consumption
            const new_layer = try self.padLayer(parents, parent_start);
            self.allocator.free(parents);
            current_layer = new_layer;
            try layers.append(self.allocator, .{ .nodes = try self.allocator.alloc([8]FieldElement, current_layer.nodes.len), .start_index = current_layer.start_index });
            @memcpy(layers.items[layers.items.len - 1].nodes, current_layer.nodes);
        }

        self.allocator.free(current_layer.nodes);

        return layers.toOwnedSlice(self.allocator);
    }

    /// Encode message as field elements (matching Rust encode_message)
    /// Uses base-p decomposition: interprets message as little-endian big integer
    /// Uses multi-precision arithmetic to handle 32-byte (256-bit) message
    pub fn encodeMessage(self: *GeneralizedXMSSSignatureScheme, MSG_LEN_FE: usize, message: [MESSAGE_LENGTH]u8) ![]FieldElement {
        const p: u256 = 2130706433; // KoalaBear field modulus
        var result = try self.allocator.alloc(FieldElement, MSG_LEN_FE);
        errdefer self.allocator.free(result);

        // Load little-endian 32-byte message into u256
        var acc: u256 = 0;
        for (message, 0..) |b, i| {
            acc +%= (@as(u256, b) << @intCast(8 * i));
        }

        // Repeated division by p to extract base-p digits (little-endian digits)
        var i: usize = 0;
        while (i < MSG_LEN_FE) : (i += 1) {
            const digit: u256 = acc % p;
            result[i] = FieldElement.fromCanonical(@intCast(digit));
            acc = acc / p;
        }

        return result;
    }

    /// Encode epoch as field elements (matching Rust encode_epoch)
    pub fn encodeEpoch(self: *GeneralizedXMSSSignatureScheme, TWEAK_LEN_FE: usize, epoch: u32) ![]FieldElement {
        const p: u64 = 2130706433; // KoalaBear field modulus
        const TWEAK_SEPARATOR_FOR_MESSAGE_HASH: u8 = 0x02; // From Rust
        var result = try self.allocator.alloc(FieldElement, TWEAK_LEN_FE);
        errdefer self.allocator.free(result);

        // Combine epoch and separator: ((epoch as u64) << 8) | separator
        const acc = (@as(u64, epoch) << 8) | @as(u64, TWEAK_SEPARATOR_FOR_MESSAGE_HASH);

        // Two-step base-p decomposition (optimization for 40-bit value)
        if (TWEAK_LEN_FE > 0) {
            result[0] = FieldElement.fromCanonical(@intCast(acc % p));
        }
        if (TWEAK_LEN_FE > 1) {
            result[1] = FieldElement.fromCanonical(@intCast(acc / p));
        }
        // Any remaining elements remain zero
        for (2..TWEAK_LEN_FE) |i| {
            result[i] = FieldElement.zero();
        }

        return result;
    }

    /// Decode field elements to chunks (matching Rust decode_to_chunks)
    /// Builds big integer: acc = 0; for fe in field_elements: acc = acc * p + fe
    /// Then extracts DIMENSION digits: for i in 0..DIMENSION: chunk = acc % BASE; acc /= BASE
    fn decodeToChunks(_: *GeneralizedXMSSSignatureScheme, comptime DIMENSION: usize, comptime BASE: usize, comptime HASH_LEN_FE: usize, field_elements: [HASH_LEN_FE]FieldElement) [DIMENSION]u8 {
        const p: u64 = 2130706433; // KoalaBear field modulus
        var result: [DIMENSION]u8 = undefined;

        // Use base 2^32 representation (6 words = 192 bits, enough for p^5 ≈ 2^155)
        var bigint_u32: [6]u32 = [6]u32{ 0, 0, 0, 0, 0, 0 };

        // Build big integer: start with 0, for each fe: bigint = bigint * p + fe.value
        // This matches Rust: acc = 0; for fe in field_elements: acc = acc * p + fe
        for (field_elements) |fe| {
            // Multiply by p: bigint_u32 = bigint_u32 * p
            var mul_carry: u64 = 0;
            for (0..bigint_u32.len) |j| {
                const prod = @as(u64, bigint_u32[j]) * @as(u64, p) + mul_carry;
                bigint_u32[j] = @as(u32, @truncate(prod));
                mul_carry = prod >> 32;
            }

            // Add fe.value to the LSB (index 0)
            var add_carry: u64 = @as(u64, fe.value);
            var add_idx: usize = 0;
            while (add_carry > 0 and add_idx < bigint_u32.len) {
                const sum = @as(u64, bigint_u32[add_idx]) + add_carry;
                bigint_u32[add_idx] = @as(u32, @truncate(sum));
                add_carry = sum >> 32;
                add_idx += 1;
            }
        }

        // Debug: check bigint value and field elements
        if (HASH_LEN_FE == 5 and DIMENSION == 64 and BASE == 8) {
            var has_nonzero = false;
            for (bigint_u32) |word| {
                if (word != 0) {
                    has_nonzero = true;
                    break;
                }
            }
            if (has_nonzero) {
                log.print("ZIG_DECODE_DEBUG: bigint_u32[0..3]={any} field_elements={any}\n", .{
                    bigint_u32[0..3],
                    field_elements,
                });
            }
        }

        // Precompute 2^32 % BASE once (constant for given BASE)
        var two_pow_32_mod: u64 = 1;
        var bit: usize = 0;
        while (bit < 32) : (bit += 1) {
            two_pow_32_mod = (two_pow_32_mod * 2) % BASE;
        }

        // Extract DIMENSION digits in base-BASE by repeatedly:
        //   1. Compute bigint_u32 % BASE
        //   2. Divide bigint_u32 by BASE
        for (0..DIMENSION) |i| {
            // Extract digit: compute bigint_u32 % BASE
            // Use Horner's method: process from MSB to LSB
            // For value = bigint_u32[5]*2^160 + ... + bigint_u32[1]*2^32 + bigint_u32[0]
            // We compute: ((...((bigint_u32[5]*2^32 + bigint_u32[4])*2^32 + bigint_u32[3])...)*2^32 + bigint_u32[0]) % BASE
            var mod_remainder: u64 = 0;
            // Process from MSB (highest index) to LSB (index 0)
            for (0..bigint_u32.len) |j| {
                const idx = bigint_u32.len - 1 - j; // MSB to LSB
                // Horner's method: remainder = (remainder * base + digit) % BASE
                mod_remainder = ((mod_remainder * two_pow_32_mod) % BASE + (bigint_u32[idx] % BASE)) % BASE;
            }
            result[i] = @as(u8, @intCast(mod_remainder));

            // Divide bigint_u32 by BASE (for next iteration)
            // Process from MSB to LSB, carrying remainder
            var div_carry: u64 = 0;
            for (0..bigint_u32.len) |j| {
                const idx = bigint_u32.len - 1 - j; // MSB to LSB
                const val = (@as(u128, div_carry) << 32) | @as(u128, bigint_u32[idx]);
                bigint_u32[idx] = @as(u32, @truncate(val / BASE));
                div_carry = @as(u64, @truncate(val % BASE));
            }
        }

        return result;
    }

    fn getLayerData(self: *GeneralizedXMSSSignatureScheme, w: usize) !*const poseidon_top_level.AllLayerInfoForBase {
        return poseidon_top_level.getLayerData(self, w);
    }

    fn hypercubeFindLayerBig(
        self: *GeneralizedXMSSSignatureScheme,
        BASE: usize,
        DIMENSION: usize,
        final_layer: usize,
        value: *const BigInt,
        offset_out: *BigInt,
    ) !usize {
        return poseidon_top_level.hypercubeFindLayerBig(self, BASE, DIMENSION, final_layer, value, offset_out);
    }

    fn mapToVertexBig(
        self: *GeneralizedXMSSSignatureScheme,
        BASE: usize,
        DIMENSION: usize,
        layer: usize,
        offset: *const BigInt,
    ) ![]u8 {
        return poseidon_top_level.mapToVertexBig(self, BASE, DIMENSION, layer, offset);
    }

    fn mapIntoHypercubePart(
        self: *GeneralizedXMSSSignatureScheme,
        DIMENSION: usize,
        BASE: usize,
        final_layer: usize,
        field_elements: []const FieldElement,
    ) ![]u8 {
        return poseidon_top_level.mapIntoHypercubePart(self, DIMENSION, BASE, final_layer, field_elements);
    }

    pub fn applyTopLevelPoseidonMessageHash(
        self: *GeneralizedXMSSSignatureScheme,
        parameter: [5]FieldElement,
        epoch: u32,
        randomness: []const FieldElement,
        message: [MESSAGE_LENGTH]u8,
    ) ![]u8 {
        return poseidon_top_level.applyTopLevelPoseidonMessageHash(self, parameter, epoch, randomness, message);
    }

    fn deriveTargetSumEncoding(
        self: *GeneralizedXMSSSignatureScheme,
        parameter: [5]FieldElement,
        epoch: u32,
        randomness: []const FieldElement,
        message: [MESSAGE_LENGTH]u8,
    ) ![]u8 {
        return target_sum_encoding.deriveTargetSumEncoding(self, parameter, epoch, randomness, message);
    }

    /// Pad a layer to ensure it starts at an even index and ends at an odd index
    /// This matches the Rust HashTreeLayer::padded algorithm exactly
    /// Thread-safe: uses mutex to protect RNG access
    fn padLayer(self: *GeneralizedXMSSSignatureScheme, nodes: [][8]FieldElement, start_index: usize) !PaddedLayer {
        self.rng_mutex.lock();
        defer self.rng_mutex.unlock();
        return self.padLayerWithRng(nodes, start_index, &self.rng.random());
    }

    /// Pad a layer with a specific RNG (for bottom trees with dummy RNG)
    fn padLayerWithRng(self: *GeneralizedXMSSSignatureScheme, nodes: [][8]FieldElement, start_index: usize, rng: *const std.Random) !PaddedLayer {
        // End index of the provided contiguous run (inclusive)
        const end_index = start_index + nodes.len - 1;

        // Do we need a front pad? Start must be even
        const needs_front = (start_index & 1) == 1;

        // Do we need a back pad? End must be odd
        const needs_back = (end_index & 1) == 0;

        // The effective start index after optional front padding (always even)
        const actual_start_index = if (needs_front) start_index - 1 else start_index;

        // Reserve exactly the space we may need: original nodes plus up to two pads
        var total_capacity = nodes.len;
        if (needs_front) total_capacity += 1;
        if (needs_back) total_capacity += 1;
        var padded_nodes = try self.allocator.alloc([8]FieldElement, total_capacity);
        errdefer self.allocator.free(padded_nodes);

        var output_index: usize = 0;

        // Optional front padding to align to an even start index
        if (needs_front) {
            // Generate random node for front padding (matching Rust TH::rand_domain(rng))
            // Rust calls rng.random() once to generate a full domain element (8 field elements)
            log.print("DEBUG: padLayer: Generating front padding node (1 RNG call)\n", .{});
            const random_domain = try self.generateRandomDomainSingleWithRng(rng);
            @memcpy(padded_nodes[output_index][0..8], random_domain[0..8]);
            output_index += 1;
            log.print("DEBUG: padLayer: Added front padding node at index {}\n", .{output_index - 1});
        }

        // Insert the actual content in order
        @memcpy(padded_nodes[output_index .. output_index + nodes.len], nodes);
        output_index += nodes.len;

        // Optional back padding to ensure we end on an odd index
        if (needs_back) {
            // Generate random node for back padding (matching Rust rng.random() for arrays)
            log.print("DEBUG: padLayer: Generating back padding node (1 RNG call)\n", .{});
            const random_domain = try self.generateRandomDomainSingleWithRng(rng);
            @memcpy(padded_nodes[output_index][0..8], random_domain[0..8]);
            log.print("DEBUG: padLayer: Added back padding node at index {}\n", .{output_index});
        }

        log.print("DEBUG: padLayer: start_index={}, nodes.len={}, end_index={}\n", .{ start_index, nodes.len, end_index });
        log.print("DEBUG: padLayer: needs_front={}, needs_back={}, actual_start_index={}\n", .{ needs_front, needs_back, actual_start_index });
        log.print("DEBUG: padLayer: total_capacity={}, padded_nodes.len={}\n", .{ total_capacity, padded_nodes.len });

        return .{
            .nodes = padded_nodes,
            .start_index = actual_start_index,
        };
    }

    /// Get RNG state for debugging
    pub fn getRngState(self: *GeneralizedXMSSSignatureScheme) [5]u32 {
        // Create a copy of the RNG to avoid consuming the original state
        var rng_copy = self.rng;
        var result: [5]u32 = undefined;
        for (0..5) |i| {
            result[i] = rng_copy.random().int(u32);
        }
        return result;
    }

    /// Build top tree from bottom tree roots and return root as array of 8 field elements
    /// This matches the Rust HashSubTree::new_top_tree algorithm exactly
    pub fn buildTopTreeAsArray(
        self: *GeneralizedXMSSSignatureScheme,
        roots_of_bottom_trees: [][8]FieldElement,
        parameter: [5]FieldElement,
        start_bottom_tree_index: usize,
    ) ![8]FieldElement {
        log.print("DEBUG: Building tree from lowest layer {} up to depth {}\n", .{
            self.lifetime_params.log_lifetime / 2,
            self.lifetime_params.log_lifetime,
        });
        log.print("DEBUG: Starting with {} bottom tree roots (start index: {})\n", .{ roots_of_bottom_trees.len, start_bottom_tree_index });

        const layers = try self.buildTopTreeLayers(roots_of_bottom_trees, parameter, start_bottom_tree_index);
        defer {
            for (layers) |pl| self.allocator.free(pl.nodes);
            self.allocator.free(layers);
        }

        if (layers.len == 0 or layers[layers.len - 1].nodes.len == 0) {
            return error.InvalidTopTree;
        }

        const root_array = layers[layers.len - 1].nodes[0];
        if (build_opts.enable_debug_logs) {
            log.print("DEBUG: Final top tree root array: {any}\n", .{root_array});
        }
        return root_array;
    }

    /// Build bottom tree from leaf hashes and return as array of 8 field elements
    fn buildBottomTreeAsArray(self: *GeneralizedXMSSSignatureScheme, leaf_hashes: []FieldElement, parameter: [5]FieldElement) ![8]FieldElement {
        // Debug: Print input information
        if (build_opts.enable_debug_logs) {
            log.print("DEBUG: buildBottomTreeAsArray called with {} leaf hashes\n", .{leaf_hashes.len});
        }

        // Instead of building to a single root, build to exactly 8 field elements
        // This matches the Rust implementation which produces 8 different values

        // Start with the leaf hashes
        var current_level = try self.allocator.alloc(FieldElement, leaf_hashes.len);
        @memcpy(current_level, leaf_hashes);

        var level_size = leaf_hashes.len;
        var level_count: usize = 0;

        // Build tree until we have exactly 8 elements or fewer
        while (level_size > 8) {
            const next_level_size = (level_size + 1) / 2;
            var next_level = try self.allocator.alloc(FieldElement, next_level_size);

            log.print("DEBUG: Level {}: {} -> {} elements\n", .{ level_count, level_size, next_level_size });

            for (0..next_level_size) |i| {
                if (i * 2 + 1 < level_size) {
                    // Hash two elements together
                    const left = current_level[i * 2];
                    const right = current_level[i * 2 + 1];
                    const pair = [_]FieldElement{ left, right };

                    // log.print("DEBUG: Hashing pair [{}] = 0x{x} + 0x{x}\n", .{ i, left.value, right.value });

                    const hash_result = try self.applyPoseidonTweakHash(&pair, 0, 0, parameter);
                    defer self.allocator.free(hash_result);
                    next_level[i] = hash_result[0];

                    // log.print("DEBUG: Result [{}] = 0x{x}\n", .{ i, next_level[i].value });
                } else {
                    // Odd number of elements, copy the last one
                    next_level[i] = current_level[i * 2];
                    // log.print("DEBUG: Copying [{}] = 0x{x}\n", .{ i, next_level[i].value });
                }
            }

            self.allocator.free(current_level);
            current_level = next_level;
            level_size = next_level_size;
            level_count += 1;
        }

        log.print("DEBUG: Final level {} has {} elements\n", .{ level_count, level_size });

        // Convert to array of 8 field elements
        var result: [8]FieldElement = undefined;

        // Copy existing elements
        for (0..@min(8, level_size)) |i| {
            result[i] = current_level[i];
            // log.print("DEBUG: result[{}] = 0x{x}\n", .{ i, result[i].value });
        }

        // Fill remaining with zeros if we have fewer than 8 elements
        for (level_size..8) |i| {
            result[i] = FieldElement{ .value = 0 };
            // log.print("DEBUG: result[{}] = 0x{x} (zero)\n", .{ i, result[i].value });
        }

        self.allocator.free(current_level);
        return result;
    }

    /// Generate random PRF key (matching Rust PRF::key_gen)
    pub fn generateRandomPRFKey(self: *GeneralizedXMSSSignatureScheme) ![32]u8 {
        return rng_flow.generateRandomPRFKey(self);
    }

    /// Generate random parameter (matching Rust TH::rand_parameter)
    pub fn generateRandomParameter(self: *GeneralizedXMSSSignatureScheme) ![5]FieldElement {
        return rng_flow.generateRandomParameter(self);
    }

    /// Generate random domain elements for padding (matching Rust TH::rand_domain)
    pub fn generateRandomDomain(self: *GeneralizedXMSSSignatureScheme, count: usize) ![8]FieldElement {
        return rng_flow.generateRandomDomain(self, count);
    }

    /// Generate a single random domain element (matching Rust TH::rand_domain exactly)
    pub fn generateRandomDomainSingle(self: *GeneralizedXMSSSignatureScheme) ![8]FieldElement {
        return rng_flow.generateRandomDomainSingle(self);
    }

    /// Generate a single random domain element using a specific RNG
    fn generateRandomDomainSingleWithRng(self: *GeneralizedXMSSSignatureScheme, rng: *const std.Random) ![8]FieldElement {
        return rng_flow.generateRandomDomainSingleWithRng(self, rng);
    }

    /// Key generation return type
    pub const KeyGenResult = struct {
        public_key: GeneralizedXMSSPublicKey,
        secret_key: *GeneralizedXMSSSecretKey,
    };

    /// Key generation (matching Rust key_gen exactly)
    pub fn keyGen(
        self: *GeneralizedXMSSSignatureScheme,
        activation_epoch: usize,
        num_active_epochs: usize,
    ) !KeyGenResult {
        // Rust leansig library multiplies num_active_epochs by 128 internally
        // To match Rust's behavior exactly, we multiply by 128 here
        // Example: Input 1024 -> Rust stores 131072 (1024 * 128) in SSZ
        const rust_compatible_num_active_epochs = num_active_epochs * 128;

        // Generate random parameter and PRF key (matching Rust order exactly)
        const parameter = try self.generateRandomParameter();
        const prf_key = try self.generateRandomPRFKey();
        // RNG has already been consumed by generateRandomPRFKey() (32 bytes)
        return self.keyGenWithParameter(activation_epoch, rust_compatible_num_active_epochs, parameter, prf_key, true);
    }

    /// Key generation with provided parameter and PRF key (for reconstructing keys from serialized data)
    /// rng_already_consumed: if true, RNG state is already after PRF key generation (32 bytes consumed)
    ///                       if false, RNG state is fresh and we need to consume 32 bytes to match
    ///                       Also used to determine if trees should be built sequentially (for deterministic RNG)
    pub fn keyGenWithParameter(
        self: *GeneralizedXMSSSignatureScheme,
        activation_epoch: usize,
        num_active_epochs: usize,
        parameter: [5]FieldElement,
        prf_key: [32]u8,
        rng_already_consumed: bool,
    ) !KeyGenResult {
        const profile_keygen = @hasDecl(build_opts, "enable_profile_keygen") and build_opts.enable_profile_keygen;
        var total_timer: std.time.Timer = undefined;
        var bottom_time_ns: u64 = 0;
        var top_time_ns: u64 = 0;
        var bottom_start_ns: u64 = 0;
        var top_start_ns: u64 = 0;
        if (profile_keygen) {
            total_timer = try std.time.Timer.start();
            bottom_start_ns = 0;
            top_start_ns = 0;
        }

        const lifetime = @as(usize, 1) << @intCast(self.lifetime_params.log_lifetime);

        // Validate activation parameters
        if (activation_epoch + num_active_epochs > lifetime) {
            return error.InvalidActivationParameters;
        }

        // Expand activation time to align with bottom trees
        const expansion_result = expandActivationTime(self.lifetime_params.log_lifetime, activation_epoch, num_active_epochs);
        const num_bottom_trees = expansion_result.end - expansion_result.start;

        if (num_bottom_trees < 2) {
            return error.InsufficientBottomTrees;
        }

        // Use the provided activation parameters directly (not expanded)
        const expanded_activation_epoch = activation_epoch;
        const expanded_num_active_epochs = num_active_epochs;

        // Consume RNG state only if it hasn't been consumed yet
        // When called from keyGen(), the RNG state is already after PRF key generation (32 bytes consumed).
        // When called directly, we need to consume 32 bytes to match that state.
        if (!rng_already_consumed) {
            var dummy_prf_key: [32]u8 = undefined;
            self.rng.fill(&dummy_prf_key);
        }

        // NOTE: RNG consumption for padding happens naturally during tree building
        // in buildTopTreeLayers() via padLayer() and padLayerWithRng() calls.
        // We don't need to pre-consume RNG state for padding here - it will be consumed at the
        // correct time during the actual tree construction.

        // Generate bottom trees and collect their roots as arrays of 8 field elements
        // Note: Worker threads mutate this array, so we need a mutable reference
        const roots_of_bottom_trees = try self.allocator.alloc([8]FieldElement, num_bottom_trees);
        defer self.allocator.free(roots_of_bottom_trees);

        if (profile_keygen) {
            // Bottom tree generation starts here
            bottom_start_ns = total_timer.read();
        }

        log.print("DEBUG: Generating {} bottom trees in parallel\n", .{num_bottom_trees});
        log.print("DEBUG: PRF key: {x}\n", .{prf_key});
        // log.print("DEBUG: Parameter: {any}\n", .{parameter});

        log.print("DEBUG: Expansion result: start={}, end={}\n", .{ expansion_result.start, expansion_result.end });

        // Parallel tree generation context
        const TreeGenContext = struct {
            scheme: *GeneralizedXMSSSignatureScheme,
            prf_key: [32]u8,
            parameter: [5]FieldElement,
            roots: [][8]FieldElement,
            tree_indices: []usize,
            trees: []?*HashSubTree,
            next_index: std.atomic.Value(usize),
            error_flag: std.atomic.Value(bool),
            error_mutex: std.Thread.Mutex,
            stored_error: ?anyerror,
        };

        // Allocate storage for trees (we'll keep the first two, deinit the rest)
        var trees = try self.allocator.alloc(?*HashSubTree, num_bottom_trees);
        defer self.allocator.free(trees);
        for (trees) |*t| t.* = null;

        // Collect all tree indices
        var tree_indices = try self.allocator.alloc(usize, num_bottom_trees);
        defer self.allocator.free(tree_indices);
        for (0..num_bottom_trees) |i| {
            tree_indices[i] = expansion_result.start + i;
        }

        var ctx = TreeGenContext{
            .scheme = self,
            .prf_key = prf_key,
            .parameter = parameter,
            .roots = @constCast(roots_of_bottom_trees), // Worker threads mutate this
            .tree_indices = tree_indices,
            .trees = trees,
            .next_index = std.atomic.Value(usize).init(0),
            .error_flag = std.atomic.Value(bool).init(false),
            .error_mutex = .{},
            .stored_error = null,
        };

        // Worker function for parallel tree generation
        // Note: This worker only processes trees starting from index 2 (first 2 are built sequentially)
        const treeWorker = struct {
            fn worker(ctx_ptr: *TreeGenContext) void {
                while (true) {
                    const array_idx = ctx_ptr.next_index.fetchAdd(1, .monotonic);
                    // Skip first 2 trees (indices 0 and 1) - they're built sequentially
                    const actual_idx = array_idx + 2;
                    if (actual_idx >= ctx_ptr.tree_indices.len) {
                        break;
                    }

                    const tree_idx = ctx_ptr.tree_indices[actual_idx];

                    // Generate tree
                    const tree = ctx_ptr.scheme.bottomTreeFromPrfKey(
                        ctx_ptr.prf_key,
                        tree_idx,
                        ctx_ptr.parameter,
                    ) catch |err| {
                        // Store error
                        ctx_ptr.error_mutex.lock();
                        defer ctx_ptr.error_mutex.unlock();
                        if (!ctx_ptr.error_flag.load(.monotonic)) {
                            ctx_ptr.error_flag.store(true, .monotonic);
                            ctx_ptr.stored_error = err;
                        }
                        return;
                    };

                    // Store tree and root (use actual_idx since we skipped first 2)
                    ctx_ptr.trees[actual_idx] = tree;
                    ctx_ptr.roots[actual_idx] = tree.root();
                }
            }
        };

        // Match Rust's behavior: first 2 trees sequential, rest in parallel
        // This ensures the first two trees are available immediately for the secret key
        // while the rest are built in parallel for performance

        // Build first 2 trees sequentially (matching Rust)
        if (num_bottom_trees >= 1) {
            const tree_idx_0 = tree_indices[0];
            const tree_0 = try self.bottomTreeFromPrfKey(prf_key, tree_idx_0, parameter);
            trees[0] = tree_0;
            roots_of_bottom_trees[0] = tree_0.root();
        }

        if (num_bottom_trees >= 2) {
            const tree_idx_1 = tree_indices[1];
            const tree_1 = try self.bottomTreeFromPrfKey(prf_key, tree_idx_1, parameter);
            trees[1] = tree_1;
            roots_of_bottom_trees[1] = tree_1.root();
        }

        // Build remaining trees in parallel (matching Rust's into_par_iter approach)
        if (num_bottom_trees > 2) {
            const num_cpus = std.Thread.getCpuCount() catch 1;
            const num_remaining_trees = num_bottom_trees - 2;
            const num_threads = @min(num_cpus, num_remaining_trees);
            log.print("DEBUG: Using {} threads for parallel tree generation ({} remaining trees)\n", .{ num_threads, num_remaining_trees });

            // Reset the atomic counter to 0 for remaining trees (indices 2+)
            ctx.next_index.store(0, .monotonic);

            // OPTIMIZATION: Use stack allocation for small thread counts
            if (num_threads <= 16) {
                var threads_stack: [16]std.Thread = undefined;
                const threads = threads_stack[0..num_threads];

                for (0..num_threads) |t| {
                    threads[t] = try std.Thread.spawn(.{}, treeWorker.worker, .{&ctx});
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            } else {
                // Spawn worker threads
                var threads = try self.allocator.alloc(std.Thread, num_threads);
                defer self.allocator.free(threads);

                for (0..num_threads) |t| {
                    threads[t] = try std.Thread.spawn(.{}, treeWorker.worker, .{&ctx});
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            }

            // This ensures we don't proceed with incomplete data
            for (2..num_bottom_trees) |i| {
                if (trees[i] == null) {
                    log.print("ZIG_KEYGEN_ERROR: Tree at index {} was not built!\n", .{i});
                    return error.TreeNotBuilt;
                }
                // Verify root was stored
                var root_zero = true;
                for (roots_of_bottom_trees[i]) |fe| {
                    if (fe.value != 0) {
                        root_zero = false;
                        break;
                    }
                }
                if (root_zero) {
                    log.print("ZIG_KEYGEN_ERROR: Root at index {} is all zeros!\n", .{i});
                    return error.RootNotStored;
                }
            }
        } else if (num_bottom_trees > 2) {
            // TEMPORARY: Build remaining trees sequentially for debugging
            log.print("DEBUG: Building remaining {} trees sequentially (parallel disabled for debugging)\n", .{num_bottom_trees - 2});
            for (2..num_bottom_trees) |i| {
                const tree_idx = tree_indices[i];
                const tree = try self.bottomTreeFromPrfKey(prf_key, tree_idx, parameter);
                trees[i] = tree;
                roots_of_bottom_trees[i] = tree.root();
            }
        }

        // Check for errors
        if (ctx.error_flag.load(.monotonic)) {
            ctx.error_mutex.lock();
            defer ctx.error_mutex.unlock();
            if (ctx.stored_error) |err| {
                // Clean up any successfully generated trees
                for (trees) |tree| {
                    if (tree) |t| t.deinit();
                }
                return err;
            }
            return error.UnknownError;
        }

        // Log roots for first tree
        if (build_opts.enable_debug_logs and num_bottom_trees > 0) {
            log.print("ZIG_KEYGEN_DEBUG: Bottom tree {} root: ", .{tree_indices[0]});
            for (roots_of_bottom_trees[0]) |fe| {
                log.print("0x{x:0>8} ", .{fe.value});
            }
            log.print("\n", .{});
        }

        if (profile_keygen) {
            // Bottom trees done
            const now = total_timer.read();
            bottom_time_ns = now - bottom_start_ns;
            top_start_ns = now;
        }

        // Store first two trees (left and right) for secret key
        const left_bottom_tree_index = expansion_result.start;
        const left_bottom_tree = trees[0] orelse return error.InvalidBottomTree;
        const right_bottom_tree = if (num_bottom_trees > 1) trees[1] orelse return error.InvalidBottomTree else return error.InsufficientBottomTrees;

        // This ensures consistency - if they don't match, the signature's Merkle path won't verify correctly
        const left_tree_root = left_bottom_tree.root();
        const right_tree_root = right_bottom_tree.root();

        var left_match = true;
        for (0..8) |i| {
            if (!left_tree_root[i].eql(roots_of_bottom_trees[0][i])) {
                log.debugPrint("ZIG_KEYGEN_ERROR: Left bottom tree root[{}] mismatch: tree=0x{x:0>8} vs roots_array=0x{x:0>8}\n", .{ i, left_tree_root[i].value, roots_of_bottom_trees[0][i].value });
                left_match = false;
            }
        }
        if (!left_match) {
            log.debugPrint("ZIG_KEYGEN_ERROR: Left bottom tree root does not match roots_of_bottom_trees[0]! This will cause verification to fail.\n", .{});
            // This is a critical error - the trees don't match, so we can't proceed
            return error.BottomTreeRootMismatch;
        }

        if (num_bottom_trees > 1) {
            var right_match = true;
            for (0..8) |i| {
                if (!right_tree_root[i].eql(roots_of_bottom_trees[1][i])) {
                    log.debugPrint("ZIG_KEYGEN_ERROR: Right bottom tree root[{}] mismatch: tree=0x{x:0>8} vs roots_array=0x{x:0>8}\n", .{ i, right_tree_root[i].value, roots_of_bottom_trees[1][i].value });
                    right_match = false;
                }
            }
            if (!right_match) {
                log.debugPrint("ZIG_KEYGEN_ERROR: Right bottom tree root does not match roots_of_bottom_trees[1]! This will cause verification to fail.\n", .{});
                return error.BottomTreeRootMismatch;
            }
        }

        // Clean up remaining trees (we only need the first two)
        for (trees[2..]) |tree| {
            if (tree) |t| t.deinit();
        }

        // Debug: log all roots before building top tree
        if (build_opts.enable_debug_logs) {
            log.debugPrint("ZIG_KEYGEN_DEBUG: All {} roots before building top tree:\n", .{roots_of_bottom_trees.len});
            for (roots_of_bottom_trees, 0..) |root, i| {
                log.debugPrint("ZIG_KEYGEN_DEBUG:   Root {} (bottom tree index {}): ", .{ i, tree_indices[i] });
                for (root) |fe| {
                    log.debugPrint("0x{x:0>8} ", .{fe.value});
                }
                log.debugPrint("\n", .{});
            }
        }

        // Build top tree from bottom tree roots and get root as array
        // This matches Rust's HashSubTree::new_top_tree call which happens after parameter generation
        // Use buildTopTreeLayers to ensure consistency with signing/verification
        log.print("DEBUG: Building top tree from {} bottom tree roots\n", .{roots_of_bottom_trees.len});
        var top_layers = try self.buildTopTreeLayers(roots_of_bottom_trees, parameter, expansion_result.start);

        // Extract root from the final layer (should have exactly 1 node)
        if (top_layers.len == 0 or top_layers[top_layers.len - 1].nodes.len == 0) {
            for (top_layers) |pl| self.allocator.free(pl.nodes);
            self.allocator.free(top_layers);
            return error.InvalidTopTree;
        }
        const root_array = top_layers[top_layers.len - 1].nodes[0];

        // Debug: log the computed root (canonical) and Montgomery (only when debug logs enabled)
        if (build_opts.enable_debug_logs) {
            log.print("ZIG_KEYGEN_DEBUG: Computed root during keygen (canonical): ", .{});
            for (root_array) |fe| {
                log.print("0x{x:0>8} ", .{fe.value});
            }
            log.print("\n", .{});
        }

        // Roots are already represented in Montgomery form in our FieldElement type.
        const root_monty: [8]FieldElement = root_array;

        if (build_opts.enable_debug_logs) {
            // Debug: log the root in Montgomery form
            log.print("ZIG_KEYGEN_DEBUG: Root in Montgomery form: ", .{});
            for (root_monty) |fe| {
                log.print("0x{x:0>8} ", .{fe.value});
            }
            log.print("\n", .{});
        }

        // Create a top tree for the secret key, preserving the layered structure for future path computation
        // Top tree depth is log_lifetime (32 for 2^32), matching Rust's encoding
        const tree_depth = self.lifetime_params.log_lifetime;
        const top_tree = try HashSubTree.initWithLayers(self.allocator, root_array, top_layers, tree_depth);
        top_layers = top_layers[0..0];

        // Create public and secret keys (store root in Montgomery form to match Rust)
        const public_key = GeneralizedXMSSPublicKey.init(root_monty, parameter, self.lifetime_params.hash_len_fe);

        // Debug: log the public key root and parameter (only when debug logs enabled)
        if (build_opts.enable_debug_logs) {
            log.print("ZIG_KEYGEN_DEBUG: Public key root: ", .{});
            for (public_key.root) |fe| {
                log.print("0x{x:0>8} ", .{fe.value});
            }
            log.print("\n", .{});

            log.print("ZIG_KEYGEN_DEBUG: Parameter passed to secret_key.init (canonical): ", .{});
            for (0..5) |i| {
                log.print("0x{x:0>8} ", .{parameter[i].toCanonical()});
            }
            log.print("(Montgomery: ", .{});
            for (0..5) |i| {
                log.print("0x{x:0>8} ", .{parameter[i].toMontgomery()});
            }
            log.print(")\n", .{});
        }

        const secret_key = try GeneralizedXMSSSecretKey.init(
            self.allocator,
            prf_key,
            parameter,
            expanded_activation_epoch,
            expanded_num_active_epochs,
            top_tree,
            left_bottom_tree_index,
            left_bottom_tree,
            right_bottom_tree,
        );

        if (build_opts.enable_debug_logs) {
            const keygen_prf_domain = self.prfDomainElement(prf_key, 1, 0);
            const keygen_chain_domain = try self.computeHashChainDomain(keygen_prf_domain, 1, 0, parameter);
            log.print("ZIG_KEYGEN_VERIFY: Chain domain for epoch 1, chain 0 during keygen: chain_domain[0]=0x{x:0>8}\n", .{keygen_chain_domain[0].value});
            log.print("ZIG_KEYGEN_VERIFY: PRF key[0..8]=", .{});
            for (prf_key[0..8]) |b| log.print("{x:0>2}", .{b});
            log.print(", parameter[0]=0x{x:0>8} (canonical: 0x{x:0>8})\n", .{ parameter[0].value, parameter[0].toCanonical() });
        }

        log.print("ZIG_KEYGEN_DEBUG: Secret key parameter after init (canonical): ", .{});
        for (0..5) |i| {
            log.print("0x{x:0>8} ", .{secret_key.parameter[i].toCanonical()});
        }
        log.print("(Montgomery: ", .{});
        for (0..5) |i| {
            log.print("0x{x:0>8} ", .{secret_key.parameter[i].toMontgomery()});
        }
        log.print(")\n", .{});

        if (profile_keygen) {
            const now = total_timer.read();
            top_time_ns = now - top_start_ns;
            const total_ns = now;

            const total_sec: f64 = @as(f64, @floatFromInt(total_ns)) / 1_000_000_000.0;
            const bottom_sec: f64 = @as(f64, @floatFromInt(bottom_time_ns)) / 1_000_000_000.0;
            const top_sec: f64 = @as(f64, @floatFromInt(top_time_ns)) / 1_000_000_000.0;

            const bottom_pct: f64 = if (total_sec > 0) (bottom_sec / total_sec) * 100.0 else 0.0;
            const top_pct: f64 = if (total_sec > 0) (top_sec / total_sec) * 100.0 else 0.0;

            log.print(
                "PROFILE_KEYGEN: bottom={d:.3}s ({d:.1}%), top={d:.3}s ({d:.1}%), total={d:.3}s\n",
                .{ bottom_sec, bottom_pct, top_sec, top_pct, total_sec },
            );
        }

        return .{
            .public_key = public_key,
            .secret_key = secret_key,
        };
    }

    /// Signing function (matching Rust sign exactly)
    pub fn sign(
        self: *GeneralizedXMSSSignatureScheme,
        secret_key: *GeneralizedXMSSSecretKey,
        epoch: u32,
        message: [MESSAGE_LENGTH]u8,
    ) !*GeneralizedXMSSSignature {
        log.debugPrint("ZIG_SIGN_DEBUG: sign() called with epoch={}\n", .{epoch});
        // Check activation interval
        const activation_interval = secret_key.getActivationInterval();
        if (epoch < activation_interval.start or epoch >= activation_interval.end) {
            return error.KeyNotActive;
        }

        // Check prepared interval
        const prepared_interval = secret_key.getPreparedInterval(self.lifetime_params.log_lifetime);
        if (epoch < prepared_interval.start or epoch >= prepared_interval.end) {
            return error.EpochNotPrepared;
        }

        // Generate Merkle path via combined bottom+top tree layers
        // Reuse pre-computed bottom trees instead of rebuilding
        // Rust doesn't rebuild bottom trees during signing - it reuses the stored trees
        const leafs_per_bottom_tree = @as(usize, 1) << @intCast(self.lifetime_params.log_lifetime / 2);
        const bottom_tree_index = @as(usize, @intCast(epoch)) / leafs_per_bottom_tree;
        const left_bottom_tree_index = secret_key.getLeftBottomTreeIndex();

        // Determine which stored bottom tree contains this epoch and get its layers
        const bottom_layers_opt: ?[]const PaddedLayer = blk: {
            if (bottom_tree_index == left_bottom_tree_index) {
                break :blk secret_key.left_bottom_tree.getLayers();
            } else if (bottom_tree_index == left_bottom_tree_index + 1) {
                break :blk secret_key.right_bottom_tree.getLayers();
            } else {
                return error.EpochNotPrepared;
            }
        };

        const bottom_layers = bottom_layers_opt orelse return error.MissingBottomTreeLayers;

        // Debug: log number of bottom layers (reused from stored tree)
        log.print("ZIG_SIGN_DEBUG: Reusing {} bottom tree layers from stored tree (bottom_tree_index={})\n", .{ bottom_layers.len, bottom_tree_index });

        if (epoch == 0 and bottom_tree_index == 0) {
            const root_layer = bottom_layers[bottom_layers.len - 1];
            if (root_layer.nodes.len > 0) {
                const stored_root = root_layer.nodes[0];
                log.print("ZIG_SIGN_DEBUG: Bottom tree 0 root from stored layers[0]=0x{x:0>8}\n", .{stored_root[0].value});
            }
            // The first layer should contain the leaf domains
            if (bottom_layers.len > 0) {
                const leaf_layer = bottom_layers[0];
                if (leaf_layer.nodes.len > 1) {
                    const stored_epoch1_leaf = leaf_layer.nodes[1];
                    log.print("ZIG_SIGN_DEBUG: Epoch 1 leaf domain from stored tree layers[0].nodes[1][0]=0x{x:0>8}\n", .{stored_epoch1_leaf[0].value});
                }
            }
        }

        // Use the stored top tree layers from the secret key (generated during keyGen)
        const top_layers = secret_key.top_tree.getLayers() orelse return error.MissingTopTreeLayers;
        log.print("ZIG_SIGN_DEBUG: top_layers.len={} (log_lifetime={} lowest_layer={})\n", .{
            top_layers.len,
            self.lifetime_params.log_lifetime,
            self.lifetime_params.log_lifetime / 2,
        });

        // Bottom path at absolute epoch, top path uses bottom_tree_index directly
        // Rust's combined_path uses epoch directly, and the top tree layers are built
        // with start_index matching the actual bottom tree indices
        const bottom_copath = try self.computePathFromLayers(bottom_layers, epoch);
        defer self.allocator.free(bottom_copath);

        if (epoch == 0 and bottom_copath.len > 0 and bottom_tree_index == 0) {
            const first_path_node = bottom_copath[0];
            if (bottom_layers.len > 0) {
                const leaf_layer = bottom_layers[0];
                if (leaf_layer.nodes.len > 1) {
                    const stored_epoch1_leaf = leaf_layer.nodes[1];
                    log.debugPrint("ZIG_SIGN_DEBUG: First bottom path node[0] (sibling of epoch 0): 0x{x:0>8}\n", .{first_path_node[0].value});
                    log.debugPrint("ZIG_SIGN_DEBUG: Stored epoch 1 leaf domain[0]: 0x{x:0>8}\n", .{stored_epoch1_leaf[0].value});
                    if (!first_path_node[0].eql(stored_epoch1_leaf[0])) {
                        log.debugPrint("ZIG_SIGN_ERROR: First path node doesn't match stored epoch 1 leaf domain!\n", .{});
                    } else {
                        log.debugPrint("ZIG_SIGN_DEBUG: First path node matches stored epoch 1 leaf domain ✓\n", .{});
                    }
                }
            }
        }

        // For top tree, use bottom_tree_index directly (absolute position)
        // Rust's combined_path uses epoch directly, and the top tree layers are built
        // with start_index = left_bottom_tree_index from keyGen, so we use bottom_tree_index
        // directly, and computePathFromLayers handles the offset via layer.start_index subtraction
        // left_bottom_tree_index already declared above
        const top_pos = @as(u32, @intCast(bottom_tree_index));

        // This ensures the path computation uses the correct offset
        if (top_layers.len > 0 and top_layers[0].start_index != left_bottom_tree_index) {
            log.debugPrint("ZIG_SIGN_ERROR: Top tree layer start_index mismatch! top_layers[0].start_index={}, left_bottom_tree_index={}\n", .{ top_layers[0].start_index, left_bottom_tree_index });
            return error.TopTreeStartIndexMismatch;
        }

        // Debug: log top tree layer start_index values
        log.print("ZIG_SIGN_DEBUG: Computing top tree path: bottom_tree_index={}, left_bottom_tree_index={}, top_pos={}\n", .{ bottom_tree_index, left_bottom_tree_index, top_pos });
        for (top_layers, 0..) |layer, i| {
            log.print("ZIG_SIGN_DEBUG: Top layer {}: {} nodes, start_index={}\n", .{ i, layer.nodes.len, layer.start_index });
        }

        const top_copath = try self.computePathFromLayers(top_layers, top_pos);
        defer self.allocator.free(top_copath);

        if (epoch == 0) {
            log.debugPrint("ZIG_SIGN_DEBUG: Top tree path for epoch 0 (top_pos={}, left_bottom_tree_index={}):\n", .{ top_pos, left_bottom_tree_index });
            for (top_copath, 0..) |node, i| {
                log.debugPrint("ZIG_SIGN_DEBUG:   Top node {}: 0x{x:0>8}\n", .{ i, node[0].value });
            }
        }

        // Debug: log path nodes (only for epoch 16 to reduce noise)
        if (epoch == 16) {
            log.print("ZIG_SIGN: Epoch {} - Bottom co-path: {} nodes\n", .{ epoch, bottom_copath.len });
            for (bottom_copath, 0..) |node, i| {
                log.print("ZIG_SIGN:   Bottom node {}: 0x{x:0>8}\n", .{ i, node[0].value });
            }
            log.print("ZIG_SIGN: Epoch {} - Top co-path for pos {}: {} nodes\n", .{ epoch, top_pos, top_copath.len });
            for (top_copath, 0..) |node, i| {
                log.print("ZIG_SIGN:   Top node {}: 0x{x:0>8}\n", .{ i, node[0].value });
            }
        }

        if (epoch == 0) {
            log.debugPrint("ZIG_SIGN_DEBUG: Bottom tree path for epoch 0: {} nodes\n", .{bottom_copath.len});
            for (bottom_copath, 0..) |node, i| {
                log.debugPrint("ZIG_SIGN_DEBUG:   Bottom node {}: 0x{x:0>8}\n", .{ i, node[0].value });
            }
        }

        // Rust's SSZ signature path encoding: encodes FULL Merkle path for ALL lifetimes
        // - For 2^8: bottom (4) + top (4) = 8 nodes
        // - For 2^18: bottom (9) + top (9) = 18 nodes
        // - For 2^32: bottom (16) + top (16) = 32 nodes
        // Concatenate bottom and top co-paths for all lifetimes
        var nodes_concat = try self.allocator.alloc([8]FieldElement, bottom_copath.len + top_copath.len);
        defer self.allocator.free(nodes_concat); // Free after HashTreeOpening.init() copies it
        @memcpy(nodes_concat[0..bottom_copath.len], bottom_copath);
        @memcpy(nodes_concat[bottom_copath.len..], top_copath);
        const path = try HashTreeOpening.init(self.allocator, nodes_concat);
        errdefer path.deinit(); // Clean up if signature creation fails

        // Try encoding with different randomness attempts (matching Rust sign retry loop)
        const MAX_TRIES: usize = 100_000;
        var attempts: u64 = 0;
        var rho_slice_opt: ?[]FieldElement = null;
        defer if (rho_slice_opt) |buf| self.allocator.free(buf);
        var rho_fixed: [7]FieldElement = undefined; // Signature struct uses fixed [7]FieldElement, pad if needed
        var x: []u8 = undefined;
        var encoding_succeeded = false;

        while (attempts < MAX_TRIES) : (attempts += 1) {
            // Generate randomness for this attempt
            if (rho_slice_opt) |buf| {
                self.allocator.free(buf);
                rho_slice_opt = null;
            }
            rho_slice_opt = try self.generateRandomness(secret_key.prf_key, epoch, message, attempts);
            const rho_slice = rho_slice_opt.?;

            // Convert to fixed array for signature structure (pad to 7 elements if needed)
            // For lifetime 2^18, rand_len_fe is 6, but signature struct uses [7]FieldElement
            const rand_len = self.lifetime_params.rand_len_fe;
            for (0..rand_len) |i| {
                rho_fixed[i] = rho_slice[i];
            }
            // Pad remaining elements with zeros (for lifetime 2^18, rand_len_fe=6, so pad 1 element)
            for (rand_len..7) |i| {
                rho_fixed[i] = FieldElement{ .value = 0 };
            }

            // Debug: print rho_slice vs rho_fixed to verify they match
            log.print("ZIG_SIGN_DEBUG: rho_slice (used for encoding, Montgomery): ", .{});
            for (0..rand_len) |i| {
                log.print("0x{x:0>8} ", .{rho_slice[i].toMontgomery()});
            }
            log.print("\n", .{});
            log.print("ZIG_SIGN_DEBUG: rho_fixed (stored in signature, Montgomery): ", .{});
            for (0..rand_len) |i| {
                log.print("0x{x:0>8} ", .{rho_fixed[i].toMontgomery()});
            }
            log.print("\n", .{});

            // Debug: print parameter used for encoding (for Zig→Zig debugging, only for successful attempt)
            log.print("ZIG_SIGN_DEBUG: secret_key.parameter BEFORE encoding (canonical): ", .{});
            for (0..5) |i| {
                log.print("0x{x:0>8} ", .{secret_key.parameter[i].toCanonical()});
            }
            log.print("(Montgomery: ", .{});
            for (0..5) |i| {
                log.print("0x{x:0>8} ", .{secret_key.parameter[i].toMontgomery()});
            }
            log.print(")\n", .{});

            // Try to encode with this randomness
            const encoding_result = self.deriveTargetSumEncoding(secret_key.parameter, epoch, rho_slice, message);
            if (encoding_result) |x_val| {
                // Debug: print parameter after encoding succeeds (for comparison with verification)
                log.print("ZIG_SIGN_DEBUG: parameter used for encoding (canonical): ", .{});
                for (0..5) |i| {
                    log.print("0x{x:0>8} ", .{secret_key.parameter[i].toCanonical()});
                }
                log.print("(Montgomery: ", .{});
                for (0..5) |i| {
                    log.print("0x{x:0>8} ", .{secret_key.parameter[i].toMontgomery()});
                }
                log.print(")\n", .{});
                x = x_val;
                encoding_succeeded = true;
                break;
            } else |err| {
                // If EncodingSumMismatch, try next attempt
                if (err != error.EncodingSumMismatch) {
                    return err; // Other errors should propagate
                }
                // Continue to next attempt
                // Debug: log progress periodically
                if (attempts < 3 or (attempts % 1000 == 0)) {
                    const chunks = try self.applyTopLevelPoseidonMessageHash(secret_key.parameter, epoch, rho_slice, message);
                    defer self.allocator.free(chunks);
                    var sum: usize = 0;
                    for (chunks) |chunk| sum += chunk;
                    const expected_sum = self.lifetime_params.target_sum;
                    log.print("ZIG_ENCODING_DEBUG: attempt {} sum={} expected={}\n", .{ attempts, sum, expected_sum });
                }
            }
        }

        if (!encoding_succeeded) {
            log.print("ZIG_ENCODING_DEBUG: Failed after {} attempts\n", .{MAX_TRIES});
            return error.EncodingAttemptsExceeded;
        }

        // Generate hashes for chains using PRF-derived starts and message-derived steps x
        // Match Rust: Rust stores hashes internally in Montgomery form (TH::Domain = [KoalaBear; HASH_LEN])
        // KoalaBear uses Montgomery internally, so Rust's chain() returns Montgomery values
        // We need to store hashes in Montgomery form to match Rust's internal representation
        const hashes = try self.allocator.alloc([8]FieldElement, self.lifetime_params.dimension);
        const hash_len = self.lifetime_params.hash_len_fe;
        const dimension = self.lifetime_params.dimension;

        // OPTIMIZATION: Parallelize chain computation since chains are independent
        // Use parallel processing for large workloads (64+ chains)
        const num_cpus = std.Thread.getCpuCount() catch 1;
        const min_parallel_chains = 64; // Threshold for parallel processing

        if (dimension < min_parallel_chains or num_cpus <= 1) {
            // Sequential processing for small workloads
            for (0..dimension) |chain_index| {
                // PRF start state (domain_elements are in Montgomery form from ShakePRFtoF)
                const domain_elements = self.prfDomainElement(secret_key.prf_key, epoch, @as(u64, @intCast(chain_index)));
                var current: [8]FieldElement = undefined;
                // domain_elements are already in Montgomery form, store directly for hash_len elements
                for (0..hash_len) |j| {
                    current[j] = FieldElement{ .value = domain_elements[j] };
                }
                // OPTIMIZATION: Use @memset instead of loop for zero-padding
                @memset(current[hash_len..8], FieldElement{ .value = 0 });

                // Walk chain for x[chain_index] steps
                const steps: u8 = x[chain_index];
                if (chain_index == 0) {
                    log.print("ZIG_SIGN_DEBUG: Chain {} starting from PRF (position 0), x[{}]={}, steps={}, initial[0]=0x{x:0>8}\n", .{ chain_index, chain_index, steps, steps, current[0].value });
                }
                if (steps > 0) {
                    var s: u8 = 1;
                    while (s <= steps) : (s += 1) {
                        if (chain_index == 0) {
                            // Debug: print every step for chain 0 during signing
                            log.debugPrint("ZIG_SIGN_DEBUG: Chain {} step {}: pos_in_chain={}, current[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical)\n", .{ chain_index, s - 1, s, current[0].value, current[0].toCanonical() });
                        }
                        const next = try self.applyPoseidonChainTweakHash(current, epoch, @as(u8, @intCast(chain_index)), s, secret_key.parameter);
                        // Batch copy using memcpy for better performance
                        @memcpy(current[0..hash_len], next[0..hash_len]);
                        @memset(current[hash_len..8], FieldElement{ .value = 0 });
                        if (chain_index == 0) {
                            log.debugPrint("ZIG_SIGN_DEBUG: Chain {} step {} result: next[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical)\n", .{ chain_index, s - 1, next[0].value, next[0].toCanonical() });
                        }
                    }
                }
                // Store hashes in Montgomery form (matching Rust's internal representation)
                hashes[chain_index] = current;
                // Debug: log first hash when storing during signing
                if (chain_index == 0) {
                    log.print("ZIG_SIGN_DEBUG: Storing hash[0] during signing (Montgomery): 0x{x:0>8}, (Canonical): 0x{x:0>8}\n", .{ current[0].value, current[0].toCanonical() });
                    // Also log full hash for comparison
                    log.print("ZIG_SIGN_DEBUG: Storing hash[0] full (Montgomery): ", .{});
                    for (0..hash_len) |h| {
                        log.print("0x{x:0>8} ", .{current[h].value});
                    }
                    log.print("\n", .{});
                }

                // Debug logging for chain 0
                if (chain_index == 0) {
                    // Debug: print full stored hash with detailed info
                    log.debugPrint("ZIG_SIGN_DEBUG: Chain {} final stored (Montgomery, x[{}]={}, steps={}): ", .{ chain_index, chain_index, steps, steps });
                    for (0..hash_len) |h| {
                        log.debugPrint("0x{x:0>8} ", .{current[h].value});
                    }
                    log.debugPrint("\n", .{});
                    log.debugPrint("ZIG_SIGN_DEBUG: Chain {} final stored (Canonical): ", .{chain_index});
                    for (0..hash_len) |h| {
                        log.debugPrint("0x{x:0>8} ", .{current[h].toCanonical()});
                    }
                    log.debugPrint("\n", .{});
                    // Also print what position in chain this represents
                    log.debugPrint("ZIG_SIGN_DEBUG: Chain {} stored at position {} in chain (walked {} steps from position 0)\n", .{ chain_index, steps, steps });

                    // gives the same result as computeHashChainDomain would produce
                    log.print("ZIG_SIGN_VERIFY_START: chain_index={}, epoch={}, steps={}\n", .{ chain_index, epoch, steps });
                    if (epoch == 0) {
                        const base_minus_one = self.lifetime_params.base - 1;
                        const remaining_steps = base_minus_one - steps;
                        log.debugPrint("ZIG_SIGN_VERIFY: base_minus_one={}, remaining_steps={}\n", .{ base_minus_one, remaining_steps });
                        if (remaining_steps > 0) {
                            var verify_current: [8]FieldElement = undefined;
                            @memcpy(&verify_current, &current);
                            // Continue walk from position steps to base_minus_one
                            for (0..remaining_steps) |j| {
                                const pos_in_chain: u8 = steps + @as(u8, @intCast(j)) + 1;
                                const next = try self.applyPoseidonChainTweakHash(verify_current, epoch, @as(u8, @intCast(chain_index)), pos_in_chain, secret_key.parameter);
                                @memcpy(verify_current[0..hash_len], next[0..hash_len]);
                                @memset(verify_current[hash_len..8], FieldElement{ .value = 0 });
                            }
                            log.debugPrint("ZIG_SIGN_VERIFY: Chain 0 continued from position {} to {}: verify_current[0]=0x{x:0>8}\n", .{ steps, base_minus_one, verify_current[0].value });

                            // Now compute from scratch using computeHashChainDomain
                            const verify_domain_elements = self.prfDomainElement(secret_key.prf_key, epoch, 0);
                            const full_chain = try self.computeHashChainDomain(verify_domain_elements, epoch, 0, secret_key.parameter);
                            log.debugPrint("ZIG_SIGN_VERIFY: Chain 0 computed from scratch to position {}: full_chain[0]=0x{x:0>8}\n", .{ base_minus_one, full_chain[0].value });

                            if (!verify_current[0].eql(full_chain[0])) {
                                log.debugPrint("ZIG_SIGN_ERROR: Chain 0 continuation mismatch! continued[0]=0x{x:0>8} vs full[0]=0x{x:0>8}\n", .{ verify_current[0].value, full_chain[0].value });
                            } else {
                                log.debugPrint("ZIG_SIGN_VERIFY: Chain 0 continuation matches ✓\n", .{});
                            }
                        }
                    }
                }
            }
        } else {
            // Parallel processing for large workloads
            const ChainSignContext = struct {
                scheme: *GeneralizedXMSSSignatureScheme,
                prf_key: [32]u8,
                epoch: u32,
                parameter: [5]FieldElement,
                x: []const u8,
                dimension: usize,
                hash_len: usize,
                hashes: [][8]FieldElement,
                next_index: std.atomic.Value(usize),
                error_flag: std.atomic.Value(bool),
                error_mutex: std.Thread.Mutex,
                stored_error: ?anyerror,
            };

            const chainSignWorker = struct {
                fn worker(ctx: *ChainSignContext) void {
                    while (true) {
                        const chain_index = ctx.next_index.fetchAdd(1, .monotonic);
                        if (chain_index >= ctx.dimension) {
                            break;
                        }

                        // PRF start state (domain_elements are in Montgomery form from ShakePRFtoF)
                        const domain_elements = ctx.scheme.prfDomainElement(ctx.prf_key, ctx.epoch, @as(u64, @intCast(chain_index)));
                        var current: [8]FieldElement = undefined;
                        // domain_elements are already in Montgomery form, store directly for hash_len elements
                        for (0..ctx.hash_len) |j| {
                            current[j] = FieldElement{ .value = domain_elements[j] };
                        }
                        // OPTIMIZATION: Use @memset instead of loop for zero-padding
                        @memset(current[ctx.hash_len..8], FieldElement{ .value = 0 });

                        // Walk chain for x[chain_index] steps
                        const steps: u8 = ctx.x[chain_index];
                        if (steps > 0) {
                            var s: u8 = 1;
                            while (s <= steps) : (s += 1) {
                                const next = ctx.scheme.applyPoseidonChainTweakHash(current, ctx.epoch, @as(u8, @intCast(chain_index)), s, ctx.parameter) catch |err| {
                                    ctx.error_mutex.lock();
                                    defer ctx.error_mutex.unlock();
                                    if (!ctx.error_flag.load(.monotonic)) {
                                        ctx.error_flag.store(true, .monotonic);
                                        ctx.stored_error = err;
                                    }
                                    return;
                                };
                                @memcpy(current[0..ctx.hash_len], next[0..ctx.hash_len]);
                                @memset(current[ctx.hash_len..8], FieldElement{ .value = 0 });
                            }
                        }
                        // Store hashes in Montgomery form (matching Rust's internal representation)
                        ctx.hashes[chain_index] = current;
                    }
                }
            };

            var chain_ctx = ChainSignContext{
                .scheme = self,
                .prf_key = secret_key.prf_key,
                .epoch = epoch,
                .parameter = secret_key.parameter,
                .x = x,
                .dimension = dimension,
                .hash_len = hash_len,
                .hashes = hashes,
                .next_index = std.atomic.Value(usize).init(0),
                .error_flag = std.atomic.Value(bool).init(false),
                .error_mutex = .{},
                .stored_error = null,
            };

            const num_threads = @min(num_cpus, dimension);
            // OPTIMIZATION: Use stack allocation for small thread counts
            if (num_threads <= 16) {
                var threads_stack: [16]std.Thread = undefined;
                const threads = threads_stack[0..num_threads];

                for (0..num_threads) |t| {
                    threads[t] = std.Thread.spawn(.{}, chainSignWorker.worker, .{&chain_ctx}) catch |err| {
                        chain_ctx.error_mutex.lock();
                        defer chain_ctx.error_mutex.unlock();
                        if (!chain_ctx.error_flag.load(.monotonic)) {
                            chain_ctx.error_flag.store(true, .monotonic);
                            chain_ctx.stored_error = err;
                        }
                        // Continue spawning remaining threads
                        continue;
                    };
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            } else {
                // Fallback to heap allocation for large thread counts
                var threads = try self.allocator.alloc(std.Thread, num_threads);
                defer self.allocator.free(threads);

                for (0..num_threads) |t| {
                    threads[t] = std.Thread.spawn(.{}, chainSignWorker.worker, .{&chain_ctx}) catch |err| {
                        chain_ctx.error_mutex.lock();
                        defer chain_ctx.error_mutex.unlock();
                        if (!chain_ctx.error_flag.load(.monotonic)) {
                            chain_ctx.error_flag.store(true, .monotonic);
                            chain_ctx.stored_error = err;
                        }
                        // Continue spawning remaining threads
                        continue;
                    };
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            }

            // Check for errors
            if (chain_ctx.error_flag.load(.monotonic)) {
                chain_ctx.error_mutex.lock();
                defer chain_ctx.error_mutex.unlock();
                if (chain_ctx.stored_error) |err| {
                    return err;
                }
                return error.UnknownError;
            }
        }

        if (epoch == 0) {
            log.debugPrint("ZIG_SIGN_VERIFY_START: Epoch 0 verification block entered\n", .{});
            // First, verify that PRF key and parameter match what was used during tree building
            log.debugPrint("ZIG_SIGN_VERIFY: secret_key.prf_key[0..8]=", .{});
            for (secret_key.prf_key[0..8]) |b| log.debugPrint("{x:0>2}", .{b});
            log.debugPrint("\n", .{});
            log.debugPrint("ZIG_SIGN_VERIFY: secret_key.parameter[0]=0x{x:0>8} (canonical: 0x{x:0>8})\n", .{ secret_key.parameter[0].value, secret_key.parameter[0].toCanonical() });

            const test_prf_domain = self.prfDomainElement(secret_key.prf_key, 1, 0);
            log.debugPrint("ZIG_SIGN_VERIFY: PRF domain for epoch 1, chain 0: domain[0]=0x{x:0>8}, prf_key[0..8]=", .{test_prf_domain[0]});
            for (secret_key.prf_key[0..8]) |b| log.debugPrint("{x:0>2}", .{b});
            log.debugPrint(", parameter[0]=0x{x:0>8} (canonical: 0x{x:0>8})\n", .{ secret_key.parameter[0].value, secret_key.parameter[0].toCanonical() });

            // Compute chain domain with exact same inputs
            const test_chain_domain = try self.computeHashChainDomain(test_prf_domain, 1, 0, secret_key.parameter);
            log.debugPrint("ZIG_SIGN_VERIFY: Chain domain for epoch 1, chain 0: chain_domain[0]=0x{x:0>8}\n", .{test_chain_domain[0].value});

            // Note: The stored trees may have been built with old code, so this check may fail
            // even though the chain domains match during keygen and signing
            log.debugPrint("ZIG_SIGN_VERIFY: Chain domain for epoch 1, chain 0: chain_domain[0]=0x{x:0>8}\n", .{test_chain_domain[0].value});
            if (test_chain_domain[0].value != 0x0599043e) {
                log.debugPrint("ZIG_SIGN_WARNING: Chain domain doesn't match stored tree value (0x0599043e), got: 0x{x:0>8}\n", .{test_chain_domain[0].value});
                log.debugPrint("ZIG_SIGN_WARNING: This may indicate stored trees were built with old code. Chain domains match during keygen and signing.\n", .{});
                // Don't return error - the chain domains match during keygen and signing, so this is expected
            }

            // (Previously: detailed ZIG_SIGN_PATH_VERIFY logs for epoch 1 chain/path debugging)
            // These logs have been removed as they were only useful during manual investigations.
        }

        if (epoch == 0) {
            var scratch_chain_domains = try self.allocator.alloc([8]FieldElement, self.lifetime_params.dimension);
            defer self.allocator.free(scratch_chain_domains);

            // Compute all chain domains from scratch (from PRF to position 7)
            for (0..self.lifetime_params.dimension) |chain_idx| {
                const scratch_domain_elements = self.prfDomainElement(secret_key.prf_key, epoch, @as(u64, @intCast(chain_idx)));
                scratch_chain_domains[chain_idx] = try self.computeHashChainDomain(scratch_domain_elements, epoch, @as(u8, @intCast(chain_idx)), secret_key.parameter);
            }

            // Compute leaf domain from scratch chain domains
            var scratch_leaf_buffer: [8]FieldElement = undefined;
            try self.reduceChainDomainsToLeafDomain(scratch_chain_domains, secret_key.parameter, epoch, &scratch_leaf_buffer);
            const scratch_leaf = scratch_leaf_buffer[0..self.lifetime_params.hash_len_fe];

            // Now compute what verification would produce (continue from stored hashes to position 7)
            var verify_chain_domains = try self.allocator.alloc([8]FieldElement, hashes.len);
            defer self.allocator.free(verify_chain_domains);
            const base_minus_one = self.lifetime_params.base - 1;

            for (hashes, 0..) |hash_domain, chain_idx| {
                var verify_current: [8]FieldElement = undefined;
                @memcpy(verify_current[0..hash_len], hash_domain[0..hash_len]);
                for (hash_len..8) |j| {
                    verify_current[j] = FieldElement{ .value = 0 };
                }
                const start_pos = x[chain_idx];
                const verify_steps = base_minus_one - start_pos;
                for (0..verify_steps) |j| {
                    const pos_in_chain: u8 = start_pos + @as(u8, @intCast(j)) + 1;
                    const next = try self.applyPoseidonChainTweakHash(verify_current, epoch, @as(u8, @intCast(chain_idx)), pos_in_chain, secret_key.parameter);
                    @memcpy(verify_current[0..hash_len], next[0..hash_len]);
                    @memset(verify_current[hash_len..8], FieldElement{ .value = 0 });
                }
                verify_chain_domains[chain_idx] = verify_current;
            }

            var verify_leaf_buffer: [8]FieldElement = undefined;
            try self.reduceChainDomainsToLeafDomain(verify_chain_domains, secret_key.parameter, epoch, &verify_leaf_buffer);
            const verify_leaf = verify_leaf_buffer[0..self.lifetime_params.hash_len_fe];

            log.debugPrint("ZIG_SIGN_LEAF_VERIFY: Scratch leaf[0]=0x{x:0>8}, verify leaf[0]=0x{x:0>8}\n", .{ scratch_leaf[0].value, verify_leaf[0].value });
            if (!scratch_leaf[0].eql(verify_leaf[0])) {
                log.debugPrint("ZIG_SIGN_LEAF_ERROR: Leaf domain mismatch! scratch[0]=0x{x:0>8} vs verify[0]=0x{x:0>8}\n", .{ scratch_leaf[0].value, verify_leaf[0].value });
            } else {
                log.debugPrint("ZIG_SIGN_LEAF_VERIFY: Leaf domains match ✓\n", .{});
            }
        }

        // Debug: print rho values before creating signature (for Zig→Zig debugging)
        const rand_len_debug = self.lifetime_params.rand_len_fe;
        log.print("ZIG_SIGN_DEBUG: rho before signature creation (Montgomery): ", .{});
        for (0..rand_len_debug) |i| {
            log.print("0x{x:0>8} ", .{rho_fixed[i].toMontgomery()});
        }
        log.print("\n", .{});

        // Create signature with proper error handling
        const signature = GeneralizedXMSSSignature.init(self.allocator, path, rho_fixed, hashes) catch |err| {
            // Clean up allocations if signature creation fails
            path.deinit();
            self.allocator.free(hashes);
            return err;
        };

        // Free the original hashes allocation after copying (done in init)
        self.allocator.free(hashes);
        self.allocator.free(x);

        return signature;
    }

    /// Generate randomness (matching Rust PRF::get_randomness)
    fn generateRandomness(
        self: *GeneralizedXMSSSignatureScheme,
        prf_key: [32]u8,
        epoch: u32,
        message: [MESSAGE_LENGTH]u8,
        counter: u64,
    ) ![]FieldElement {
        return rng_flow.generateRandomness(self, prf_key, epoch, message, counter);
    }

    /// Verification function (matching Rust verify exactly)
    pub fn verify(
        self: *GeneralizedXMSSSignatureScheme,
        public_key: *const GeneralizedXMSSPublicKey,
        epoch: u32,
        message: [MESSAGE_LENGTH]u8,
        signature: *const GeneralizedXMSSSignature,
    ) !bool {
        const lifetime = @as(u64, 1) << @intCast(self.lifetime_params.log_lifetime);
        if (epoch >= lifetime) return error.EpochTooLarge;

        // message is used below to derive target-sum digits

        // Debug: Validate signature struct is accessible
        log.print("ZIG_VERIFY_DEBUG: verify() called, signature=0x{x}\n", .{@intFromPtr(signature)});
        // Access struct fields using @field to avoid potential pointer issues
        const path_ptr = @field(signature, "path");
        log.print("ZIG_VERIFY_DEBUG: signature.path accessed via @field, path=0x{x}\n", .{@intFromPtr(path_ptr)});
        const rho_field = @field(signature, "rho");
        log.print("ZIG_VERIFY_DEBUG: signature.rho accessed via @field, rho[0]=0x{x}\n", .{rho_field[0].toCanonical()});

        // 1) Get x from encoding using signature's rho (matching Rust IE::encode)
        // During verification, we compute encoding without checking the sum
        // The sum check is only for signing (to ensure we find a valid encoding)
        const rho = rho_field; // Use @field access instead of direct access

        // Debug: log rho values (only first rand_len_fe elements are used)
        const rand_len = self.lifetime_params.rand_len_fe;
        // Print a test line first to verify we reach this code
        log.print("ZIG_VERIFY_DEBUG: About to print rho, rand_len={}\n", .{rand_len});
        // Debug: print rho in BOTH Montgomery and canonical to verify it matches what we read
        log.print("ZIG_VERIFY_DEBUG: Signature rho (first {} elements, Montgomery): ", .{rand_len});
        for (0..rand_len) |i| {
            log.print("0x{x:0>8} ", .{rho[i].toMontgomery()});
        }
        log.print("\n", .{});
        log.print("ZIG_VERIFY_DEBUG: Signature rho (first {} elements, canonical): ", .{rand_len});
        for (0..rand_len) |i| {
            log.print("0x{x:0>8} ", .{rho[i].toCanonical()});
        }
        log.print("\n", .{});
        // Debug: also log public key parameter to compare
        log.print("ZIG_VERIFY_DEBUG: Public key parameter (canonical): ", .{});
        for (0..5) |i| {
            log.print("0x{x:0>8} ", .{public_key.parameter[i].toCanonical()});
        }
        log.print("\n", .{});

        // Use parameter as-is (canonical); Poseidon handles Montgomery internally
        // Only use first rand_len_fe elements of rho (6 for lifetime 2^18, 7 for lifetime 2^8)
        const rho_slice = rho[0..rand_len];

        // Debug: print rho_slice used for encoding (for Zig→Zig debugging)
        log.print("ZIG_VERIFY_DEBUG: rho_slice used for encoding (Montgomery): ", .{});
        for (0..rand_len) |i| {
            log.print("0x{x:0>8} ", .{rho_slice[i].toMontgomery()});
        }
        log.print("\n", .{});

        // Debug: print parameter used for encoding (for Zig→Zig debugging)
        log.print("ZIG_VERIFY_DEBUG: parameter used for encoding (canonical): ", .{});
        for (0..5) |i| {
            log.print("0x{x:0>8} ", .{public_key.parameter[i].toCanonical()});
        }
        log.print("(Montgomery: ", .{});
        for (0..5) |i| {
            log.print("0x{x:0>8} ", .{public_key.parameter[i].toMontgomery()});
        }
        log.print(")\n", .{});

        const chunks = try self.applyTopLevelPoseidonMessageHash(public_key.parameter, epoch, rho_slice, message);
        defer self.allocator.free(chunks);

        // Allocate and copy chunks (take first dimension elements)
        const x = try self.allocator.alloc(u8, self.lifetime_params.dimension);
        defer self.allocator.free(x);
        @memcpy(x, chunks[0..self.lifetime_params.dimension]);

        // Debug: log encoding sum and first few chunks
        var encoding_sum: usize = 0;
        for (x) |chunk| encoding_sum += chunk;
        // Debug: log encoding sum and first few chunks
        log.print("ZIG_VERIFY_DEBUG: Encoding sum={} (expected 375)\n", .{encoding_sum});
        log.print("ZIG_VERIFY_DEBUG: Encoding chunks[0..5]: ", .{});
        for (0..@min(5, x.len)) |i| {
            log.print("x[{}]={} ", .{ i, x[i] });
        }
        log.print("\n", .{});

        // 2) Advance each chain domain to max based on message-derived x (target-sum digits)
        const base_minus_one: u8 = @as(u8, @intCast(self.lifetime_params.base - 1));

        const hashes = signature.getHashes();

        // Debug: log first hash to compare with what was stored
        if (hashes.len > 0) {
            log.print("ZIG_VERIFY_DEBUG: First hash[0] from signature (Montgomery): 0x{x:0>8}, (Canonical): 0x{x:0>8}\n", .{ hashes[0][0].value, hashes[0][0].toCanonical() });
        }

        // Handle hashes for both cross-implementation (Rust→Zig) and same-implementation (Zig→Zig):
        // - Both implementations: hashes come from binary as Montgomery values (wrappers use Montgomery form)
        //   Both libraries store hashes internally in Montgomery form, so we use them directly in Montgomery form
        // Use plonky3_field.KoalaBearField which uses Montgomery form (not core.KoalaBearField which uses canonical)
        const plonky3_field = @import("../../poseidon2/plonky3_field.zig");
        const F = plonky3_field.KoalaBearField; // Montgomery form implementation
        var final_chain_domains = try self.allocator.alloc([8]FieldElement, hashes.len);
        defer self.allocator.free(final_chain_domains);

        // Debug: print that we're starting chain computation
        log.debugPrint("ZIG_VERIFY_DEBUG: Starting chain computation, hashes.len={}, x[0]={}, base_minus_one={}\n", .{ hashes.len, x[0], base_minus_one });

        const hash_len = self.lifetime_params.hash_len_fe; // 7 for lifetime 2^18, 8 for lifetime 2^8

        // OPTIMIZATION: Parallelize chain computation since chains are independent
        // Use parallel processing for large workloads (64+ chains)
        const num_cpus = std.Thread.getCpuCount() catch 1;
        const min_parallel_chains = 64; // Threshold for parallel processing

        if (hashes.len < min_parallel_chains or num_cpus <= 1) {
            // Sequential processing for small workloads
            for (hashes, 0..) |domain, i| {
                var current: [8]FieldElement = undefined;
                @memcpy(current[0..hash_len], domain[0..hash_len]);
                // OPTIMIZATION: Use @memset instead of loop for zero-padding
                @memset(current[hash_len..8], FieldElement{ .value = 0 });
                const start_pos_in_chain: u8 = x[i];
                const steps: u8 = base_minus_one - start_pos_in_chain;

                // Debug: print starting state for first chain with detailed comparison
                if (i == 0) {
                    log.print("ZIG_VERIFY_DEBUG: Chain {} starting from hashes (Montgomery): ", .{i});
                    for (0..hash_len) |h| {
                        log.print("0x{x:0>8} ", .{current[h].value});
                    }
                    log.print("(x[{}]={}, steps={}, base_minus_one={})\n", .{ i, start_pos_in_chain, steps, base_minus_one });
                    log.print("ZIG_VERIFY_DEBUG: Chain {} starting from hashes (Canonical): ", .{i});
                    for (0..hash_len) |h| {
                        log.print("0x{x:0>8} ", .{current[h].toCanonical()});
                    }
                    log.print("\n", .{});
                    log.print("ZIG_VERIFY_DEBUG: Chain {} starting at position {} in chain, need to walk {} steps to reach position {}\n", .{ i, start_pos_in_chain, steps, base_minus_one });
                    // Also print what domain[0] contains directly
                    log.print("ZIG_VERIFY_DEBUG: domain[0] directly (Montgomery): 0x{x:0>8}, (Canonical): 0x{x:0>8}\n", .{ domain[0].value, domain[0].toCanonical() });
                }

                if (i == 0 or i == 2) {
                    // domain[0] is in Montgomery form (we read it as Montgomery)
                    const initial_monty = domain[0].value;
                    const initial_canonical = domain[0].toCanonical();
                    log.print("ZIG_VERIFY_DEBUG: Chain {} starting from position {} (x[i]={}), steps={}, initial_monty[0]=0x{x:0>8} initial_canonical[0]=0x{x:0>8}\n", .{ i, start_pos_in_chain, start_pos_in_chain, steps, initial_monty, initial_canonical });
                }

                // Walk 'steps' steps from start_pos_in_chain (matching Rust exactly)
                // Rust: for j in 0..steps { tweak = chain_tweak(epoch, chain_index, start_pos_in_chain + j + 1) }
                for (0..steps) |j| {
                    const pos_in_chain: u8 = start_pos_in_chain + @as(u8, @intCast(j)) + 1;
                    if (i == 0) {
                        // Debug: print every step for chain 0
                        log.debugPrint("ZIG_VERIFY_DEBUG: Chain {} step {}: pos_in_chain={}, current[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical)\n", .{ i, j, pos_in_chain, current[0].value, current[0].toCanonical() });
                    }
                    const next = try self.applyPoseidonChainTweakHash(current, epoch, @as(u8, @intCast(i)), pos_in_chain, public_key.parameter);
                    // Only use hash_len_fe elements (7 for lifetime 2^18, 8 for lifetime 2^8)
                    @memcpy(current[0..hash_len], next[0..hash_len]);
                    // OPTIMIZATION: Use @memset instead of loop for zero-padding
                    @memset(current[hash_len..8], FieldElement{ .value = 0 });
                    if (i == 0) {
                        log.debugPrint("ZIG_VERIFY_DEBUG: Chain {} step {} result: next[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical)\n", .{ i, j, next[0].value, next[0].toCanonical() });
                    }
                }

                final_chain_domains[i] = current;

                // Debug: print first chain final value for comparison
                if (i == 0) {
                    log.debugPrint("ZIG_VERIFY_DEBUG: Chain {} final domain after walking {} steps (Montgomery): ", .{ i, steps });
                    for (0..hash_len) |h| {
                        log.debugPrint("0x{x:0>8} ", .{current[h].value});
                    }
                    log.debugPrint("\n", .{});
                    // Compare with tree building
                    if (epoch == 0) {
                        log.debugPrint("ZIG_VERIFY_DEBUG: Epoch 0 chain 0 final domain at position {} (Montgomery, should match tree building): ", .{base_minus_one});
                        for (0..hash_len) |h| {
                            log.debugPrint("0x{x:0>8} ", .{current[h].value});
                        }
                        log.debugPrint("\n", .{});
                    }
                }
                // Debug: print a few more chains for epoch 0 to verify they're all correct
                if (epoch == 0 and (i == 1 or i == 2 or i == 63)) {
                    log.debugPrint("ZIG_VERIFY_DEBUG: Epoch 0 chain {} final domain[0] at position {}: 0x{x:0>8} (x[{}]={}, steps={})\n", .{ i, base_minus_one, current[0].value, i, x[i], steps });
                }
                if (i == 0) {
                    log.print("ZIG_VERIFY_DEBUG: Chain {} final (canonical): ", .{i});
                    for (0..hash_len) |j| {
                        log.print("0x{x:0>8} ", .{current[j].toCanonical()});
                    }
                    log.print("\n", .{});
                }
                if (i == 0 or i == 2) {
                    // Convert Montgomery to canonical for comparison with Rust
                    const monty_f = F{ .value = current[0].value };
                    const canonical = monty_f.toU32();
                    log.print("ZIG_VERIFY_DEBUG: Chain {} final[0]=0x{x:0>8} (Montgomery) = 0x{x:0>8} (canonical)\n", .{ i, current[0].value, canonical });
                }
            }
        } else {
            // Parallel processing for large workloads
            const ChainVerifyContext = struct {
                scheme: *GeneralizedXMSSSignatureScheme,
                hashes: [][8]FieldElement,
                x: []const u8,
                epoch: u32,
                parameter: [5]FieldElement,
                base_minus_one: u8,
                hash_len: usize,
                final_chain_domains: [][8]FieldElement,
                next_index: std.atomic.Value(usize),
                error_flag: std.atomic.Value(bool),
                error_mutex: std.Thread.Mutex,
                stored_error: ?anyerror,
            };

            const chainVerifyWorker = struct {
                fn worker(ctx: *ChainVerifyContext) void {
                    while (true) {
                        const i = ctx.next_index.fetchAdd(1, .monotonic);
                        if (i >= ctx.hashes.len) {
                            break;
                        }

                        const domain = ctx.hashes[i];
                        var current: [8]FieldElement = undefined;
                        @memcpy(current[0..ctx.hash_len], domain[0..ctx.hash_len]);
                        // OPTIMIZATION: Use @memset instead of loop for zero-padding
                        @memset(current[ctx.hash_len..8], FieldElement{ .value = 0 });
                        const start_pos_in_chain: u8 = ctx.x[i];
                        const steps: u8 = ctx.base_minus_one - start_pos_in_chain;

                        // Walk 'steps' steps from start_pos_in_chain
                        for (0..steps) |j| {
                            const pos_in_chain: u8 = start_pos_in_chain + @as(u8, @intCast(j)) + 1;
                            const next = ctx.scheme.applyPoseidonChainTweakHash(current, ctx.epoch, @as(u8, @intCast(i)), pos_in_chain, ctx.parameter) catch |err| {
                                ctx.error_mutex.lock();
                                defer ctx.error_mutex.unlock();
                                if (!ctx.error_flag.load(.monotonic)) {
                                    ctx.error_flag.store(true, .monotonic);
                                    ctx.stored_error = err;
                                }
                                return;
                            };
                            @memcpy(current[0..ctx.hash_len], next[0..ctx.hash_len]);
                            // OPTIMIZATION: Use @memset instead of loop for zero-padding
                            @memset(current[ctx.hash_len..8], FieldElement{ .value = 0 });
                        }

                        ctx.final_chain_domains[i] = current;
                    }
                }
            };

            var chain_ctx = ChainVerifyContext{
                .scheme = self,
                .hashes = hashes,
                .x = x,
                .epoch = epoch,
                .parameter = public_key.parameter,
                .base_minus_one = base_minus_one,
                .hash_len = hash_len,
                .final_chain_domains = final_chain_domains,
                .next_index = std.atomic.Value(usize).init(0),
                .error_flag = std.atomic.Value(bool).init(false),
                .error_mutex = .{},
                .stored_error = null,
            };

            const num_threads = @min(num_cpus, hashes.len);
            // OPTIMIZATION: Use stack allocation for small thread counts
            if (num_threads <= 16) {
                var threads_stack: [16]std.Thread = undefined;
                const threads = threads_stack[0..num_threads];

                for (0..num_threads) |t| {
                    threads[t] = std.Thread.spawn(.{}, chainVerifyWorker.worker, .{&chain_ctx}) catch |err| {
                        chain_ctx.error_mutex.lock();
                        defer chain_ctx.error_mutex.unlock();
                        if (!chain_ctx.error_flag.load(.monotonic)) {
                            chain_ctx.error_flag.store(true, .monotonic);
                            chain_ctx.stored_error = err;
                        }
                        // Continue spawning remaining threads
                        continue;
                    };
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            } else {
                // Fallback to heap allocation for large thread counts
                var threads = try self.allocator.alloc(std.Thread, num_threads);
                defer self.allocator.free(threads);

                for (0..num_threads) |t| {
                    threads[t] = std.Thread.spawn(.{}, chainVerifyWorker.worker, .{&chain_ctx}) catch |err| {
                        chain_ctx.error_mutex.lock();
                        defer chain_ctx.error_mutex.unlock();
                        if (!chain_ctx.error_flag.load(.monotonic)) {
                            chain_ctx.error_flag.store(true, .monotonic);
                            chain_ctx.stored_error = err;
                        }
                        // Continue spawning remaining threads
                        continue;
                    };
                }

                // Wait for all threads
                for (threads) |thread| {
                    thread.join();
                }
            }

            // Check for errors
            if (chain_ctx.error_flag.load(.monotonic)) {
                chain_ctx.error_mutex.lock();
                defer chain_ctx.error_mutex.unlock();
                if (chain_ctx.stored_error) |err| {
                    return err;
                }
                return error.UnknownError;
            }
        }

        // Debug: print first chain final value for comparison (only for parallel path)
        if (hashes.len >= min_parallel_chains and num_cpus > 1 and final_chain_domains.len > 0) {
            const i: usize = 0;
            const current = final_chain_domains[i];
            const debug_steps = base_minus_one - x[i];
            if (i == 0) {
                log.debugPrint("ZIG_VERIFY_DEBUG: Chain {} final domain after walking {} steps (Montgomery): ", .{ i, debug_steps });
                for (0..hash_len) |h| {
                    log.debugPrint("0x{x:0>8} ", .{current[h].value});
                }
                log.debugPrint("\n", .{});
                // Compare with tree building
                if (epoch == 0) {
                    log.debugPrint("ZIG_VERIFY_DEBUG: Epoch 0 chain 0 final domain at position {} (Montgomery, should match tree building): ", .{base_minus_one});
                    for (0..hash_len) |h| {
                        log.debugPrint("0x{x:0>8} ", .{current[h].value});
                    }
                    log.debugPrint("\n", .{});
                }
            }
            // Debug: print a few more chains for epoch 0 to verify they're all correct
            if (epoch == 0 and (i == 1 or i == 2 or i == 63)) {
                const debug_steps2 = base_minus_one - x[i];
                log.debugPrint("ZIG_VERIFY_DEBUG: Epoch 0 chain {} final domain[0] at position {}: 0x{x:0>8} (x[{}]={}, steps={})\n", .{ i, base_minus_one, current[0].value, i, x[i], debug_steps2 });
            }
            if (i == 0) {
                log.print("ZIG_VERIFY_DEBUG: Chain {} final (canonical): ", .{i});
                for (0..hash_len) |j| {
                    log.print("0x{x:0>8} ", .{current[j].toCanonical()});
                }
                log.print("\n", .{});
            }
            if (i == 0 or i == 2) {
                // Convert Montgomery to canonical for comparison with Rust
                const monty_f = F{ .value = current[0].value };
                const canonical = monty_f.toU32();
                log.print("ZIG_VERIFY_DEBUG: Chain {} final[0]=0x{x:0>8} (Montgomery) = 0x{x:0>8} (canonical)\n", .{ i, current[0].value, canonical });
            }
        }

        // Debug: log chain ends before reduction
        if (epoch == 16) {
            log.print("ZIG_VERIFY_DEBUG: Chain ends before reduction for epoch {} (first 3 chains): ", .{epoch});
            for (0..@min(3, final_chain_domains.len)) |ci| {
                log.print("chain{}[0]=0x{x:0>8} ", .{ ci, final_chain_domains[ci][0].value });
            }
            log.print("\n", .{});
        }

        // 3) Reduce 64 chain domains to a single leaf domain using tree-tweak hashing
        if (epoch == 16) {
            log.print("ZIG_VERIFY_DEBUG: Calling reduceChainDomainsToLeafDomain with final_chain_domains.len={} epoch={}\n", .{ final_chain_domains.len, epoch });
        }
        var leaf_domain_buffer: [8]FieldElement = undefined;
        try self.reduceChainDomainsToLeafDomain(final_chain_domains, public_key.parameter, epoch, &leaf_domain_buffer);
        const leaf_domain_slice = leaf_domain_buffer[0..self.lifetime_params.hash_len_fe];
        // Debug: print leaf domain after reduction
        log.debugPrint("ZIG_VERIFY_DEBUG: Leaf domain after reduction (Montgomery): ", .{});
        for (0..hash_len) |h| {
            log.debugPrint("0x{x:0>8} ", .{leaf_domain_slice[h].value});
        }
        log.debugPrint("\n", .{});
        // Convert to fixed-size [8]FieldElement array (pad with zeros if needed)
        // hash_len is already declared above (line 2643)
        // OPTIMIZATION: Use @memcpy and @memset for efficient copying
        var current_domain: [8]FieldElement = undefined;
        @memcpy(current_domain[0..hash_len], leaf_domain_slice[0..hash_len]);
        @memset(current_domain[hash_len..8], FieldElement{ .value = 0 });

        // Debug: log leaf domain
        log.print("ZIG_VERIFY_DEBUG: Leaf domain after reduction (canonical): ", .{});
        for (0..hash_len) |i| {
            log.print("0x{x:0>8} ", .{current_domain[i].toCanonical()});
        }
        log.print("\n", .{});
        if (epoch == 16) {
            log.print("ZIG_VERIFY_DEBUG: Leaf domain after reduction: ", .{});
            for (0..hash_len) |i| {
                log.print("0x{x:0>8} ", .{current_domain[i].value});
            }
            log.print("\n", .{});
        }

        // 4) Walk Merkle path using tweak hash and epoch-based orientation
        // Calculate bottom tree index to know where bottom tree ends
        const leafs_per_bottom_tree = @as(usize, 1) << @intCast(self.lifetime_params.log_lifetime / 2);

        // Debug: log leaf domain for epoch 16
        if (epoch == 16) {
            const bottom_tree_index = epoch / @as(u32, @intCast(leafs_per_bottom_tree));
            log.print("ZIG_VERIFY: Epoch {} - Initial leaf domain[0]=0x{x:0>8} (bottom_tree_index={} final_chain_domains.len={})\n", .{ epoch, current_domain[0].value, bottom_tree_index, final_chain_domains.len });
        }
        const bottom_tree_index = @as(usize, @intCast(epoch)) / leafs_per_bottom_tree;
        // For top tree, we need to compute the position relative to where the top tree starts
        // During verification, we don't have access to left_bottom_tree_index, but we can infer it
        // from the path structure. However, the path was computed during signing with:
        // top_pos = bottom_tree_index - left_bottom_tree_index
        // Since verification walks the path sequentially, the position naturally transitions
        // from bottom tree position to top tree position after log_lifetime/2 levels.
        // The top_pos here is only used for debug logging - the actual path walk uses the
        // position that naturally evolves from the epoch.
        const top_pos = @as(u32, @intCast(bottom_tree_index));

        var position: u32 = epoch;
        const nodes = signature.getPath().getNodes();
        var level: u8 = 0;
        const bottom_tree_depth = self.lifetime_params.log_lifetime / 2;
        log.print("ZIG_VERIFY_DEBUG: Starting Merkle path walk from epoch {} with {} nodes (bottom_tree_index={} top_pos={} bottom_depth={})\n", .{ epoch, nodes.len, bottom_tree_index, top_pos, bottom_tree_depth });

        if (epoch == 0 and nodes.len > 0) {
            const first_path_node = nodes[0];
            log.debugPrint("ZIG_VERIFY_DEBUG: First path node[0] (sibling of epoch 0, should be epoch 1 leaf): 0x{x:0>8}\n", .{first_path_node[0].value});
            log.debugPrint("ZIG_VERIFY_DEBUG: Current leaf domain[0] (epoch 0): 0x{x:0>8}\n", .{current_domain[0].value});
        }

        for (nodes, 0..) |sibling_domain, node_idx| {
            // Note: After walking the bottom tree (depth/2 levels), position should naturally
            // be bottom_tree_index, which is the position in the top tree's first layer.
            // No need to reset position - it continues shifting naturally.
            const is_bottom_tree = node_idx < bottom_tree_depth;
            const is_top_tree = !is_bottom_tree;

            // Determine if current is left or right child (matching Rust: current_position.is_multiple_of(2))
            // Use position BEFORE shifting (matching Rust exactly)
            const original_position = position;
            const is_left = (position & 1) == 0;
            const is_right = !is_left;

            // Debug: log current_domain and sibling for epoch 0 and 16
            if (epoch == 0 or epoch == 16) {
                log.debugPrint("ZIG_VERIFY: Epoch {} - Node {}: current_domain[0]=0x{x:0>8} position={} sibling[0]=0x{x:0>8} is_left={}\n", .{ epoch, node_idx, current_domain[0].value, original_position, sibling_domain[0].value, is_left });
            }

            // Build children array (matching Rust exactly: [current_node, opening.co_path[l]] for left, [opening.co_path[l], current_node] for right)
            // Only use first hash_len_fe elements (7 for lifetime 2^18, 8 for lifetime 2^8)
            // hash_len is already declared above
            const left_slice = if (is_left) current_domain[0..hash_len] else sibling_domain[0..hash_len];
            const right_slice = if (is_left) sibling_domain[0..hash_len] else current_domain[0..hash_len];

            // Determine new position (position of the parent) - shift BEFORE computing tweak (matching Rust)
            // pos_in_level is the position of the parent in its level, which is position / 2
            // During tree building, pos_in_level = parent_start + i, where parent_start = start_index >> 1
            // For the first level, start_index = 0, so parent_start = 0, and pos_in_level = i (0, 1, 2, ...)
            // During verification, original_position is the position of the current node, and pos_in_level should be original_position / 2
            const pos_in_level: u32 = original_position >> 1;
            position >>= 1;

            // Debug: log hash inputs for epoch 16, levels 0-4
            if (epoch == 16 and node_idx <= 4) {
                log.print("ZIG_VERIFY: Epoch {} - Level {} hash inputs: current_domain[0]=0x{x:0>8} sibling[0]=0x{x:0>8} left[0]=0x{x:0>8} right[0]=0x{x:0>8} pos_in_level={} level_for_tweak={} (original_position={} is_left={})\n", .{ epoch, level, current_domain[0].value, sibling_domain[0].value, left_slice[0].value, right_slice[0].value, pos_in_level, level, original_position, is_left });
            }

            if (is_top_tree and node_idx == bottom_tree_depth) {
                log.debugPrint("ZIG_VERIFY_DEBUG: Transitioning to top tree at node_idx={}, position={}, bottom_tree_index={}, pos_in_level={}, level={}, current_domain[0]=0x{x:0>8}, sibling[0]=0x{x:0>8}\n", .{ node_idx, position, bottom_tree_index, pos_in_level, level, current_domain[0].value, sibling_domain[0].value });
                // Check if current_domain matches what was used during tree building
                // For epoch 0, bottom_tree_index=0, so current_domain should be root of bottom tree 0
                // The sibling should be root of bottom tree 1 (the first top tree path node)
                log.debugPrint("ZIG_VERIFY_DEBUG: After bottom tree (level {}), current_domain should be root of bottom tree {}: ", .{ level - 1, bottom_tree_index });
                for (0..hash_len) |i| {
                    log.debugPrint("0x{x:0>8} ", .{current_domain[i].value});
                }
                log.debugPrint("\n", .{});
                log.debugPrint("ZIG_VERIFY_DEBUG: Sibling (first top tree path node) should be root of bottom tree {}: ", .{bottom_tree_index + 1});
                for (0..hash_len) |i| {
                    log.debugPrint("0x{x:0>8} ", .{sibling_domain[i].value});
                }
                log.debugPrint("\n", .{});
                log.debugPrint("ZIG_VERIFY_DEBUG: For first top tree hash, level={}, pos_in_level={} (should match tree building: level=16, pos=0)\n", .{ level, pos_in_level });
                log.debugPrint("ZIG_VERIFY_DEBUG: First top tree hash inputs: left[0]=0x{x:0>8}, right[0]=0x{x:0>8}, level={}, pos_in_level={}, param[0]=0x{x:0>8}\n", .{ left_slice[0].value, right_slice[0].value, level, pos_in_level, public_key.parameter[0].value });
                log.debugPrint("ZIG_VERIFY_ERROR: Bottom tree 0 root mismatch! Computed=0x{x:0>8}, Expected=0x50175e49\n", .{current_domain[0].value});
            }

            if (epoch == 0 and is_top_tree) {
                log.debugPrint("ZIG_VERIFY_DEBUG: Top tree node {} (epoch 0): position={}, pos_in_level={}, sibling[0]=0x{x:0>8}\n", .{ node_idx, position, pos_in_level, sibling_domain[0].value });
            }
            if (epoch == 16) {
                log.print("ZIG_VERIFY: Epoch {} - Level {} node {}: original_position={} is_right={} pos_in_level={} sibling[0]=0x{x:0>8} (bottom={} top={})\n", .{ epoch, level, node_idx, original_position, is_right, pos_in_level, sibling_domain[0].value, is_bottom_tree, is_top_tree });
            }

            log.print("ZIG_VERIFY_DEBUG: Level {} node {}: original_position={} is_right={} pos_in_level={} (bottom={} top={})\n", .{ level, node_idx, original_position, is_right, pos_in_level, is_bottom_tree, is_top_tree });

            // Debug: log first element of left and right before hashing
            log.print("ZIG_VERIFY_DEBUG:   current[0]=0x{x:0>8} sibling[0]=0x{x:0>8}\n", .{ current_domain[0].value, sibling_domain[0].value });
            log.print("ZIG_VERIFY_DEBUG:   left[0]=0x{x:0>8} right[0]=0x{x:0>8}\n", .{ left_slice[0].value, right_slice[0].value });

            // Use level+1 for tweak (matching Rust: (l + 1))
            // Debug: log parameter for epoch 16
            if (epoch == 16 and level == 0) {
                log.print("ZIG_VERIFY: Epoch {} - Parameter[0]=0x{x:0>8}\n", .{ epoch, public_key.parameter[0].value });
                log.print("ZIG_VERIFY: Epoch {} - Level {} tweak params: level={} pos_in_level={} (compare with tree building: level=0 parent_pos=8)\n", .{ epoch, level, level, pos_in_level });
            }
            // Debug: log hash call for epoch 0 and 16, first top tree level
            if ((epoch == 0 or epoch == 16) and is_top_tree and node_idx == bottom_tree_depth) {
                // Create call_id to match with tree building
                const verify_call_id = left_slice[0].value ^ right_slice[0].value ^ public_key.parameter[0].value ^ @as(u32, @intCast(level)) ^ @as(u32, @intCast(pos_in_level));
                log.debugPrint("ZIG_VERIFY_HASH: Epoch {} - First top tree hash (level={} node_idx={}): left[0]=0x{x:0>8} right[0]=0x{x:0>8} level={} pos_in_level={} param[0]=0x{x:0>8} (canonical: 0x{x:0>8}) call_id=0x{x:0>8}\n", .{ epoch, level, node_idx, left_slice[0].value, right_slice[0].value, level, pos_in_level, public_key.parameter[0].value, public_key.parameter[0].toCanonical(), verify_call_id });
            }
            // Debug: log hash call for epoch 16, level 0
            if (epoch == 16 and level == 0) {
                // Create call_id to match with tree building
                const verify_call_id = left_slice[0].value ^ right_slice[0].value ^ public_key.parameter[0].value ^ @as(u32, @intCast(level)) ^ @as(u32, @intCast(pos_in_level));
                log.print("ZIG_VERIFY_HASH: Epoch {} - Level {} calling hash with left[0]=0x{x:0>8} right[0]=0x{x:0>8} level={} pos_in_level={} param[0]=0x{x:0>8} hash_len={} left_slice.len={} right_slice.len={} call_id=0x{x:0>8} left_all=", .{ epoch, level, left_slice[0].value, right_slice[0].value, level, pos_in_level, public_key.parameter[0].value, hash_len, left_slice.len, right_slice.len, verify_call_id });
                for (left_slice) |fe| log.print("0x{x:0>8} ", .{fe.value});
                log.print("right_all=", .{});
                for (right_slice) |fe| log.print("0x{x:0>8} ", .{fe.value});
                log.print("\n", .{});
            }
            if (epoch == 0 and is_top_tree and node_idx == bottom_tree_depth) {
                log.debugPrint("ZIG_VERIFY_DEBUG: First top tree hash call (epoch 0): level={}, pos_in_level={}, left[0]=0x{x:0>8}, right[0]=0x{x:0>8}\n", .{ level, pos_in_level, left_slice[0].value, right_slice[0].value });
                log.debugPrint("ZIG_VERIFY_DEBUG:   current_domain[0]=0x{x:0>8}, sibling[0]=0x{x:0>8}, is_left={}\n", .{ current_domain[0].value, sibling_domain[0].value, is_left });
            }

            const hash_level = level;
            const parent = try self.applyPoseidonTreeTweakHashWithSeparateInputs(left_slice, right_slice, hash_level, pos_in_level, public_key.parameter);
            defer self.allocator.free(parent);

            // Debug: For node 0, log the hash call details
            if (epoch == 0 and node_idx == 0) {
                log.debugPrint("ZIG_VERIFY_DEBUG: First bottom tree hash (epoch 0): left[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical), right[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical), level={}, hash_level={}, tweak_level={}, pos_in_level={}, param[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical), parent[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (Canonical)\n", .{ left_slice[0].value, left_slice[0].toCanonical(), right_slice[0].value, right_slice[0].toCanonical(), level, hash_level, hash_level + 1, pos_in_level, public_key.parameter[0].value, public_key.parameter[0].toCanonical(), parent[0].value, parent[0].toCanonical() });
            }

            if (epoch == 0 and is_top_tree and node_idx == bottom_tree_depth) {
                const tweak_level = level + 1;
                const tweak_bigint = (@as(u128, tweak_level) << 40) | (@as(u128, pos_in_level) << 8) | 0x01;
                log.debugPrint("ZIG_VERIFY_DEBUG: First top tree hash result[0]: 0x{x:0>8}, tweak_level={}, pos_in_level={}, tweak=0x{x}\n", .{ parent[0].value, tweak_level, pos_in_level, tweak_bigint });
            }

            if (epoch == 0 and is_bottom_tree) {
                log.debugPrint("ZIG_VERIFY_DEBUG: Bottom tree node {} (epoch 0): level={}, pos_in_level={}, current[0]=0x{x:0>8}, sibling[0]=0x{x:0>8}, parent[0]=0x{x:0>8}, is_left={}\n", .{ node_idx, level, pos_in_level, current_domain[0].value, sibling_domain[0].value, parent[0].value, is_left });
                // For the first node, also log the hash inputs in detail (after hash_level is computed)
                // This will be logged after the hash call below
                // For node 1, also log the hash inputs in detail (for 2^8 debugging)
                if (node_idx == 1) {
                    const hash_level_for_debug = if (node_idx == 0) level else level + 1;
                    log.debugPrint("ZIG_VERIFY_DEBUG: Node 1 hash (epoch 0): left[0]=0x{x:0>8}, right[0]=0x{x:0>8}, level={}, hash_level={}, tweak_level={}, pos_in_level={}, param[0]=0x{x:0>8}\n", .{ left_slice[0].value, right_slice[0].value, level, hash_level_for_debug, hash_level_for_debug + 1, pos_in_level, public_key.parameter[0].value });
                    log.debugPrint("ZIG_VERIFY_DEBUG: Node 1 should produce 0x0a7d8c40 (grandparents[0] from level 2), but got 0x{x:0>8}\n", .{parent[0].value});
                }
                // For the first node (node_idx=0), also log the hash inputs in detail
                if (node_idx == 0) {
                    log.debugPrint("ZIG_VERIFY_DEBUG: First bottom tree hash (epoch 0): left[0]=0x{x:0>8}, right[0]=0x{x:0>8}, level={}, pos_in_level={}, param[0]=0x{x:0>8} (Montgomery) / 0x{x:0>8} (canonical)\n", .{ left_slice[0].value, right_slice[0].value, level, pos_in_level, public_key.parameter[0].value, public_key.parameter[0].toCanonical() });
                }
            }

            if (epoch == 0 and is_top_tree and node_idx == bottom_tree_depth) {
                log.debugPrint("ZIG_VERIFY_DEBUG: First top tree hash result (epoch 0): parent[0]=0x{x:0>8}\n", .{parent[0].value});
            }
            // Debug: log hash result for epoch 16, level 0
            if (epoch == 16 and level == 0) {
                const verify_call_id = left_slice[0].value ^ right_slice[0].value ^ public_key.parameter[0].value ^ @as(u32, @intCast(level)) ^ @as(u32, @intCast(pos_in_level));
                log.print("ZIG_VERIFY_HASH: Epoch {} - Level {} hash result parent[0]=0x{x:0>8} parent.len={} hash_len={} call_id=0x{x:0>8}\n", .{ epoch, level, parent[0].value, parent.len, hash_len, verify_call_id });
            }

            // Debug: log parent computation for bottom tree (level 0-3) for epoch 16
            if (epoch == 16 and level < 4) {
                log.print("ZIG_VERIFY: Bottom tree level {} node {}: parent[0]=0x{x:0>8} (left[0]=0x{x:0>8} right[0]=0x{x:0>8} pos_in_level={} original_position={})\n", .{ level, node_idx, parent[0].value, left_slice[0].value, right_slice[0].value, pos_in_level, original_position });
            }
            // Debug: log parent after hashing for top tree (level 4+)
            if (level >= 4) {
                log.print("ZIG_VERIFY: Top tree level {} node {}: parent[0]=0x{x:0>8} (left[0]=0x{x:0>8} right[0]=0x{x:0>8} pos_in_level={} original_position={} is_right={} current[0]=0x{x:0>8} sibling[0]=0x{x:0>8})\n", .{ level, node_idx, parent[0].value, left_slice[0].value, right_slice[0].value, pos_in_level, original_position, is_right, current_domain[0].value, sibling_domain[0].value });
            }
            log.print("ZIG_VERIFY_DEBUG:   parent[0]=0x{x:0>8}\n", .{parent[0].value});

            // Copy back hash_len_fe elements into current_domain (7 for lifetime 2^18, 8 for lifetime 2^8)
            // current_domain is [8]FieldElement, but we only use the first hash_len_fe elements
            // hash_len is already declared above
            for (0..hash_len) |i| current_domain[i] = parent[i];
            // Zero out remaining elements to ensure clean state
            for (hash_len..8) |i| current_domain[i] = FieldElement{ .value = 0 };

            // Debug: log current_domain after each step for epoch 16
            if (epoch == 16) {
                log.print("ZIG_VERIFY: Epoch {} - After level {}: current_domain[0]=0x{x:0>8} (original_position={}, is_left={}, pos_in_level={})\n", .{ epoch, level, current_domain[0].value, original_position, is_left, pos_in_level });
            }
            if (epoch == 0 and node_idx == 15) {
                log.debugPrint("ZIG_VERIFY_DEBUG: After bottom tree walk (node 15): current_domain[0]=0x{x:0>8}, expected root=0x50175e49\n", .{current_domain[0].value});
                if (current_domain[0].value != 0x50175e49) {
                    log.debugPrint("ZIG_VERIFY_ERROR: Bottom tree 0 root mismatch! Computed=0x{x:0>8}, Expected=0x50175e49\n", .{current_domain[0].value});
                } else {
                    log.debugPrint("ZIG_VERIFY_DEBUG: Bottom tree 0 root matches! ✓\n", .{});
                }
            }

            level += 1;
        }
        log.print("ZIG_VERIFY_DEBUG: Final computed root after Merkle path walk (Montgomery): ", .{});
        for (0..hash_len) |h| {
            log.print("0x{x:0>8} ", .{current_domain[h].value});
        }
        log.print("\n", .{});
        log.print("ZIG_VERIFY_DEBUG: Public key root (Montgomery): ", .{});
        for (0..hash_len) |h| {
            log.print("0x{x:0>8} ", .{public_key.root[h].value});
        }
        log.print("\n", .{});

        // 4) Compare computed root with public key root (both stored as canonical field elements)
        // Root length is hash_len_fe (7 for lifetime 2^18, 8 for lifetime 2^8)
        const root_len = self.lifetime_params.hash_len_fe;
        var match = true;
        // Root comparison debug output (always print for debugging)
        log.print("ZIG_VERIFY_DEBUG: Comparing roots (length={}):\n", .{root_len});
        for (0..root_len) |i| {
            const computed_val = current_domain[i].toCanonical();
            const expected_val = public_key.root[i].toCanonical();
            const computed_monty = current_domain[i].value;
            const expected_monty = public_key.root[i].value;
            if (!current_domain[i].eql(public_key.root[i])) {
                log.print("ZIG_VERIFY_ERROR: Root mismatch at index {}: computed=0x{x:0>8} (canonical) / 0x{x:0>8} (monty) expected=0x{x:0>8} (canonical) / 0x{x:0>8} (monty)\n", .{ i, computed_val, computed_monty, expected_val, expected_monty });
                match = false;
            } else {
                log.print("ZIG_VERIFY_DEBUG: Root[{}] matches: 0x{x:0>8} (canonical) / 0x{x:0>8} (monty)\n", .{ i, computed_val, computed_monty });
            }
        }
        if (match) {
            log.debugPrint("ZIG_VERIFY_DEBUG: Root matches! Verification successful.\n", .{});
        } else {
            log.debugPrint("ZIG_VERIFY_DEBUG: Root mismatch! Computed root (Montgomery): ", .{});
            for (0..root_len) |i| {
                log.debugPrint("0x{x:0>8} ", .{current_domain[i].value});
            }
            log.debugPrint("\nZIG_VERIFY_DEBUG: Expected root (Montgomery): ", .{});
            for (0..root_len) |i| {
                log.debugPrint("0x{x:0>8} ", .{public_key.root[i].value});
            }
            log.debugPrint("\n", .{});
        }
        return match;
    }
};

// Test functions
/// Convert tweak encoding to field elements using base-p representation (matching Rust)
fn tweakToFieldElements(tweak_encoding: u128) [2]FieldElement {
    const KOALABEAR_ORDER_U64 = 0x7f000001; // 2^31 - 2^24 + 1

    var acc = tweak_encoding;
    var result: [2]FieldElement = undefined;

    for (0..2) |i| {
        const digit = @as(u64, @intCast(acc % KOALABEAR_ORDER_U64));
        acc /= KOALABEAR_ORDER_U64;
        result[i] = FieldElement.fromCanonical(@intCast(digit));
    }

    return result;
}

test "generalized_xmss_keygen" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    var scheme = try GeneralizedXMSSSignatureScheme.init(allocator, .lifetime_2_8);
    defer scheme.deinit();

    const keypair = try scheme.keyGen(0, 256);
    defer keypair.secret_key.deinit();

    // Verify key structure
    try std.testing.expect(keypair.public_key.root[0].value != 0);
    try std.testing.expect(keypair.secret_key.activation_epoch == 0);
    try std.testing.expect(keypair.secret_key.num_active_epochs >= 256);
}

test "generalized_xmss_sign_verify" {
    var gpa = std.heap.GeneralPurposeAllocator(.{}){};
    defer _ = gpa.deinit();
    const allocator = gpa.allocator();

    var scheme = try GeneralizedXMSSSignatureScheme.init(allocator, .lifetime_2_8);
    defer scheme.deinit();

    const keypair = try scheme.keyGen(0, 256);
    defer keypair.secret_key.deinit();

    const message = [_]u8{0x42} ** MESSAGE_LENGTH;
    const signature = try scheme.sign(keypair.secret_key, 0, message);
    defer signature.deinit();

    const is_valid = try scheme.verify(&keypair.public_key, 0, message, signature);
    try std.testing.expect(is_valid);
}
